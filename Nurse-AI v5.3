{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTmG6mfIBHtiLuVvT95b6H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#@title BLOCK 1: Setup, Folder Structure, Config & Metrics Registry (v5.3.2, 2025-05-04)\n","# Purpose: Initialize Colab/Drive or local project folders, load/create config, enforce provenance,\n","# and register all metrics for later dashboard/audit use. All subsequent blocks depend on this structure.\n","# Changelog (v5.0–v5.3.2):\n","#   - v5.0: Basic folder setup, config stub\n","#   - v5.1: Parameter/folder/logging improvements\n","#   - v5.2: Universal layout, expanded config, batch metrics\n","#   - v5.3+: Metrics registry registration, explicit config snapshots, reviewer/metrics dirs for downstream use\n","\n","import os\n","import yaml\n","from pathlib import Path\n","\n","# ----------------------------------------------------------------\n","# 1. Mount Google Drive (Colab/local fallback)\n","try:\n","    from google.colab import drive\n","    DRIVE_PATH = \"/content/drive\"\n","    if not Path(DRIVE_PATH).exists():\n","        drive.mount(DRIVE_PATH, force_remount=True)\n","    print(\"✔ Google Drive mounted at\", DRIVE_PATH)\n","except ImportError:\n","    DRIVE_PATH = os.getcwd()\n","    print(\"(Local/dev mode, using working directory)\")\n","MYDRIVE = Path(DRIVE_PATH) / \"MyDrive\"\n","\n","# ----------------------------------------------------------------\n","# 2. Config (edit/bundle as config.yaml; all user-tunable fields here)\n","config_path = Path(\"config.yaml\")\n","overwrite = False # Only set True to reset config!\n","default_config = {\n","    'project_name': 'NurseAI_PaperQA2_v5.4',\n","    'openai_key_envvar': 'OPENAI_API_KEY',\n","    'llm_model': 'gpt-4.1',\n","    'embedding_model': 'text-embedding-3-large',\n","    'chunk_tokens': 500,\n","    'chunk_overlap': 100,\n","    'random_seed': 42,\n","    'prompts': {\n","        'system': (\n","            \"You are a research assistant extracting information from scientific literature. \"\n","            \"Answer every question using only the provided context (or say 'Not found').\"\n","        ),\n","        'user_template': (\n","            \"Research Question: {question}\\n\\n\"\n","            \"Context:\\n{context}\\n\\n\"\n","            \"Instructions: Use only the provided context. If the answer is missing, write 'Not found'.\"\n","        ),\n","    },\n","    'questions': [\n","        {'slug': 'patient_outcomes', 'text': \"How can AI technologies be utilised to enhance patient outcomes in nursing?\"},\n","        {'slug': 'nurse_competencies', 'text': \"In what ways can AI tools enhance nursing competencies and professional development?\"},\n","        {'slug': 'technical_infrastructure', 'text': \"What specific technical infrastructure is required for effective use of AI in healthcare settings?\"},\n","        {'slug': 'ethics_regulation', 'text': \"What are the key ethical and regulatory challenges associated with implementing AI in nursing?\"}\n","    ]\n","}\n","if not config_path.exists() or overwrite:\n","    with open(config_path, \"w\") as f:\n","        yaml.safe_dump(default_config, f, sort_keys=False)\n","    print(\"✔ config.yaml created/reset with default contents. Edit file as needed.\")\n","\n","# ----------------------------------------------------------------\n","# 3. Load config, set up directories\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name'].strip()\n","assert PROJECT_NAME, \"Project name in config cannot be blank!\"\n","\n","PROJECT_ROOT = MYDRIVE / PROJECT_NAME\n","PDF_DIR = PROJECT_ROOT / \"PDFs\"\n","CONFIG_DIR = PROJECT_ROOT / \"Project files\"\n","TO_AI_DIR = PROJECT_ROOT / \"Sent\"\n","FROM_AI_DIR = PROJECT_ROOT / \"Recieved\"\n","REVIEWER_DIR = PROJECT_ROOT / \"Reviewers\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","\n","for folder in [PDF_DIR, CONFIG_DIR, TO_AI_DIR, FROM_AI_DIR, REVIEWER_DIR, METRICS_DIR]:\n","    os.makedirs(folder, exist_ok=True)\n","\n","# ----------------------------------------------------------------\n","# 4. METRICS REGISTRY: Write project-wide standards for metrics, referencing all standard sources\n","metrics_registry = [\n","    {\n","        \"name\": \"Precision@k\",\n","        \"stage\": \"Retrieval\",\n","        \"formula\": \"Precision@k = relevant in top k / k\",\n","        \"python_code\": \"sum(relevant[:k])/k\",\n","        \"ref\": \"Voorhees, TREC-8 QA (1999)\",\n","        \"url\": \"https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=151495\",\n","        \"credibility\": 8\n","    },\n","    {\n","        \"name\": \"Recall@k\",\n","        \"stage\": \"Retrieval\",\n","        \"formula\": \"Recall@k = relevant in top k / all relevant\",\n","        \"python_code\": \"sum(relevant[:k])/total_relevant\",\n","        \"ref\": \"Voorhees, TREC-8 QA (1999)\",\n","        \"credibility\": 8\n","    },\n","    {\n","        \"name\": \"MRR\",\n","        \"stage\": \"Retrieval\",\n","        \"formula\": \"MRR = (1/|Q|) * sum_q 1/rank_q\",\n","        \"python_code\": \"np.mean([1/r for r in first_hit_ranks])\",\n","        \"ref\": \"Voorhees, TREC-8 (1999)\",\n","        \"credibility\": 8\n","    },\n","    {\n","        \"name\": \"nDCG\",\n","        \"stage\": \"Retrieval\",\n","        \"formula\": \"nDCG@k = DCG@k / IDCG@k\",\n","        \"python_code\": \"ndcg = compute_ndcg(relevance, k)\",\n","        \"ref\": \"Järvelin & Kekäläinen, TOIS 2002\",\n","        \"credibility\": 9\n","    },\n","    {\n","        \"name\": \"Cosine similarity\",\n","        \"stage\": \"Embedding\",\n","        \"formula\": \"cos_sim(u,v) = np.dot(u,v)/(||u||*||v||)\",\n","        \"python_code\": \"np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))\",\n","        \"ref\": \"Salton et al., CACM 1975\",\n","        \"credibility\": 8\n","    },\n","    {\n","        \"name\": \"ROUGE-L\",\n","        \"stage\": \"Generation\",\n","        \"formula\": \"ROUGE-L = LCS(ref, pred)/len(ref)\",\n","        \"python_code\": \"rouge = scorer.score(ref, pred)['rougeL']\",\n","        \"ref\": \"Lin, ACL 2004\",\n","        \"credibility\": 9\n","    },\n","    {\n","        \"name\": \"BERTScore\",\n","        \"stage\": \"Generation/semantic match\",\n","        \"formula\": \"BERTScore: P, R, F1 over contextualized word pairs\",\n","        \"python_code\": \"P,R,F = bert_score(pred, ref)\",\n","        \"ref\": \"Zhang et al., ICLR 2020\",\n","        \"credibility\": 8\n","    },\n","    {\n","        \"name\": \"Exact Match\",\n","        \"stage\": \"Generation\",\n","        \"formula\": \"EM = int(pred.strip() == ref.strip())\",\n","        \"python_code\": \"int(pred.strip()==ref.strip())\",\n","        \"ref\": \"Rajpurkar et al., SQuAD (EMNLP 2016)\",\n","        \"credibility\": 9\n","    },\n","    {\n","        \"name\": \"F1 Score\",\n","        \"stage\": \"Generation\",\n","        \"formula\": \"F1 = 2*P*R/(P+R); token overlap harmonic mean\",\n","        \"python_code\": \"2*precision*recall/(precision+recall)\",\n","        \"ref\": \"Rajpurkar et al., SQuAD (EMNLP 2016)\",\n","        \"credibility\": 9\n","    }\n","    # Add others as needed...\n","]\n","metrics_path = CONFIG_DIR / \"metrics_registry.yaml\"\n","with open(metrics_path, \"w\") as f:\n","    yaml.safe_dump(metrics_registry, f, sort_keys=False)\n","\n","# ----------------------------------------------------------------\n","# 5. Print config/metrics for audit trail\n","def print_param(label, value):\n","    print(f\"{label:<20}: {value}\")\n","print(f\"\\n---[ Nurse-AI PaperQA2 Pipeline v5.3: '{PROJECT_NAME}' ]---\")\n","print_param(\"PDF Directory\", PDF_DIR)\n","print_param(\"To AI Dir\", TO_AI_DIR)\n","print_param(\"From AI Dir\", FROM_AI_DIR)\n","print_param(\"Reviewer Dir\", REVIEWER_DIR)\n","print_param(\"Config Dir\", CONFIG_DIR)\n","print_param(\"Metrics Dir\", METRICS_DIR)\n","print_param(\"LLM Model\", config['llm_model'])\n","print_param(\"Embed Model\", config['embedding_model'])\n","print_param(\"Chunk/Overlap\", f\"{config['chunk_tokens']} / {config['chunk_overlap']}\")\n","print_param(\"Random Seed\", config['random_seed'])\n","print_param(\"# Questions\", len(config['questions']))\n","print_param(\"Metrics config\", metrics_path)\n","print(\"---- Example Question(s) ----\")\n","for q in config['questions']:\n","    print(f\" - [{q['slug']}] {q['text'][:80]}{'...' if len(q['text'])>80 else ''}\")\n","print(f\"\\nSystem Prompt:\\n  {config['prompts']['system']}\")\n","print(\"\\nEdit config.yaml (and metrics_registry.yaml) in your Project files folder to update configuration.\\n\")\n","print(\"✔ Block 1 complete. All downstream blocks will use these settings.\\n\")\n","\n","# ----------------------------------------------------------------\n","# 6. Write config snapshot for reproducibility\n","with open(CONFIG_DIR / \"config_snapshot.yaml\", \"w\") as f:\n","    yaml.safe_dump(config, f, sort_keys=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zu7haz5JssXB","executionInfo":{"status":"ok","timestamp":1746337302361,"user_tz":-720,"elapsed":24086,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"78531f97-1530-4ed0-dff6-9fe61f8afe5e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","✔ Google Drive mounted at /content/drive\n","✔ config.yaml created/reset with default contents. Edit file as needed.\n","\n","---[ Nurse-AI PaperQA2 Pipeline v5.3: 'NurseAI_PaperQA2_v5.4' ]---\n","PDF Directory       : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/PDFs\n","To AI Dir           : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Sent\n","From AI Dir         : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Recieved\n","Reviewer Dir        : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Reviewers\n","Config Dir          : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Project files\n","Metrics Dir         : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/outputs/metrics\n","LLM Model           : gpt-4.1\n","Embed Model         : text-embedding-3-large\n","Chunk/Overlap       : 500 / 100\n","Random Seed         : 42\n","# Questions         : 4\n","Metrics config      : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Project files/metrics_registry.yaml\n","---- Example Question(s) ----\n"," - [patient_outcomes] How can AI technologies be utilised to enhance patient outcomes in nursing?\n"," - [nurse_competencies] In what ways can AI tools enhance nursing competencies and professional developm...\n"," - [technical_infrastructure] What specific technical infrastructure is required for effective use of AI in he...\n"," - [ethics_regulation] What are the key ethical and regulatory challenges associated with implementing ...\n","\n","System Prompt:\n","  You are a research assistant extracting information from scientific literature. Answer every question using only the provided context (or say 'Not found').\n","\n","Edit config.yaml (and metrics_registry.yaml) in your Project files folder to update configuration.\n","\n","✔ Block 1 complete. All downstream blocks will use these settings.\n","\n"]}]},{"cell_type":"code","source":["#@title BLOCK 2: Dependency Install & Environment Echo (v5.3.2, 2025-05-04)\n","# Purpose: Install/check all dependencies, print and log environment/Python/library versions, and verify\n","# project folder accessibility. Requirements are snapshot for reproducibility and CI. Reviewer gets clear\n","# feedback if modules or folders are missing. All errors and warnings are explicit.\n","# Changelog:\n","#   - v5.0: Core env echo; basic pip install/print\n","#   - v5.2: Log all deps, support pip and conda tracking, robust error handling\n","#   - v5.3: Adds ROUGE/BERTScore, upgraded feedback/messages for reviewer/PI\n","\n","import sys\n","import subprocess\n","\n","def pip_install(pkgs):\n","    for p in pkgs:\n","        try:\n","            __import__(p)\n","        except ImportError:\n","            print(f\"Installing: {p} ...\")\n","            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", p])\n","\n","# Minimal version pins for reproducibility and metrics support\n","required_pkgs = [\n","    'openai',\n","    'tiktoken',\n","    'pymupdf',\n","    'PyYAML',\n","    'pandas',\n","    'numpy',\n","    'scikit-learn',\n","    'rouge-score',\n","    'bert-score'\n","]\n","pip_install(required_pkgs)\n","\n","# --- Imports (AFTER install) ---\n","import openai\n","from openai import OpenAI\n","import tiktoken\n","import fitz   # PyMuPDF\n","import pandas as pd\n","import numpy as np\n","import yaml\n","from pathlib import Path\n","import os\n","\n","# --- ECHO critical package versions and platform info ---\n","def print_ver(title, mod, attr=\"__version__\"):\n","    try:\n","        v = getattr(mod, attr)\n","    except Exception:\n","        v = \"?\"\n","    print(f\"{title:<15}: {v}\")\n","\n","print(\"\\n[Dependency Versions]\")\n","import platform\n","print(f\"Python           : {platform.python_version()}\")\n","print_ver(\"openai\", openai)\n","print_ver(\"tiktoken\", tiktoken)\n","print_ver(\"pymupdf\", fitz)\n","print_ver(\"PyYAML\", yaml)\n","print_ver(\"pandas\", pd)\n","print_ver(\"numpy\", np)\n","try:\n","    import sklearn\n","    print_ver(\"sklearn\", sklearn)\n","except ImportError:\n","    print(\"scikit-learn    : (NOT INSTALLED)\")\n","try:\n","    import rouge_score\n","    print_ver(\"rouge_score\", rouge_score)\n","except ImportError:\n","    print(\"rouge-score     : (NOT INSTALLED)\")\n","try:\n","    import bert_score\n","    print_ver(\"bert_score\", bert_score)\n","except ImportError:\n","    print(\"bert-score      : (NOT INSTALLED)\")\n","try:\n","    import torch\n","    print_ver(\"torch\", torch)\n","except ImportError:\n","    pass\n","\n","# --- Reload config.yaml for downstream, confirm project folders ---\n","config = None\n","config_path = Path(\"config.yaml\")\n","if not config_path.exists():\n","    raise FileNotFoundError(\"config.yaml missing! Run Block 1 first.\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","\n","dirs = {\n","    \"PDF Directory\": PROJECT_ROOT / \"PDFs\",\n","    \"To AI Dir\": PROJECT_ROOT / \"To AI\",\n","    \"From AI Dir\": PROJECT_ROOT / \"From AI\",\n","    \"Reviewer Dir\": PROJECT_ROOT / \"Reviewers\",\n","    \"Config Dir\": PROJECT_ROOT / \"Project files\",\n","    \"Metrics Dir\": PROJECT_ROOT / \"outputs\" / \"metrics\"\n","}\n","print(\"\\n[Project Directory Structure Check]\")\n","for label, folder in dirs.items():\n","    try:\n","        os.makedirs(folder, exist_ok=True)\n","        assert folder.exists()\n","        print(f\"✔ {label:<17}: {folder}\")\n","    except Exception as e:\n","        print(f\" {label:<17}: {folder} [FAILED: {e}]\")\n","        raise\n","\n","print(\"\\nAll dependencies loaded and folders accessible. Ready for Block 3.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"fUAWG5qEssjf","executionInfo":{"status":"ok","timestamp":1746336856072,"user_tz":-720,"elapsed":140364,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"4ff8ec2f-5337-4774-c47f-67e73babaa51"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing: tiktoken ...\n","Installing: pymupdf ...\n","Installing: PyYAML ...\n","Installing: scikit-learn ...\n","Installing: rouge-score ...\n","Installing: bert-score ...\n","\n","[Dependency Versions]\n","Python           : 3.11.12\n","openai         : 1.76.0\n","tiktoken       : 0.9.0\n","pymupdf        : 1.25.5\n","PyYAML         : 6.0.2\n","pandas         : 2.2.2\n","numpy          : 2.0.2\n","sklearn        : 1.6.1\n","rouge_score    : ?\n","bert_score     : 0.3.12\n","torch          : 2.6.0+cu124\n","\n","[Project Directory Structure Check]\n","✔ PDF Directory    : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/PDFs\n","✔ To AI Dir        : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/To AI\n","✔ From AI Dir      : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/From AI\n","✔ Reviewer Dir     : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Reviewers\n","✔ Config Dir       : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/Project files\n","✔ Metrics Dir      : /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/outputs/metrics\n","\n","All dependencies loaded and folders accessible. Ready for Block 3.\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"qPyfKKgC0XXN","executionInfo":{"status":"ok","timestamp":1746336943464,"user_tz":-720,"elapsed":2635,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"f88cc070-c4b5-47aa-bbb4-1299cd2b9b28"},"outputs":[{"output_type":"stream","name":"stdout","text":["✔ Block 3 loaded: utilities, prompts, paths, and real metrics ready.\n"]}],"source":["#@title BLOCK 3: Universal Utilities, Config, Metadata, Prompt Tools (v5.3.2, 2025-05-04)\n","# Purpose: Create all core utility functions for text cleaning, chunking, prompt generation, output path resolution,\n","# and metric computation. Ensures all methods are config-driven and vettable for downstream blocks.\n","# Changelog:\n","#   - v5.0–v5.1: Initial utilities, docstring improvement\n","#   - v5.2: Metrics utilities, prompt/output refactor\n","#   - v5.3+: Uses robust library calls (e.g. ROUGE, BERTScore, etc.), reviewer-facing warnings and guidance added.\n","\n","import re\n","import yaml\n","import tiktoken\n","from pathlib import Path\n","import fitz\n","import numpy as np\n","\n","# Optional packages for metrics\n","try:\n","    from rouge_score import rouge_scorer\n","    ROUGE_AVAILABLE = True\n","except ImportError:\n","    ROUGE_AVAILABLE = False\n","\n","try:\n","    from bert_score import score as bert_score_fn\n","    BERTSCORE_AVAILABLE = True\n","except ImportError:\n","    BERTSCORE_AVAILABLE = False\n","\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","\n","def get_tokenizer(llm_model=None):\n","    \"\"\"Get tiktoken encoding based on config LLM model name.\"\"\"\n","    if not llm_model:\n","        llm_model = config.get('llm_model', 'gpt-4.1')\n","    try:\n","        enc = tiktoken.encoding_for_model(llm_model)\n","    except Exception:\n","        enc = tiktoken.get_encoding(\"cl100k_base\")\n","    return enc\n","\n","enc = get_tokenizer()\n","\n","def clean_text(text: str) -> str:\n","    \"\"\"\n","    Removes non-printing chars and irregular whitespace, keeps structure.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        Raw text input.\n","    Returns\n","    -------\n","    str\n","        Cleaned text.\n","    \"\"\"\n","    text = text.replace(\"\\f\", \" \")\n","    text = re.sub(r'\\s*\\n\\s*', '\\n', text)\n","    text = re.sub(r'[ \\t]+', ' ', text)\n","    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n","    return text.strip()\n","\n","def split_text_by_tokens(text, chunk_tokens=None, overlap=None, tokenizer=None):\n","    \"\"\"\n","    Splits text by token count with optional overlap for chunking.\n","\n","    Parameters\n","    ----------\n","    text : str\n","        Input text to split.\n","    chunk_tokens : int, optional\n","        Chunk size (tokens).\n","    overlap : int, optional\n","        Number of tokens overlapping between chunks.\n","    tokenizer : callable, optional\n","        Tokenizer/encoder with .encode() and .decode().\n","\n","    Returns\n","    -------\n","    list of str\n","        List of chunked text.\n","    \"\"\"\n","    if chunk_tokens is None:\n","        chunk_tokens = config['chunk_tokens']\n","    if overlap is None:\n","        overlap = config['chunk_overlap']\n","    if tokenizer is None:\n","        tokenizer = enc\n","    tokens = tokenizer.encode(text)\n","    chunks = []\n","    start = 0\n","    while start < len(tokens):\n","        end = min(len(tokens), start + chunk_tokens)\n","        chunk_txt = tokenizer.decode(tokens[start:end])\n","        if chunk_txt.strip():\n","            chunks.append(chunk_txt)\n","        start += chunk_tokens - overlap\n","    return chunks\n","\n","def extract_metadata(pdf_path, raw_text=None, ai_metadata_func=None):\n","    \"\"\"\n","    Metadata extractor using PDF info, then fallbacks if needed.\n","\n","    Parameters\n","    ----------\n","    pdf_path : str or Path\n","        Path to PDF.\n","    raw_text : str, optional\n","        PDF text if already read.\n","    ai_metadata_func : callable, optional\n","        LLM-powered fallback to extract metadata.\n","\n","    Returns\n","    -------\n","    tuple\n","        (metadata dict, source dict)\n","    \"\"\"\n","    try:\n","        doc = fitz.open(pdf_path)\n","        meta = doc.metadata or {}\n","    except Exception:\n","        meta = {}\n","    found = {\"title\": meta.get(\"title\"), \"author\": meta.get(\"author\"), \"year\": meta.get(\"year\")}\n","    source = {}\n","    # Heuristic fallback\n","    if not found['title'] or not found['author']:\n","        if raw_text is None:\n","            try:\n","                doc = fitz.open(pdf_path)\n","                raw_text = \"\\n\".join(page.get_text() for page in doc)\n","            except Exception:\n","                raw_text = \"\"\n","        candidates = raw_text[:1500].split('\\n')\n","        h_title, h_author, h_year = \"\", \"\", \"\"\n","        for line in candidates:\n","            line_clean = line.strip()\n","            if not h_title and ((line_clean.isupper() and 20 < len(line_clean) < 100) or len(line_clean.split()) > 6):\n","                h_title = line_clean\n","            if not h_author and re.match(r'^[A-Z][A-Za-z,\\s.]+$', line_clean) and 2 < len(line_clean.split()) < 7:\n","                h_author = line_clean\n","            if not h_year:\n","                found_year = re.search(r'(19|20)\\d{2}', line_clean)\n","                if found_year:\n","                    h_year = found_year.group(0)\n","        if h_title: found['title'] = h_title\n","        if h_author: found['author'] = h_author\n","        if h_year: found['year'] = h_year\n","        source['used_heuristic'] = True\n","    else:\n","        source['pdf_metadata'] = True\n","    # AI/LLM fallback\n","    if (not found['title'] or not found['author']) and ai_metadata_func:\n","        ai_meta = ai_metadata_func(raw_text or \"\")\n","        for key in ['title', 'author', 'year']:\n","            if not found[key] and ai_meta.get(key):\n","                found[key] = ai_meta.get(key)\n","        source['ai_fallback'] = True\n","    return found, source\n","\n","def build_prompt(question_dict, context, config=config, as_json=False):\n","    \"\"\"\n","    Constructs system and user prompt from config.\n","\n","    Parameters\n","    ----------\n","    question_dict : dict\n","        Contains 'text' as research question.\n","    context : str\n","        Chosen context chunk.\n","    config : dict\n","        Loaded config.\n","    as_json : bool\n","        Output prompt for JSON output.\n","    Returns\n","    -------\n","    tuple\n","        (system_prompt, user_prompt)\n","    \"\"\"\n","    system_prompt = config['prompts']['system']\n","    if as_json:\n","        user_prompt = (\n","            f\"Extract a JSON object with fields required by the research team from the snippet.\\n\"\n","            f\"Context:\\n{context}\\n\\n\"\n","            f\"If a field is missing, output empty string. Output valid JSON only.\"\n","        )\n","    else:\n","        user_template = config['prompts'].get('user_template')\n","        user_prompt = user_template.format(\n","            question=question_dict['text'],\n","            context=context\n","        )\n","    return system_prompt, user_prompt\n","\n","def get_output_path(kind, question_slug=None, ext=\"jsonl\"):\n","    \"\"\"\n","    Returns project output paths for defined kind.\n","    \"\"\"\n","    project_root = Path(\"/content/drive/MyDrive\") / config['project_name']\n","    if kind == \"mini_chunks\":\n","        return project_root / \"To AI\" / f\"mini_chunks_{config['chunk_tokens']}.jsonl\"\n","    elif kind == \"embedding\":\n","        return project_root / \"To AI\" / f\"embeddings_{config['chunk_tokens']}.pkl\"\n","    elif kind == \"batch\" and question_slug:\n","        return project_root / \"To AI\" / f\"{question_slug}_batch_{config['chunk_tokens']}.jsonl\"\n","    elif kind == \"qa_output\" and question_slug:\n","        return project_root / \"From AI\" / f\"{question_slug}_qa_output.{ext}\"\n","    elif kind == \"reviewer_export\" and question_slug:\n","        return project_root / \"Reviewers\" / f\"{question_slug}_reviewer_output.{ext}\"\n","    elif kind == \"summary\" and question_slug:\n","        return project_root / \"Reviewers\" / f\"{question_slug}_summary.{ext}\"\n","    elif kind == \"metadata\":\n","        return project_root / \"Project files\" / f\"metadata_all_pdfs.csv\"\n","    elif kind == \"retrieval_metrics\":\n","        return project_root / \"Project files\" / \"retrieval_metrics.csv\"\n","    elif kind == \"generation_metrics\":\n","        return project_root / \"Project files\" / \"generation_metrics.csv\"\n","    elif kind == \"panel_metrics\":\n","        return project_root / \"Project files\" / \"panel_metrics.csv\"\n","    else:\n","        return project_root / f\"unknown_{kind}.{ext}\"\n","\n","# --- METRIC UTILITIES (now REAL) ---\n","def precision_at_k(relevant, retrieved, k):\n","    \"\"\"\n","    Precision@k = (# relevant in top k) / k\n","    \"\"\"\n","    top_k = retrieved[:k]\n","    n_rel = sum([doc in relevant for doc in top_k])\n","    return n_rel / k if k > 0 else 0.0\n","\n","def recall_at_k(relevant, retrieved, k):\n","    \"\"\"\n","    Recall@k = (# relevant in top k) / total relevant\n","    \"\"\"\n","    top_k = retrieved[:k]\n","    n_rel = sum([doc in relevant for doc in top_k])\n","    return n_rel / len(relevant) if len(relevant) > 0 else 0.0\n","\n","def mean_reciprocal_rank(relevance_lists):\n","    \"\"\"\n","    Mean Reciprocal Rank over multiple queries.\n","    \"\"\"\n","    ranks = []\n","    for rels in relevance_lists:\n","        try:\n","            rank = rels.index(True) + 1\n","            ranks.append(1 / rank)\n","        except ValueError:\n","            ranks.append(0.0)\n","    return np.mean(ranks) if ranks else 0.0\n","\n","def ndcg_at_k(relevance, k):\n","    \"\"\"\n","    Normalized Discounted Cumulative Gain at k.\n","    \"\"\"\n","    dcg = sum([rel / np.log2(idx+2) for idx, rel in enumerate(relevance[:k])])\n","    ideal = sorted(relevance, reverse=True)\n","    idcg = sum([rel / np.log2(idx+2) for idx, rel in enumerate(ideal[:k])])\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def cosine_similarity(v1, v2):\n","    \"\"\"\n","    Computes cosine similarity between two vectors.\n","    \"\"\"\n","    v1 = np.asarray(v1)\n","    v2 = np.asarray(v2)\n","    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n","        return 0.0\n","    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","def exact_match(prediction, reference):\n","    \"\"\"\n","    Exact match for QA tasks.\n","    \"\"\"\n","    return int(prediction.strip() == reference.strip())\n","\n","def f1_score(prediction, reference):\n","    \"\"\"\n","    F1: token overlap harmonic mean (for QA).\n","    \"\"\"\n","    pred_tokens = prediction.strip().split()\n","    ref_tokens = reference.strip().split()\n","    common = set(pred_tokens) & set(ref_tokens)\n","    if not common:\n","        return 0.0\n","    precision = len(common) / len(pred_tokens)\n","    recall = len(common) / len(ref_tokens)\n","    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n","\n","def rouge_l(prediction, reference):\n","    \"\"\"\n","    Computes ROUGE-L score between prediction and reference.\n","\n","    Returns\n","    -------\n","    float\n","        ROUGE-L F1 score (or 0.0 if unavailable).\n","    \"\"\"\n","    if not ROUGE_AVAILABLE:\n","        return 0.0\n","    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    scores = scorer.score(reference, prediction)\n","    return scores[\"rougeL\"].fmeasure if \"rougeL\" in scores else 0.0\n","\n","def bert_score(prediction, reference):\n","    \"\"\"\n","    Computes BERTScore F1 between prediction and reference.\n","\n","    Returns\n","    -------\n","    float\n","        BERTScore F1 (or 0.0 if unavailable).\n","    \"\"\"\n","    if not BERTSCORE_AVAILABLE:\n","        return 0.0\n","    P, R, F1 = bert_score_fn([prediction], [reference], lang='en', rescale_with_baseline=True)\n","    return float(F1[0])\n","\n","print(\"✔ Block 3 loaded: utilities, prompts, paths, and real metrics ready.\")\n","if not ROUGE_AVAILABLE:\n","    print(\"⚠️ [Warning] rouge_score not installed; ROUGE-L metric will be zero. Install with 'pip install rouge-score'.\")\n","if not BERTSCORE_AVAILABLE:\n","    print(\"⚠️ [Warning] bert-score not installed; BERTScore metric will be zero. Install with 'pip install bert-score'.\")"]},{"cell_type":"code","source":["#@title BLOCK 4: PDF Ingestion, Cleaning, Metadata Extraction (v5.3.2, 2025-05-04)\n","# Purpose: Extract, clean, and repair PDF text/metadata from all source files, logging errors and provenance for audit.\n","# All cleaned texts and metadata tables are output to disk and dashboard for reviewer usability and drift checks.\n","# Changelog:\n","#   - v5.0: Basic PDF clean/extract\n","#   - v5.1: Improved error/metadata logic\n","#   - v5.2: Robust error logging, provenance export to metrics\n","#   - v5.3+: Reviewer alerts, no silent/skipped errors, all outputs metrics-ready\n","\n","import json\n","import pandas as pd\n","from pathlib import Path\n","import yaml\n","from tqdm import tqdm\n","import os\n","\n","def ingest_clean_pdfs(pdf_dir, output_jsonl, metadata_csv, error_log_path):\n","    \"\"\"\n","    Loads, cleans, and extracts metadata from all PDFs in a directory.\n","    Writes cleaned text to JSONL, and metadata/errors to CSV/log files.\n","\n","    Parameters\n","    ----------\n","    pdf_dir : Path\n","        Directory containing PDF files.\n","    output_jsonl : Path\n","        Where to write text chunks as jsonl.\n","    metadata_csv : Path\n","        Where to write the metadata as CSV.\n","    error_log_path : Path\n","        Where to log errors.\n","    Returns\n","    -------\n","    all_metadata : list of dicts\n","        Collected metadata for summary.\n","    n_ok : int\n","        Number of successful extractions.\n","    n_failed : int\n","        Number of failures.\n","    \"\"\"\n","    all_metadata = []\n","    ok, failed = 0, 0\n","    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n","    if not pdf_files:\n","        raise FileNotFoundError(f\"[ABORT] No PDFs found in {pdf_dir}. Please upload files before running ingestion.\")\n","\n","    with open(output_jsonl, \"w\", encoding=\"utf-8\") as jsonl_out, \\\n","         open(error_log_path, \"w\", encoding=\"utf-8\") as err_out:\n","        for pdf_path in tqdm(pdf_files, desc=\"Ingesting PDFs...\"):\n","            try:\n","                import fitz\n","                doc = fitz.open(pdf_path)\n","                full_text = \"\\n\".join(page.get_text() for page in doc)\n","                cleaned = clean_text(full_text)\n","                md, md_source = extract_metadata(pdf_path, raw_text=full_text)\n","                all_metadata.append({'file': pdf_path.name, **md, 'metadata_source': str(md_source)})\n","                json.dump({\n","                    'file_name': pdf_path.name,\n","                    'file_path': str(pdf_path),\n","                    'text': cleaned,\n","                    'num_pages': doc.page_count,\n","                    'metadata': md\n","                }, jsonl_out, ensure_ascii=False)\n","                jsonl_out.write(\"\\n\")\n","                ok += 1\n","            except Exception as e:\n","                err_msg = f\"Error [{pdf_path.name}]: {repr(e)}\"\n","                print(err_msg)\n","                err_out.write(json.dumps({'file': pdf_path.name, 'error': str(e)}) + \"\\n\")\n","                failed += 1\n","    meta_df = pd.DataFrame(all_metadata)\n","    meta_df.to_csv(metadata_csv, index=False)\n","    return all_metadata, ok, failed\n","\n","# ---- MAIN PROCEDURE ----\n","\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","PDF_DIR = PROJECT_ROOT /  \"PDFs\"\n","TO_AI_DIR = PROJECT_ROOT / \"To AI\"\n","CONFIG_DIR = PROJECT_ROOT / \"Project files\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","os.makedirs(METRICS_DIR, exist_ok=True)\n","\n","output_jsonl = TO_AI_DIR / f\"mini_chunks_{config['chunk_tokens']}.jsonl\"\n","metadata_csv = TO_AI_DIR / \"metadata_all_pdfs.csv\"   # canonical location\n","error_log_path = CONFIG_DIR / \"pdf_errors.log\"\n","\n","print(f\"\\nScanning for PDFs in: {PDF_DIR}\")\n","\n","all_metadata, ok, failed = ingest_clean_pdfs(PDF_DIR, output_jsonl, metadata_csv, error_log_path)\n","\n","# Export metadata/provenance to metrics dir for dashboard use\n","meta_df = pd.DataFrame(all_metadata)\n","meta_df.to_csv(METRICS_DIR / \"metadata_all_pdfs.csv\", index=False)\n","\n","# Block summary printout (reviewer-friendly)\n","print(f\"\\n✔ Block 4 complete: {ok} PDFs processed, {failed} failed.\")\n","print(f\"  Cleaned text written to: {output_jsonl}\")\n","print(f\"  Metadata/provenance table: {metadata_csv}\")\n","if failed:\n","    print(f\"  Errors logged to: {error_log_path}\")\n","    err_lines = 0\n","    with open(error_log_path, \"r\", encoding=\"utf-8\") as errf:\n","        for _ in errf:\n","            err_lines += 1\n","    print(f\"  [Reviewer: Check {error_log_path} for {err_lines} error entries]\")\n","else:\n","    print(\"  No extraction errors detected.\")\n","\n","print(f\"\\n  {len(all_metadata)} metadata records written (see also {METRICS_DIR/'metadata_all_pdfs.csv'}).\")\n","print(\"  Reviewer: If any PDFs missing or errors noted in the log, recheck file integrity or OCR.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"id":"hOfCPaQq6YeE","executionInfo":{"status":"error","timestamp":1746337008182,"user_tz":-720,"elapsed":103,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"7413f956-09af-43d1-b53c-fa5d55aa2940"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Scanning for PDFs in: /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/PDFs\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[ABORT] No PDFs found in /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/PDFs. Please upload files before running ingestion.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-dc3591a14aed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nScanning for PDFs in: {PDF_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mall_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_clean_pdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDF_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_jsonl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_log_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Export metadata/provenance to metrics dir for dashboard use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-dc3591a14aed>\u001b[0m in \u001b[0;36mingest_clean_pdfs\u001b[0;34m(pdf_dir, output_jsonl, metadata_csv, error_log_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpdf_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpdf_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[ABORT] No PDFs found in {pdf_dir}. Please upload files before running ingestion.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_jsonl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjsonl_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [ABORT] No PDFs found in /content/drive/MyDrive/NurseAI_PaperQA2_v5.4/PDFs. Please upload files before running ingestion."]}]},{"cell_type":"code","source":["#@title BLOCK 5: Token-wise Chunking (Config-driven, v5.3.2, 2025-05-04)\n","# Purpose: Breaks cleaned PDF text into config-rooted, token-overlapping chunks for RAG. Logs all chunk/cardinality data,\n","# flags token overflows for reviewer, and writes out all stats/alerts for drift/audit.\n","# Changelog:\n","#   - v5.0: Basic chunker\n","#   - v5.1: Reviewer alerts for error/overlap\n","#   - v5.2: Cardinality/stat output export, config on all parameters\n","#   - v5.3+: Reviewer flags, overflow logging, full provenance in output\n","\n","import json\n","from pathlib import Path\n","from tqdm import tqdm\n","import yaml\n","import pandas as pd\n","import os\n","\n","def chunk_and_log(in_jsonl, out_jsonl, cardinality_csv, chunk_tokens, overflow_limit=8191):\n","    \"\"\"\n","    Splits cleaned texts into token-sized chunks, writes all chunk info as JSONL, and logs per-document stats.\n","    Alerts reviewer if any chunk overflows embedding model window.\n","\n","    Parameters\n","    ----------\n","    in_jsonl : Path\n","        Input file, one json per document (from Block 4).\n","    out_jsonl : Path\n","        Output file for mini-chunks, one json per chunk.\n","    cardinality_csv : Path\n","        Output CSV file logging document-to-chunk mapping.\n","    chunk_tokens : int\n","        Chunk size in tokens.\n","    overflow_limit : int\n","        Maximum chunk length allowed for downstream embedding model.\n","\n","    Returns\n","    -------\n","    n_docs : int\n","        Number of documents processed.\n","    n_chunks : int\n","        Number of chunks written.\n","    overflow_chunks : int\n","        Number of chunks exceeding overflow_limit.\n","    \"\"\"\n","    n_docs = 0\n","    n_chunks = 0\n","    doc_cardinalities = []\n","    chunk_lengths = []\n","    chunks_written = []\n","    with open(in_jsonl, \"r\", encoding=\"utf-8\") as in_f, \\\n","         open(out_jsonl, \"w\", encoding=\"utf-8\") as out_f:\n","        for line in tqdm(in_f, desc=\"Chunking cleaned texts\"):\n","            doc = json.loads(line)\n","            doc_text = doc['text']\n","            file_name = doc['file_name']\n","            metadata = doc.get('metadata', {})\n","            chunks = split_text_by_tokens(doc_text)\n","            doc_cardinalities.append({'file': file_name, 'n_chunks': len(chunks)})\n","            for idx, chunk in enumerate(chunks):\n","                num_tokens = len(enc.encode(chunk))\n","                entry = {\n","                    \"file_name\": file_name,\n","                    \"chunk_id\": idx,\n","                    \"chunk_type\": \"mini\",\n","                    \"text\": chunk,\n","                    \"num_tokens\": num_tokens,\n","                    \"num_chunks_in_doc\": len(chunks),\n","                    \"metadata\": metadata,\n","                    \"split_method\": f\"tokens_{chunk_tokens}_overlap_{config['chunk_overlap']}\"\n","                }\n","                json.dump(entry, out_f, ensure_ascii=False)\n","                out_f.write(\"\\n\")\n","                n_chunks += 1\n","                chunk_lengths.append(num_tokens)\n","                chunks_written.append(entry)\n","            n_docs += 1\n","\n","    pd.DataFrame(doc_cardinalities).to_csv(cardinality_csv, index=False)\n","    overflow_chunks = sum([cl > overflow_limit for cl in chunk_lengths])\n","    return n_docs, n_chunks, overflow_chunks, chunk_lengths, chunks_written\n","\n","# --- MAIN PROCEDURE ---\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","TO_AI_DIR = PROJECT_ROOT / \"To AI\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","\n","input_jsonl = TO_AI_DIR / f\"mini_chunks_{config['chunk_tokens']}.jsonl\"\n","output_jsonl = TO_AI_DIR / f\"mini_chunks_{config['chunk_tokens']}_tokenized.jsonl\"\n","chunk_cardinality_path = METRICS_DIR / f\"chunk_cardinality_{config['chunk_tokens']}.csv\"\n","os.makedirs(METRICS_DIR, exist_ok=True)\n","\n","n_docs, n_chunks, overflow_chunks, chunk_lengths, chunks_written = chunk_and_log(\n","    in_jsonl=input_jsonl,\n","    out_jsonl=output_jsonl,\n","    cardinality_csv=chunk_cardinality_path,\n","    chunk_tokens=config['chunk_tokens'],\n","    overflow_limit=8191\n",")\n","\n","# Summary stats (printed and written out for review)\n","chunk_stats = {\n","    \"N_docs\": n_docs,\n","    \"N_chunks\": n_chunks,\n","    \"Chunk_size_min\": int(min(chunk_lengths)) if chunk_lengths else 0,\n","    \"Chunk_size_max\": int(max(chunk_lengths)) if chunk_lengths else 0,\n","    \"Chunk_size_mean\": round(float(sum(chunk_lengths) / len(chunk_lengths)),2) if chunk_lengths else 0,\n","    \"Overflows (>8191)\": int(overflow_chunks)\n","}\n","chunk_stats_csv = METRICS_DIR / \"chunk_size_stats.csv\"\n","pd.DataFrame([chunk_stats]).to_csv(chunk_stats_csv, index=False)\n","\n","print(f\"\\n✔ Block 5 complete: {n_docs} docs chunked into {n_chunks} mini-chunks.\")\n","print(f\"  Output chunks: {output_jsonl}\")\n","print(f\"  Chunk cardinality CSV: {chunk_cardinality_path}\")\n","print(f\"  Chunk size stats: {chunk_stats_csv}\")\n","if overflow_chunks > 0:\n","    print(f\"  ⚠️ Reviewer: {overflow_chunks} chunk(s) exceed embedding model's 8191-token limit—see CSV and check output_jsonl for affected chunks.\")\n","    # Optionally, save overflows for review\n","    of_chunks = [c for c in chunks_written if c[\"num_tokens\"] > 8191]\n","    if of_chunks:\n","        oversamp_fn = METRICS_DIR / f\"overflowed_chunks_{config['chunk_tokens']}.jsonl\"\n","        with open(oversamp_fn, \"w\", encoding=\"utf-8\") as overflow_f:\n","            for c in of_chunks:\n","                json.dump(c, overflow_f)\n","                overflow_f.write(\"\\n\")\n","        print(f\"  Overflow chunk details: {oversamp_fn}\")\n","else:\n","    print(\"  No chunk overflows for embedding found.\")\n","print(\"  Reviewer: If outlier chunk sizes are reported above, consider reviewing the associated PDFs for OCR or formatting errors.\")\n","\n","# [END]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"PhzpxE3tstCN","executionInfo":{"status":"ok","timestamp":1746324024352,"user_tz":-720,"elapsed":432,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"3ca84f6b-2b84-43df-9ab1-abe05774db62"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Chunking cleaned texts: 20it [00:00, 50.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✔ Block 5 complete: 20 docs chunked into 533 mini-chunks.\n","  Output chunks: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/mini_chunks_500_tokenized.jsonl\n","  Chunk cardinality CSV: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/chunk_cardinality_500.csv\n","  Chunk size stats: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/chunk_size_stats.csv\n","  No chunk overflows for embedding found.\n","  Reviewer: If outlier chunk sizes are reported above, consider reviewing the associated PDFs for OCR or formatting errors.\n"]}]},{"cell_type":"code","source":["#@title BLOCK 6: Embedding All Chunks with Caching (v5.3.2, 2025-05-04)\n","# Purpose: Embed all text chunks using OpenAI (or chosen) embeddings, cache for speed, and export metrics (cosine drift, error/skips)\n","# for reviewer dashboards. All errors/skips are logged and reviewer flagged.\n","# Changelog:\n","#   - v5.0: Initial embedding/caching\n","#   - v5.2: Strong provenance, config-based, error stats\n","#   - v5.3+: Cosine mean/sd reporting, reviewer alerts for drift/skips, all caches versioned\n","\n","import pickle\n","import time\n","import yaml\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def embed_all_chunks_with_stats(\n","    chunk_jsonl, embeddings_pkl, error_log, embedding_model, enc, max_tokens=8191\n","):\n","    \"\"\"\n","    Embeds all mini-chunks using an OpenAI embedding model (caching results).\n","    Skips/token-overflowed chunks, logs errors and stats for reviewer.\n","    ALWAYS returns (chunk_list, skips, errors, drift_stats).\n","    If loaded from cache, skips/errors are set to 0, drift_stats is recomputed if possible.\n","\n","    Parameters\n","    ----------\n","    chunk_jsonl : Path\n","        Input JSONL of text chunks.\n","    embeddings_pkl : Path\n","        Output cache file.\n","    error_log : Path\n","        Error log path.\n","    embedding_model : str\n","        Which embedding model to use (eg text-embedding-3-large).\n","    enc : tiktoken.Encoding\n","        Tokenizer for chunk-length checking.\n","    max_tokens : int\n","        Maximum tokens allowed in chunk for embedding API.\n","\n","    Returns\n","    -------\n","    chunk_list : list of dict\n","        All chunk dictionaries with embeddings, errors, and provenance.\n","    skips : int\n","        Number of skipped (oversize) chunks.\n","    errors : int\n","        Number of chunk embedding errors.\n","    drift_stats : dict\n","        Cosine mean/sd/count drift stats (empty dict if none computable).\n","    \"\"\"\n","\n","    if embeddings_pkl.exists():\n","        print(f\"✔ Loading cached embeddings from: {embeddings_pkl}\")\n","        with open(embeddings_pkl, \"rb\") as f:\n","            chunk_list = pickle.load(f)\n","        # Recompute drift_stats from embeddings if possible\n","        all_embeddings = [c['embedding'] for c in chunk_list if c.get('embedding') is not None]\n","        drift_stats = {}\n","        if all_embeddings:\n","            mean_emb = np.mean(np.stack(all_embeddings), axis=0)\n","            cos_sims = np.array([\n","                np.dot(e, mean_emb)/(np.linalg.norm(e)*np.linalg.norm(mean_emb))\n","                for e in all_embeddings\n","            ])\n","            drift_stats = {\n","                'cosine_mean': float(np.mean(cos_sims)),\n","                'cosine_sd'  : float(np.std(cos_sims)),\n","                'count'      : int(len(cos_sims)),\n","            }\n","        return chunk_list, 0, 0, drift_stats\n","\n","    try:\n","        from google.colab import userdata\n","        openai_api_key = userdata.get(config['openai_key_envvar'])\n","    except ImportError:\n","        openai_api_key = os.environ.get(config['openai_key_envvar'])\n","    if not openai_api_key:\n","        raise EnvironmentError(\n","            f\"OpenAI API key not found! Set {config['openai_key_envvar']} via Colab secrets or environment.\"\n","        )\n","    from openai import OpenAI\n","    client = OpenAI(api_key=openai_api_key)\n","\n","    chunk_list = []\n","    errors, skips = 0, 0\n","    all_embeddings = []\n","    with open(chunk_jsonl, \"r\", encoding=\"utf-8\") as in_f, \\\n","         open(error_log, \"w\", encoding=\"utf-8\") as err_log:\n","        for line in tqdm(in_f, desc=\"Embedding\"):\n","            chunk = json.loads(line)\n","            toks = enc.encode(chunk['text'])\n","            if len(toks) > max_tokens:\n","                print(f\" Skipping chunk (too long): {chunk['file_name']} id {chunk['chunk_id']} tokens={len(toks)}\")\n","                chunk['embedding'] = None\n","                chunk['embedding_error'] = \"too_long\"\n","                err_log.write(json.dumps({'file': chunk['file_name'], 'chunk_id': chunk['chunk_id'], 'reason': 'too_long'}) + \"\\n\")\n","                skips += 1\n","                chunk_list.append(chunk)\n","                continue\n","            try:\n","                resp = client.embeddings.create(model=embedding_model, input=[chunk['text']])\n","                chunk['embedding'] = resp.data[0].embedding\n","                all_embeddings.append(chunk['embedding'])\n","            except Exception as e:\n","                chunk['embedding'] = None\n","                chunk['embedding_error'] = str(e)\n","                err_log.write(json.dumps({'file': chunk['file_name'], 'chunk_id': chunk['chunk_id'], 'error': str(e)}) + \"\\n\")\n","                errors += 1\n","            chunk_list.append(chunk)\n","\n","    with open(embeddings_pkl, \"wb\") as f:\n","        pickle.dump(chunk_list, f)\n","\n","    # --- Output embedding stats for dashboard ---\n","    drift_stats = {}\n","    if all_embeddings:\n","        mean_emb = np.mean(np.stack(all_embeddings), axis=0)\n","        cos_sims = np.array([\n","            np.dot(e, mean_emb)/(np.linalg.norm(e)*np.linalg.norm(mean_emb))\n","            for e in all_embeddings\n","        ])\n","        drift_stats = {\n","            'cosine_mean': float(np.mean(cos_sims)),\n","            'cosine_sd'  : float(np.std(cos_sims)),\n","            'count'      : int(len(cos_sims)),\n","        }\n","    return chunk_list, skips, errors, drift_stats\n","\n","# ---- MAIN PROCEDURE ----\n","\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","TO_AI_DIR = PROJECT_ROOT / \"To AI\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","os.makedirs(METRICS_DIR, exist_ok=True)\n","\n","mini_chunks_jsonl = TO_AI_DIR / f\"mini_chunks_{config['chunk_tokens']}_tokenized.jsonl\"\n","embeddings_pkl = TO_AI_DIR / f\"embeddings_{config['chunk_tokens']}.pkl\"\n","error_log = METRICS_DIR / \"embed_errors.log\"\n","embedding_model = config['embedding_model']\n","\n","chunk_list, skips, errors, drift_stats = embed_all_chunks_with_stats(\n","    chunk_jsonl=mini_chunks_jsonl,\n","    embeddings_pkl=embeddings_pkl,\n","    error_log=error_log,\n","    embedding_model=embedding_model,\n","    enc=enc,\n","    max_tokens=8191\n",")\n","\n","# Save cosine drift stats to CSV\n","cosine_csv = METRICS_DIR / \"embedding_cosine_stats.csv\"\n","if drift_stats:\n","    pd.DataFrame([drift_stats]).to_csv(cosine_csv, index=False)\n","\n","# Reviewer-friendly printout\n","print(f\"\\n✔ Block 6 complete: {len(chunk_list)} chunks processed. {skips} skipped (too long), {errors} errors.\")\n","print(f\"  Embeddings cache: {embeddings_pkl}\")\n","print(f\"  Error log: {error_log}\")\n","if drift_stats:\n","    print(f\"  Embedding cosine stats: mean={drift_stats['cosine_mean']:.4f}, sd={drift_stats['cosine_sd']:.4f}, n={drift_stats['count']}\")\n","    print(f\"  Cosine drift stats CSV: {cosine_csv}\")\n","    if drift_stats['cosine_mean'] < 0.5:\n","        print(\"  ⚠️ Reviewer: Embedding mean-cosine similarity is low (<0.5). Check chunk quality and tokenization.\")\n","else:\n","    print(\"  No embedding stats saved (no valid embeddings found).\")\n","\n","if skips > 0 or errors > 0:\n","    print(f\"  ⚠️ Reviewer: {skips} chunks skipped (oversized), {errors} errors. See {error_log} for details.\")\n","else:\n","    print(\"  All chunks embedded successfully.\")\n","\n","# [END]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"govD3AuEstHJ","executionInfo":{"status":"ok","timestamp":1746324209987,"user_tz":-720,"elapsed":181888,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"47988374-6d1e-4327-9ecc-c10a4290dcfb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Embedding: 533it [03:00,  2.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✔ Block 6 complete: 533 chunks processed. 0 skipped (too long), 0 errors.\n","  Embeddings cache: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/embeddings_500.pkl\n","  Error log: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/embed_errors.log\n","  Embedding cosine stats: mean=0.6519, sd=0.0893, n=533\n","  Cosine drift stats CSV: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/embedding_cosine_stats.csv\n","  All chunks embedded successfully.\n"]}]},{"cell_type":"code","source":["#@title BLOCK 7: Multi-Question Batch Builder (Diverse RAG, v5.3.2, 2025-05-04)\n","# Purpose: Build mini-chunk RAG batches for all questions, using real question embeddings,\n","# with full metrics and provenance in outputs for drift and audit.\n","# Changelog:\n","#   - v5.1: Single-question batch\n","#   - v5.2: Multi-question, full provenance\n","#   - v5.3+: Real OpenAI question embeddings, diversity/enrichment, automatic metric export, reviewer flagging for low overlap\n","\n","import numpy as np\n","import pickle\n","import json\n","import yaml\n","import pandas as pd\n","from pathlib import Path\n","from tqdm import tqdm\n","import os\n","\n","# --- Load config, paths, and all embedded chunks ---\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","TO_AI_DIR = PROJECT_ROOT / \"To AI\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","embeddings_pkl = TO_AI_DIR / f\"embeddings_{config['chunk_tokens']}.pkl\"\n","out_batch_template = TO_AI_DIR / \"{qslug}_batch_{chunk_tokens}.jsonl\"\n","chunk_tokens = config['chunk_tokens']\n","TOP_N = 1000         # Max total retrieved per question\n","MAX_CHUNKS_PER_PAPER = 10\n","TOKEN_LIMIT = 1_000_000\n","\n","with open(embeddings_pkl, \"rb\") as f:\n","    chunk_list = pickle.load(f)\n","chunk_embeddings = [c['embedding'] for c in chunk_list]\n","all_file_names = [c['file_name'] for c in chunk_list]\n","all_chunk_ids = [c['chunk_id'] for c in chunk_list]\n","\n","# --- Metric utilities imported from Block 3 ---\n","# precision_at_k, recall_at_k, mean_reciprocal_rank, ndcg_at_k, cosine_similarity\n","\n","# --- ACTUAL QUESTION EMBEDDING FUNCTION ---\n","from openai import OpenAI\n","import os\n","try:\n","    from google.colab import userdata\n","    openai_api_key = userdata.get(config['openai_key_envvar'])\n","except ImportError:\n","    openai_api_key = os.environ.get(config['openai_key_envvar'])\n","if not openai_api_key:\n","    raise EnvironmentError(f\"OpenAI API key not found! Set {config['openai_key_envvar']} via Colab secrets or environment.\")\n","\n","client = OpenAI(api_key=openai_api_key)\n","embedding_model = config['embedding_model']\n","\n","def get_question_embedding(question_text, model=embedding_model):\n","    \"\"\"\n","    Embeds the question using the specified embedding model.\n","    Returns a vector.\n","    \"\"\"\n","    # OpenAI API: batch of 1\n","    resp = client.embeddings.create(model=model, input=[question_text])\n","    return resp.data[0].embedding\n","\n","def build_rag_batch_and_metrics():\n","    \"\"\"\n","    For each question:\n","    - Embeds question,\n","    - Scores all chunks by cosine similarity,\n","    - Assembles diverse batch (cap per paper, total tokens),\n","    - Computes retrieval metrics,\n","    - Saves batch and metrics,\n","    - Prints summary/alerts for reviewers.\n","    \"\"\"\n","    retrieval_metrics = []\n","    print(f\"\\nStarting multi-question batch build for {len(config['questions'])} research questions:\")\n","    for qdict in config['questions']:\n","        question = qdict['text']\n","        qslug = qdict['slug']\n","        print(f\"\\n— [{qslug}] {question[:60]}...\")\n","\n","        # 1. Embed the question   ==================\n","        q_emb = get_question_embedding(question)\n","\n","        # 2. Score all chunk embeddings vs question\n","        scored = [(i, cosine_similarity(q_emb, chunk_emb)) for i, chunk_emb in enumerate(chunk_embeddings)]\n","        scored.sort(key=lambda x: x[1], reverse=True)  # Descending\n","\n","        # 3. Build batch (strict diversity cap, total tokens) ===\n","        per_paper = {}\n","        batch_chunks = []\n","        sims_for_batch = []\n","        total_tokens = 0\n","        included_papers = set()\n","\n","        for idx, sim in scored:\n","            chunk = chunk_list[idx]\n","            paper = chunk[\"file_name\"]\n","            per_paper.setdefault(paper, 0)\n","            if per_paper[paper] >= MAX_CHUNKS_PER_PAPER:\n","                continue\n","            if total_tokens + chunk[\"num_tokens\"] > TOKEN_LIMIT:\n","                break\n","            entry = {\n","                \"question_slug\": qslug,\n","                \"question\": question,\n","                \"file_name\": paper,\n","                \"chunk_id\": chunk[\"chunk_id\"],\n","                \"similarity\": float(sim),\n","                \"num_tokens\": chunk[\"num_tokens\"],\n","                \"text\": chunk[\"text\"],\n","                \"metadata\": chunk.get(\"metadata\", {})\n","            }\n","            batch_chunks.append(entry)\n","            sims_for_batch.append(sim)\n","            per_paper[paper] += 1\n","            included_papers.add(paper)\n","            total_tokens += chunk[\"num_tokens\"]\n","            if len(batch_chunks) >= TOP_N:\n","                break\n","\n","        # 4. Save batch for this question\n","        outbatch_path = Path(str(out_batch_template).format(qslug=qslug, chunk_tokens=chunk_tokens))\n","        with open(outbatch_path, \"w\", encoding=\"utf-8\") as out_f:\n","            for chunk in batch_chunks:\n","                json.dump(chunk, out_f, ensure_ascii=False)\n","                out_f.write(\"\\n\")\n","        print(f\"  Batch built: {len(batch_chunks)} chunks | {len(included_papers)} papers\")\n","        print(f\"  Output:      {outbatch_path}\")\n","\n","        # 5. Simulate relevant set for now: all selected papers count as \"relevant\" as placeholder.\n","        # To use a gold set: relevant_set = set(gold_answers[qslug]), ... (needs reviewer input).\n","        top_k_docs = [c[\"file_name\"] for c in batch_chunks[:5]]\n","        relevant_set = set([c[\"file_name\"] for c in batch_chunks])  # All in batch, placeholder\n","\n","        # 6. Compute metrics (report on possible reviewer gold once available)\n","        p_at5 = precision_at_k(relevant_set, top_k_docs, 5)\n","        rec_at5 = recall_at_k(relevant_set, top_k_docs, 5)\n","        mrr = mean_reciprocal_rank([[doc in relevant_set for doc in top_k_docs]])\n","        ndcg = ndcg_at_k([1]*min(5, len(top_k_docs)), 5)\n","        mean_sim = np.mean(sims_for_batch) if sims_for_batch else 0.0\n","        retrieval_metrics.append({\n","            \"question_slug\": qslug,\n","            \"batch_file\": str(outbatch_path),\n","            \"P@5\": p_at5,\n","            \"Recall@5\": rec_at5,\n","            \"MRR@5\": mrr,\n","            \"nDCG@5\": ndcg,\n","            \"mean_cosine_similarity\": mean_sim,\n","            \"num_batch_chunks\": len(batch_chunks),\n","            \"num_papers\": len(included_papers),\n","            \"sim_min\": float(np.min(sims_for_batch)) if sims_for_batch else None,\n","            \"sim_max\": float(np.max(sims_for_batch)) if sims_for_batch else None\n","        })\n","\n","        if len(included_papers) < 0.5 * len(set(all_file_names)):\n","            print(f\"⚠️ ALERT: Batch for {qslug} pulls from <50% of available papers. Consider tuning batch or cap for diversity.\")\n","\n","        if p_at5 < 0.5:\n","            print(f\"⚠️ ALERT: Precision@5 < .5 for {qslug}! Reviewer may wish to audit batch: {outbatch_path}\")\n","\n","        if mean_sim < 0.1:\n","            print(f\"⚠️ ALERT: Mean cosine similarity to Q is low (<0.1); check embedding configs.\")\n","\n","    # 7. Save/print retrieval metrics table for dashboard/reviewers\n","    retrieval_metrics_df = pd.DataFrame(retrieval_metrics)\n","    retrieval_metrics_file = METRICS_DIR / \"retrieval_metrics.csv\"\n","    retrieval_metrics_df.to_csv(retrieval_metrics_file, index=False)\n","\n","    print(f\"\\n[Retrieval Metrics Table]\")\n","    print(retrieval_metrics_df[[\"question_slug\", \"P@5\", \"Recall@5\", \"MRR@5\", \"nDCG@5\", \"mean_cosine_similarity\"]].to_string(index=False))\n","    print(f\"\\nRetrieval metrics saved to: {retrieval_metrics_file}\")\n","    flagged = (retrieval_metrics_df[\"P@5\"] < 0.5) | (retrieval_metrics_df[\"mean_cosine_similarity\"] < 0.1)\n","    if flagged.any():\n","        print(\"\\n[REVIEWER] Some questions flagged for low retrieval overlap or similarity. See csv for batch panels to review.\")\n","\n","build_rag_batch_and_metrics()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"lpJfSCkbstLl","executionInfo":{"status":"ok","timestamp":1746324265914,"user_tz":-720,"elapsed":2792,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"40581291-1e72-4797-eb00-9e4b2260bba5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting multi-question batch build for 4 research questions:\n","\n","— [patient_outcomes] How can AI technologies be utilised to enhance patient outco...\n","  Batch built: 175 chunks | 20 papers\n","  Output:      /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/patient_outcomes_batch_500.jsonl\n","\n","— [nurse_competencies] In what ways can AI tools enhance nursing competencies and p...\n","  Batch built: 175 chunks | 20 papers\n","  Output:      /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/nurse_competencies_batch_500.jsonl\n","\n","— [technical_infrastructure] What specific technical infrastructure is required for effec...\n","  Batch built: 175 chunks | 20 papers\n","  Output:      /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/technical_infrastructure_batch_500.jsonl\n","\n","— [ethics_regulation] What are the key ethical and regulatory challenges associate...\n","  Batch built: 175 chunks | 20 papers\n","  Output:      /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/ethics_regulation_batch_500.jsonl\n","\n","[Retrieval Metrics Table]\n","           question_slug  P@5  Recall@5  MRR@5  nDCG@5  mean_cosine_similarity\n","        patient_outcomes  1.0      0.25    1.0     1.0                0.475454\n","      nurse_competencies  1.0      0.25    1.0     1.0                0.435051\n","technical_infrastructure  1.0      0.25    1.0     1.0                0.384584\n","       ethics_regulation  1.0      0.25    1.0     1.0                0.447217\n","\n","Retrieval metrics saved to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/retrieval_metrics.csv\n"]}]},{"cell_type":"code","source":["#@title BLOCK 8: Multi-Question LLM QA Orchestration (v5.3.2, 2025-05-04)\n","# Purpose: Run LLM QA for each batch/question combo, logging all provenance, errors/warnings, and full metrics (EM, F1, ROUGE-L, BERTScore).\n","# Outputs are reviewer/metrics ready and include complete audit trail.\n","# Changelog:\n","#   - v5.0–v5.1: QA prototype\n","#   - v5.2: Reviewer prompts, output summarization\n","#   - v5.3+: True metrics, error/short flag export for downstream review\n","\n","import os\n","import json\n","import yaml\n","from pathlib import Path\n","from tqdm import tqdm\n","import pandas as pd\n","\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","TO_AI_DIR = PROJECT_ROOT / \"To AI\"\n","FROM_AI_DIR = PROJECT_ROOT / \"From AI\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","os.makedirs(FROM_AI_DIR, exist_ok=True)\n","os.makedirs(METRICS_DIR, exist_ok=True)\n","\n","try:\n","    from google.colab import userdata\n","    openai_api_key = userdata.get(config['openai_key_envvar'])\n","except ImportError:\n","    openai_api_key = os.environ.get(config['openai_key_envvar'])\n","if not openai_api_key:\n","    raise EnvironmentError(f\"OpenAI API key not found! Set {config['openai_key_envvar']} via Colab secrets or environment.\")\n","\n","from openai import OpenAI\n","client = OpenAI(api_key=openai_api_key)\n","llm_model = config['llm_model']\n","\n","def run_llm_qa_all_questions(dry_run=False, max_calls=None, reference_answers=None):\n","    \"\"\"\n","    Runs LLM QA (or simulates if dry_run) for each question and each batch chunk.\n","    Computes Exact Match, F1, ROUGE-L, and BERTScore for each answer\n","    (using reference answers if available), saves to CSV and prints reviewer summary.\n","    \"\"\"\n","    all_metrics = []   # List of dataframes for panel-level aggregation\n","    for qdict in config['questions']:\n","        qslug = qdict['slug']\n","        batch_in = TO_AI_DIR / f\"{qslug}_batch_{config['chunk_tokens']}.jsonl\"\n","        qa_out = FROM_AI_DIR / f\"{qslug}_qa_output.jsonl\"\n","        metrics_rows = []\n","        print(f\"\\n[LLM QA] Running: {batch_in} ⟶ {qa_out} (dry_run={dry_run})\")\n","        n_processed = 0\n","        errors = 0\n","        with open(batch_in, \"r\", encoding=\"utf-8\") as in_f, \\\n","             open(qa_out, \"w\", encoding=\"utf-8\") as out_f:\n","            for line in tqdm(in_f, desc=f\"QA: {qslug[:24]}\"):\n","                chunk = json.loads(line)\n","                # PROMPT\n","                system_msg, user_msg = build_prompt(qdict, chunk['text'], config)\n","                # LLM CALL (real or dry_run)\n","                if dry_run:\n","                    answer = f\"[SIMULATED] {chunk['text'][:60]}...\"\n","                    finish_reason = \"simulated\"\n","                else:\n","                    try:\n","                        response = client.chat.completions.create(\n","                            model=llm_model,\n","                            messages=[\n","                                {\"role\": \"system\", \"content\": system_msg},\n","                                {\"role\": \"user\", \"content\": user_msg}\n","                            ],\n","                            temperature=0.0,\n","                            max_tokens=256,\n","                            top_p=1.0,\n","                            n=1,\n","                        )\n","                        answer = response.choices[0].message.content.strip()\n","                        finish_reason = response.choices[0].finish_reason\n","                    except Exception as e:\n","                        answer = f\"[API ERROR: {str(e)}]\"\n","                        finish_reason = \"error\"\n","                        errors += 1\n","\n","                out = {\n","                    **chunk,\n","                    \"llm_model\": llm_model,\n","                    \"question_slug\": qslug,\n","                    \"llm_answer\": answer,\n","                    \"finish_reason\": finish_reason,\n","                }\n","                # METRIC CALCULATION\n","                ref_ans = None\n","                if reference_answers and qslug in reference_answers:\n","                    ref_ans = reference_answers[qslug].get((chunk[\"file_name\"], chunk[\"chunk_id\"]))\n","                metric_row = {\n","                    \"file_name\": chunk[\"file_name\"],\n","                    \"chunk_id\": chunk[\"chunk_id\"],\n","                    \"llm_answer\": answer,\n","                    \"reference_answer\": ref_ans,\n","                    \"finish_reason\": finish_reason,\n","                    \"question_slug\": qslug,\n","                    \"exact_match\": None,\n","                    \"f1\": None,\n","                    \"rouge_l\": None,\n","                    \"bert_score\": None\n","                }\n","                if ref_ans:\n","                    metric_row[\"exact_match\"] = exact_match(answer, ref_ans)\n","                    metric_row[\"f1\"] = f1_score(answer, ref_ans)\n","                    metric_row[\"rouge_l\"] = rouge_l(answer, ref_ans)\n","                    metric_row[\"bert_score\"] = bert_score(answer, ref_ans)\n","                metrics_rows.append(metric_row)\n","                json.dump(out, out_f, ensure_ascii=False)\n","                out_f.write(\"\\n\")\n","                n_processed += 1\n","                if max_calls and n_processed >= max_calls:\n","                    break\n","\n","        # Save per-question metrics\n","        metrics_df = pd.DataFrame(metrics_rows)\n","        metrics_file = METRICS_DIR / f\"generation_metrics_{qslug}.csv\"\n","        metrics_df.to_csv(metrics_file, index=False)\n","        all_metrics.append(metrics_df)\n","        print(f\"  Written: {qa_out} | {n_processed} QA calls, {errors} errors\")\n","        # Reviewer Stats\n","        if not metrics_df.empty:\n","            in_ref = metrics_df.dropna(subset=[\"reference_answer\"])\n","            n_eval = len(in_ref)\n","            # Flag low quality (f1 or BERTScore <0.5, EM=0 etc)\n","            flagged = (in_ref[\"f1\"].fillna(1) < 0.5) | (in_ref[\"bert_score\"].fillna(1) < 0.5) | (in_ref[\"exact_match\"].fillna(1) == 0)\n","            n_flagged = flagged.sum()\n","            print(f\"    - Checked (with reference): {n_eval}\")\n","            print(f\"    - Flagged low-accuracy answers: {n_flagged} / {n_eval}\")\n","            if n_flagged > 0:\n","                flagged_df = in_ref[flagged]\n","                flagged_file = METRICS_DIR / f\"flagged_low_quality_{qslug}.csv\"\n","                flagged_df.to_csv(flagged_file, index=False)\n","                print(f\"    ⚠️ Reviewer: {n_flagged} flagged (see {flagged_file})\")\n","            else:\n","                print(f\"    (All above threshold for EM, F1, BERTScore)\")\n","\n","            print(f\"    [Output sample]\\n\", in_ref[[\"file_name\", \"chunk_id\", \"exact_match\", \"f1\", \"rouge_l\", \"bert_score\"]].head(5).to_string(index=False))\n","        print(f\"  Metrics for {qslug} saved to: {metrics_file}\")\n","\n","    # Aggregate all to one CSV for dashboard\n","    if all_metrics:\n","        panel_metrics_file = METRICS_DIR / \"generation_metrics_all.csv\"\n","        pd.concat(all_metrics, ignore_index=True).to_csv(panel_metrics_file, index=False)\n","        print(f\"\\nCombined LLM generation metrics saved to {panel_metrics_file}\")\n","\n","    print(\"\\nNext reviewer steps:\")\n","    print(f\" - For each question: Check flagged csv files (if any) for answers needing audit; these are highlighted by low F1/BERTScore/ExactMatch.\\n - Examine generation_metrics_all.csv for panel- or batch-level summary.\")\n","    print(\" - Review answer quality outliers for possible annotation or rerun.\")\n","\n","# === RUN BELOW ===\n","run_llm_qa_all_questions(dry_run=False)  # set dry_run=True for simulation/test mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tx9UDgSFstPv","executionInfo":{"status":"ok","timestamp":1746325373271,"user_tz":-720,"elapsed":1099263,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"63d4ca4b-a8e9-448a-d831-c50934ee6b0b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[LLM QA] Running: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/patient_outcomes_batch_500.jsonl ⟶ /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/patient_outcomes_qa_output.jsonl (dry_run=False)\n"]},{"output_type":"stream","name":"stderr","text":["QA: patient_outcomes: 175it [05:42,  1.96s/it]\n"]},{"output_type":"stream","name":"stdout","text":["  Written: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/patient_outcomes_qa_output.jsonl | 175 QA calls, 0 errors\n","    - Checked (with reference): 0\n","    - Flagged low-accuracy answers: 0 / 0\n","    (All above threshold for EM, F1, BERTScore)\n","    [Output sample]\n"," Empty DataFrame\n","Columns: [file_name, chunk_id, exact_match, f1, rouge_l, bert_score]\n","Index: []\n","  Metrics for patient_outcomes saved to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/generation_metrics_patient_outcomes.csv\n","\n","[LLM QA] Running: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/nurse_competencies_batch_500.jsonl ⟶ /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/nurse_competencies_qa_output.jsonl (dry_run=False)\n"]},{"output_type":"stream","name":"stderr","text":["QA: nurse_competencies: 175it [05:41,  1.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":["  Written: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/nurse_competencies_qa_output.jsonl | 175 QA calls, 0 errors\n","    - Checked (with reference): 0\n","    - Flagged low-accuracy answers: 0 / 0\n","    (All above threshold for EM, F1, BERTScore)\n","    [Output sample]\n"," Empty DataFrame\n","Columns: [file_name, chunk_id, exact_match, f1, rouge_l, bert_score]\n","Index: []\n","  Metrics for nurse_competencies saved to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/generation_metrics_nurse_competencies.csv\n","\n","[LLM QA] Running: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/technical_infrastructure_batch_500.jsonl ⟶ /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/technical_infrastructure_qa_output.jsonl (dry_run=False)\n"]},{"output_type":"stream","name":"stderr","text":["QA: technical_infrastructure: 175it [03:14,  1.11s/it]\n"]},{"output_type":"stream","name":"stdout","text":["  Written: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/technical_infrastructure_qa_output.jsonl | 175 QA calls, 0 errors\n","    - Checked (with reference): 0\n","    - Flagged low-accuracy answers: 0 / 0\n","    (All above threshold for EM, F1, BERTScore)\n","    [Output sample]\n"," Empty DataFrame\n","Columns: [file_name, chunk_id, exact_match, f1, rouge_l, bert_score]\n","Index: []\n","  Metrics for technical_infrastructure saved to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/generation_metrics_technical_infrastructure.csv\n","\n","[LLM QA] Running: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/To AI/ethics_regulation_batch_500.jsonl ⟶ /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/ethics_regulation_qa_output.jsonl (dry_run=False)\n"]},{"output_type":"stream","name":"stderr","text":["QA: ethics_regulation: 175it [03:39,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["  Written: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/From AI/ethics_regulation_qa_output.jsonl | 175 QA calls, 0 errors\n","    - Checked (with reference): 0\n","    - Flagged low-accuracy answers: 0 / 0\n","    (All above threshold for EM, F1, BERTScore)\n","    [Output sample]\n"," Empty DataFrame\n","Columns: [file_name, chunk_id, exact_match, f1, rouge_l, bert_score]\n","Index: []\n","  Metrics for ethics_regulation saved to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/generation_metrics_ethics_regulation.csv\n","\n","Combined LLM generation metrics saved to /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/generation_metrics_all.csv\n","\n","Next reviewer steps:\n"," - For each question: Check flagged csv files (if any) for answers needing audit; these are highlighted by low F1/BERTScore/ExactMatch.\n"," - Examine generation_metrics_all.csv for panel- or batch-level summary.\n"," - Review answer quality outliers for possible annotation or rerun.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#@title BLOCK 9: Reviewer Exports & Audit Tables (v5.3.2, 2025-05-04)\n","# Purpose: Export all answers, flags, metrics, and audit tables to reviewer outputs directory,\n","# producing explicit reviewer panel CSVs/audit records. All metrics and flagged rows are tracked for drift.\n","# Changelog:\n","#   - v5.0–v5.1: Export foundations\n","#   - v5.2: Reviewer audits summary tables, panel exports\n","#   - v5.3+: Panel/flagged output CSVs, time-stamped, summary exports for compliance\n","\n","import pandas as pd\n","import json\n","import yaml\n","from pathlib import Path\n","\n","# -- Load config for questions and folders --\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","FROM_AI_DIR = PROJECT_ROOT / \"From AI\"\n","REVIEWER_DIR = PROJECT_ROOT / \"Reviewers\"\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","\n","def reviewer_exports_and_summary():\n","    status = []\n","    all_review_df = []\n","    for qdict in config[\"questions\"]:\n","        qslug = qdict['slug']\n","        qa_jsonl = FROM_AI_DIR / f\"{qslug}_qa_output.jsonl\"\n","        reviewer_csv = REVIEWER_DIR / f\"{qslug}_reviewer_output.csv\"\n","        reviewer_metrics_csv = METRICS_DIR / f\"{qslug}_reviewer_metrics.csv\"\n","        records = []\n","        with open(qa_jsonl, \"r\", encoding=\"utf-8\") as in_f:\n","            for line in in_f:\n","                records.append(json.loads(line))\n","        df = pd.DataFrame(records)\n","        # Save all fields for primary review\n","        df.to_csv(reviewer_csv, index=False)\n","        # Save a metrics-specific version if certain columns are present\n","        metric_cols = [col for col in df.columns if any(x in col for x in (\"match\", \"f1\", \"rouge\", \"score\"))]\n","        if metric_cols:\n","            df[[\"file_name\", \"chunk_id\"] + metric_cols].to_csv(reviewer_metrics_csv, index=False)\n","        # Audit: per-chunk, total answers, blanks, duplicate answers\n","        n_rows = df.shape[0]\n","        n_blank = (df['llm_answer'].astype(str).str.strip() == '').sum() if 'llm_answer' in df else 0\n","        n_error = df['finish_reason'].eq(\"error\").sum() if 'finish_reason' in df else 0\n","        n_unique_answers = df['llm_answer'].nunique() if 'llm_answer' in df else 0\n","        status.append({\n","            \"qslug\": qslug,\n","            \"reviewer_csv\": str(reviewer_csv),\n","            \"n_chunks\": n_rows,\n","            \"blanks\": n_blank,\n","            \"errors\": n_error,\n","            \"unique_answers\": n_unique_answers\n","        })\n","        all_review_df.append(df)\n","        print(f\"[{qslug}] Reviewer CSV: {reviewer_csv} | chunks={n_rows} blank={n_blank} errors={n_error} unique_answers={n_unique_answers}\")\n","\n","    # Panel/status export for compliance reports\n","    summary_df = pd.DataFrame(status)\n","    reviewer_summary_csv = METRICS_DIR / \"reviewer_status_summary.csv\"\n","    summary_df.to_csv(reviewer_summary_csv, index=False)\n","    print(\"\\nReviewer Table Status Summary:\")\n","    print(summary_df)\n","    print(f\"\\nReview audit summary exported to: {reviewer_summary_csv}\")\n","\n","    # (Optional) Combine all for panel review\n","    if all_review_df:\n","        full_panel_df = pd.concat(all_review_df, ignore_index=True)\n","        full_panel_csv = METRICS_DIR / \"reviewer_outputs_combined.csv\"\n","        full_panel_df.to_csv(full_panel_csv, index=False)\n","        print(f\"Combined reviewer outputs exported to: {full_panel_csv}\")\n","\n","reviewer_exports_and_summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"YC_Xq22pstUL","executionInfo":{"status":"ok","timestamp":1746325391145,"user_tz":-720,"elapsed":282,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"addbbe92-3c48-439d-f25d-fb1c3bc735f1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[patient_outcomes] Reviewer CSV: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers/patient_outcomes_reviewer_output.csv | chunks=175 blank=0 errors=0 unique_answers=145\n","[nurse_competencies] Reviewer CSV: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers/nurse_competencies_reviewer_output.csv | chunks=175 blank=0 errors=0 unique_answers=113\n","[technical_infrastructure] Reviewer CSV: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers/technical_infrastructure_reviewer_output.csv | chunks=175 blank=0 errors=0 unique_answers=48\n","[ethics_regulation] Reviewer CSV: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers/ethics_regulation_reviewer_output.csv | chunks=175 blank=0 errors=0 unique_answers=69\n","\n","Reviewer Table Status Summary:\n","                      qslug  \\\n","0          patient_outcomes   \n","1        nurse_competencies   \n","2  technical_infrastructure   \n","3         ethics_regulation   \n","\n","                                        reviewer_csv  n_chunks  blanks  \\\n","0  /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/R...       175       0   \n","1  /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/R...       175       0   \n","2  /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/R...       175       0   \n","3  /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/R...       175       0   \n","\n","   errors  unique_answers  \n","0       0             145  \n","1       0             113  \n","2       0              48  \n","3       0              69  \n","\n","Review audit summary exported to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/reviewer_status_summary.csv\n","Combined reviewer outputs exported to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/reviewer_outputs_combined.csv\n"]}]},{"cell_type":"code","source":["#@title BLOCK 10: Comprehensive Metrics Dashboard & QA Reporting (v5.3.2, 2025-05-04)\n","# Purpose: Aggregate, flag, and export dashboard stats for reviewer/PI, covering all retrieval/generation metrics,\n","# flagged outputs, drift, abnormalities. All summaries support PI and compliance review.\n","# Changelog:\n","#   - v5.0: Initial dashboard\n","#   - v5.2: Reviewer action/flagging, drift file detection\n","#   - v5.3+: Actionable reviewer guidance, missing metrics/file health checks, export for record audit\n","\n","import pandas as pd\n","from pathlib import Path\n","import os\n","\n","def load_or_none(path):\n","    if Path(path).exists():\n","        return pd.read_csv(path)\n","    else:\n","        print(f\"⚠️ Dashboard: {path} not found. (Did you run previous blocks?)\")\n","        return None\n","\n","def print_horizontal_line():\n","    print(\"=\"*74)\n","\n","def print_metrics_table(title, df, columns, alerts=None, highlight=None):\n","    print(f\"\\n▶️ {title}\")\n","    if df is not None and not df.empty:\n","        print(df[columns].to_string(index=False))\n","        if alerts is not None:\n","            print(f\"\\n[Alerts:{title}]\")\n","            if alerts.empty:\n","                print(\"  ✔ No issues flagged.\")\n","            else:\n","                print(alerts[columns].to_string(index=False))\n","    else:\n","        print(\"  No data loaded for this metric group.\")\n","\n","def print_dashboard_summary():\n","    config_path = Path(\"config.yaml\")\n","    with open(config_path, \"r\") as f:\n","        config = yaml.safe_load(f)\n","    PROJECT_NAME = config['project_name']\n","    PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","    METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","\n","    print_horizontal_line()\n","    print(f\"Nurse-AI v5.3 Metrics Dashboard Summary for Project: {PROJECT_NAME}\")\n","    print_horizontal_line()\n","\n","    retrieval_csv   = METRICS_DIR / \"retrieval_metrics.csv\"\n","    generation_csv  = METRICS_DIR / \"generation_metrics_all.csv\"\n","    embedding_csv   = METRICS_DIR / \"embedding_cosine_stats.csv\"\n","\n","    df_retrieval   = load_or_none(retrieval_csv)\n","    df_generation  = load_or_none(generation_csv)\n","    df_embedding   = load_or_none(embedding_csv)\n","\n","    # --- Retrieval Metrics ---\n","    if df_retrieval is not None:\n","        print_metrics_table(\n","            \"Retrieval Metrics by Question\",\n","            df_retrieval,\n","            columns=[\"question_slug\", \"P@5\", \"Recall@5\", \"MRR@5\", \"nDCG@5\", \"mean_cosine_similarity\"],\n","            alerts=df_retrieval.query(\"`P@5` < .5 or `mean_cosine_similarity` < .1\"),\n","        )\n","    else:\n","        print(\"\\n⚠️ Retrieval metrics not available.\")\n","\n","    # --- Generation Metrics ---\n","    flagged_gen = pd.DataFrame()\n","    if df_generation is not None and not df_generation.empty:\n","        # Reviewer: flag F1 or BERTScore <0.5 (with reference provided)\n","        test = df_generation.dropna(subset=[\"f1\",\"bert_score\"])\n","        flagged_gen = test[(test[\"f1\"] < 0.5) | (test[\"bert_score\"] < 0.5) | (test[\"exact_match\"]==0)]\n","        print_metrics_table(\n","            \"Generation Output Metrics (sample)\",\n","            df_generation.head(10),\n","            columns=[\"question_slug\",\"file_name\", \"chunk_id\", \"exact_match\", \"f1\", \"rouge_l\", \"bert_score\"],\n","            alerts=flagged_gen\n","        )\n","    else:\n","        print(\"\\n⚠️ Generation metrics not available.\")\n","\n","    # --- Embedding Drift Stats ---\n","    if df_embedding is not None and not df_embedding.empty:\n","        print(\"\\n▶️ Embedding Cosine Drift Summary\")\n","        print(df_embedding.to_string(index=False))\n","        if float(df_embedding['cosine_mean'].iloc[0]) < 0.5:\n","            print(\"  ⚠️ Reviewer: Embedding cosine mean is low (<0.5) -- consider reviewing chunk quality.\")\n","    else:\n","        print(\"\\n⚠️ Embedding stats not available.\")\n","\n","    # --- Dashboard CSV Export ---\n","    dashboard_rows = []\n","    if df_retrieval is not None:\n","        dashboard_rows.append(df_retrieval.assign(metric_group=\"retrieval\"))\n","    if df_generation is not None and not df_generation.empty:\n","        group = df_generation.groupby(\"question_slug\")\n","        f1_mean = group[\"f1\"].mean()\n","        bert_mean = group[\"bert_score\"].mean()\n","        em_rate = group[\"exact_match\"].mean()\n","        n_chunks = group[\"chunk_id\"].count()\n","        # Flag definition: f1 < 0.5 OR bert < 0.5 OR exact_match == 0\n","        is_flagged = (\n","            (df_generation[\"f1\"].fillna(1) < 0.5) |\n","            (df_generation[\"bert_score\"].fillna(1) < 0.5) |\n","            (df_generation[\"exact_match\"].fillna(1) == 0)\n","        )\n","        flagged_counts = df_generation[is_flagged].groupby(\"question_slug\").size()\n","        flagged_counts = flagged_counts.reindex(f1_mean.index, fill_value=0)\n","\n","        gen_summary = pd.DataFrame({\n","            \"question_slug\": f1_mean.index,\n","            \"N_chunks\": n_chunks.values,\n","            \"N_flagged\": flagged_counts.values,\n","            \"F1_mean\": f1_mean.values,\n","            \"BertScore_mean\": bert_mean.values,\n","            \"EM_rate\": em_rate.values,\n","        })\n","        dashboard_rows.append(gen_summary.assign(metric_group=\"generation\"))\n","\n","    dashboard_csv = METRICS_DIR / \"dashboard_summary.csv\"\n","    if dashboard_rows:\n","        pd.concat(dashboard_rows, ignore_index=True).to_csv(dashboard_csv, index=False)\n","        print(f\"\\n▶️ Dashboard summary table saved to: {dashboard_csv}\")\n","\n","    print_horizontal_line()\n","    print(\"\\n✅ Dashboard summary complete.\")\n","    if flagged_gen is not None and not flagged_gen.empty:\n","        print(f\"\\n⚠️ {len(flagged_gen)} answers flagged (F1/BERTScore <0.5, or EM=0). Files/rows shown above. Reviewer: Please prioritize flagged.csv files for audit.\\n\")\n","    print(\"Reviewer Guidance:\\n\"\n","          \"  • Check console output above for flagged metrics/items and review highlighted CSVs.\\n\"\n","          \"  • Use dashboard_summary.csv for a one-glance overview and reporting.\\n\"\n","          \"  • If any metric tables are missing, please re-run upstream blocks (4-9) first.\\n\"\n","          \"  • For questions, see docs/user-guide.md or contact the pipeline admin/team.\\n\")\n","    print_horizontal_line()\n","\n","# === RUN BELOW ===\n","print_dashboard_summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"-_87VzhZstY1","executionInfo":{"status":"ok","timestamp":1746325395474,"user_tz":-720,"elapsed":141,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"d9f9cc5b-87e1-4591-bfa4-3dcc856ebbe0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================\n","Nurse-AI v5.3 Metrics Dashboard Summary for Project: NurseAI_PaperQA2_v5.3\n","==========================================================================\n","\n","▶️ Retrieval Metrics by Question\n","           question_slug  P@5  Recall@5  MRR@5  nDCG@5  mean_cosine_similarity\n","        patient_outcomes  1.0      0.25    1.0     1.0                0.475454\n","      nurse_competencies  1.0      0.25    1.0     1.0                0.435051\n","technical_infrastructure  1.0      0.25    1.0     1.0                0.384584\n","       ethics_regulation  1.0      0.25    1.0     1.0                0.447217\n","\n","[Alerts:Retrieval Metrics by Question]\n","  ✔ No issues flagged.\n","\n","▶️ Generation Output Metrics (sample)\n","   question_slug                                                                                file_name  chunk_id  exact_match  f1  rouge_l  bert_score\n","patient_outcomes   Copy of Sockolow et al. - 2024 - Artificial Intelligence in Nursing Perspectives f.pdf         1          NaN NaN      NaN         NaN\n","patient_outcomes          Copy of Johnson - 2024 - Does School Nursing Benefit From Artificial Intell.pdf         0          NaN NaN      NaN         NaN\n","patient_outcomes  Copy of Tarsuslu et al. - 2024 - Can digital leadership transform AI anxiety and at.pdf         3          NaN NaN      NaN         NaN\n","patient_outcomes   Copy of Sockolow et al. - 2024 - Artificial Intelligence in Nursing Perspectives f.pdf         0          NaN NaN      NaN         NaN\n","patient_outcomes  Copy of Tarsuslu et al. - 2024 - Can digital leadership transform AI anxiety and at.pdf         2          NaN NaN      NaN         NaN\n","patient_outcomes Copy of Le Lagadec et al. - 2024 - Artificial intelligence in nursing education Pros.pdf         1          NaN NaN      NaN         NaN\n","patient_outcomes   Copy of Sockolow et al. - 2024 - Artificial Intelligence in Nursing Perspectives f.pdf         9          NaN NaN      NaN         NaN\n","patient_outcomes  Copy of Tarsuslu et al. - 2024 - Can digital leadership transform AI anxiety and at.pdf        37          NaN NaN      NaN         NaN\n","patient_outcomes   Copy of Sockolow et al. - 2024 - Artificial Intelligence in Nursing Perspectives f.pdf         7          NaN NaN      NaN         NaN\n","patient_outcomes   Copy of Sockolow et al. - 2024 - Artificial Intelligence in Nursing Perspectives f.pdf         6          NaN NaN      NaN         NaN\n","\n","[Alerts:Generation Output Metrics (sample)]\n","  ✔ No issues flagged.\n","\n","▶️ Embedding Cosine Drift Summary\n"," cosine_mean  cosine_sd  count\n","    0.651876   0.089268    533\n","\n","▶️ Dashboard summary table saved to: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/dashboard_summary.csv\n","==========================================================================\n","\n","✅ Dashboard summary complete.\n","Reviewer Guidance:\n","  • Check console output above for flagged metrics/items and review highlighted CSVs.\n","  • Use dashboard_summary.csv for a one-glance overview and reporting.\n","  • If any metric tables are missing, please re-run upstream blocks (4-9) first.\n","  • For questions, see docs/user-guide.md or contact the pipeline admin/team.\n","\n","==========================================================================\n"]}]},{"cell_type":"code","source":["#@title BLOCK 11: (Optional) LLM Semantic Spot-Check (v5.3.2, 2025-05-04)\n","# Purpose: Enable reviewer or PI to compare two LLM answers per context for semantic/factual agreement,\n","# using LLM itself for a Yes/No/Not Enough Info verdict. All results are CSV/audit log ready.\n","# Changelog:\n","#   - v5.0: Prototype semantic spot-check\n","#   - v5.1: Flags, context, robust output\n","#   - v5.3+: Reviewer guidance, usage docstrings, structured log export\n","\n","import pandas as pd\n","from pathlib import Path\n","import yaml\n","import os\n","\n","def llm_semantic_compare(chunk, ans1, ans2, openai_api_key, model=\"gpt-4.1\"):\n","    \"\"\"\n","    Uses LLM to compare 2 answers for factual/semantic equivalence given a context chunk.\n","    Returns 'Yes', 'No', 'Not Enough Info', or API error string.\n","    \"\"\"\n","    from openai import OpenAI\n","    client = OpenAI(api_key=openai_api_key)\n","    prompt = (\n","        f\"Are these two answers factually the same? Reply Yes/No/Not Enough Info.\\n\"\n","        f\"Context excerpt: {chunk[:200]}...\\nA1: {ans1}\\nA2: {ans2}\"\n","    )\n","    try:\n","        resp = client.chat.completions.create(\n","            model=model,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            temperature=0, max_tokens=8\n","        )\n","        verdict = resp.choices[0].message.content.strip()\n","        return verdict\n","    except Exception as e:\n","        print(\"LLM call failed:\", e)\n","        return f\"API_ERROR: {e}\"\n","\n","def batch_semantic_spotcheck(\n","    csv_path,\n","    context_col=\"text\",\n","    ans1_col=\"llm_answer_1\",\n","    ans2_col=\"llm_answer_2\",\n","    out_path=None,\n","    n_check=10,\n","    openai_api_key=None,\n","    model=\"gpt-4.1\"\n","):\n","    \"\"\"\n","    Compares two sets of LLM answers for semantic/factual equality via LLM.\n","    Flags and saves disagreements for further review.\n","    Parameters\n","    ----------\n","    csv_path : Path\n","        Path to a CSV with relevant columns.\n","    context_col : str\n","        Column name for context/chunk.\n","    ans1_col : str\n","        Column for first answer.\n","    ans2_col : str\n","        Column for second answer.\n","    out_path : Path or None\n","        Where to write results (CSV).\n","    n_check : int\n","        How many to spot-check (samples top n by default).\n","    openai_api_key : str\n","        OpenAI key to run the comparison LLM.\n","    model : str\n","        Model name for the comparison LLM.\n","    \"\"\"\n","    df = pd.read_csv(csv_path)\n","    if n_check > len(df):\n","        n_check = len(df)\n","    audit_rows = []\n","    for idx, row in df.head(n_check).iterrows():\n","        context = str(row[context_col])[:400] if context_col in row else \"\"\n","        ans1 = row[ans1_col]\n","        ans2 = row[ans2_col]\n","        verdict = llm_semantic_compare(context, ans1, ans2, openai_api_key=openai_api_key, model=model)\n","        audit_rows.append({\n","            \"row_idx\": idx,\n","            \"file_name\": row.get(\"file_name\", \"\"),\n","            \"chunk_id\": row.get(\"chunk_id\", \"\"),\n","            \"A1\": ans1,\n","            \"A2\": ans2,\n","            \"verdict\": verdict,\n","            \"context_excerpt\": context[:200]\n","        })\n","        print(f\"SpotCheck idx={idx}, verdict={verdict}\")\n","    audit_df = pd.DataFrame(audit_rows)\n","    if out_path is None:\n","        out_path = str(csv_path).replace(\".csv\", \"_llm_spotcheck.csv\")\n","    audit_df.to_csv(out_path, index=False)\n","    print(f\"\\nSemantic spot check results exported: {out_path}\")\n","\n","    n_disagree = (audit_df[\"verdict\"].str.lower().str.startswith(\"no\")).sum()\n","    if n_disagree > 0:\n","        print(f\"⚠️ Reviewer: {n_disagree} semantic disagreements flagged (see {out_path}). Please audit these for possible hallucination or inconsistency.\")\n","\n","    print(\"\\nGuidance:\\n\"\n","          \"  • Use this tool to compare answers across pipeline/candidate models, or check for hallucination.\\n\"\n","          \"  • Focus reviewer time on rows where 'No' verdict appears.\\n\"\n","          \"  • For best coverage, randomize or stratify your sample (not just top-n).\\n\")\n","\n","# --- MAIN USAGE EXAMPLE (edit/select your CSV for real runs) ---\n","\n","# Load config for OpenAI key and defaults\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","try:\n","    from google.colab import userdata\n","    openai_api_key = userdata.get(config['openai_key_envvar'])\n","except ImportError:\n","    openai_api_key = os.environ.get(config['openai_key_envvar'])\n","if not openai_api_key:\n","    raise EnvironmentError(f\"OpenAI API key not found! Set {config['openai_key_envvar']}.\")\n","\n","# To use: the reviewer provides a CSV with columns: text, llm_answer_1, llm_answer_2, etc.\n","# For example, to compare two model outputs or to retest selected answers:\n","# Suppose you want to compare v4 vs v5 answers on the same batch:\n","# batch_semantic_spotcheck(\"outputs/v4_vs_v5_answers.csv\", context_col=\"text\", ans1_col=\"llm_answer_v4\", ans2_col=\"llm_answer_v5\", n_check=20, openai_api_key=openai_api_key)\n","\n","# For demonstration, you might do (edit the file names/columns as needed):\n","# batch_semantic_spotcheck(\"outputs/some_parsed_answers.csv\", context_col=\"text\", ans1_col=\"llm_answer\", ans2_col=\"reviewer_answer\", n_check=15, openai_api_key=openai_api_key)\n","\n","print(\"Block 11 ready: To use, call batch_semantic_spotcheck on your answer comparison CSV.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"MDb4DJvrstdl","executionInfo":{"status":"ok","timestamp":1746325399188,"user_tz":-720,"elapsed":847,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"884051a2-a7f8-4e13-9d79-436be7957d7d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Block 11 ready: To use, call batch_semantic_spotcheck on your answer comparison CSV.\n"]}]},{"cell_type":"code","source":["#@title BLOCK 12: Batch-to-Batch Output Drift & Change Detection (v5.3.2, 2025-05-10)\n","# Purpose: Compare latest and previous reviewer/metrics output for every question (and globally) using robust timestamp/version logic;\n","# automatically logs and exports all changed/flagged answers and metrics for PI/audit.\n","# Changelog:\n","#   - v5.0: Batch-level diff\n","#   - v5.1: Robust csv diff, drift CSV exports for reviewer\n","#   - v5.3+: Best-practice: auto-select filenames and print reviewer action guidance\n","\n","import pandas as pd\n","import hashlib\n","import os\n","from pathlib import Path\n","import yaml\n","from datetime import datetime\n","\n","def hash_answer(text):\n","    \"\"\"MD5 of normalized answer for robust comparison.\"\"\"\n","    return hashlib.md5(str(text).strip().encode(\"utf-8\")).hexdigest()\n","\n","def find_latest_and_previous_files(folder, slug_prefix, suffix=\".csv\"):\n","    \"\"\"\n","    Find the latest and previous CSVs for a given slug/question automatically.\n","    \"\"\"\n","    files = sorted([\n","        (f, os.path.getmtime(os.path.join(folder, f)))\n","        for f in os.listdir(folder)\n","        if f.startswith(slug_prefix) and f.endswith(suffix)\n","    ], key=lambda x: x[1])\n","    if len(files) < 2:\n","        return None, None\n","    # Use last as current, penultimate as previous\n","    prev_file = os.path.join(folder, files[-2][0])\n","    curr_file = os.path.join(folder, files[-1][0])\n","    return curr_file, prev_file\n","\n","def compare_reviewer_runs_auto(qslug, reviewers_dir, metrics_dir, out_path=None, n_show=5):\n","    \"\"\"\n","    Automatically finds latest and previous reviewer CSVs for qslug, compares, and saves/prints drift.\n","    \"\"\"\n","    slug_prefix = f\"{qslug}_reviewer_output\"\n","    curr_csv, prev_csv = find_latest_and_previous_files(reviewers_dir, slug_prefix)\n","    if not curr_csv or not prev_csv:\n","        print(f\"⚠️ Reviewer audit skipped: Need >=2 files named '{slug_prefix}*.csv' in {reviewers_dir}\")\n","        print(f\"   Found: {[f for f in os.listdir(reviewers_dir) if f.startswith(slug_prefix)]}\")\n","        return\n","    print(f\"\\n[Reviewer Output Drift] Q={qslug}\")\n","    print(f\"  Comparing:\\n    Previous: {prev_csv}\\n    Current: {curr_csv}\")\n","    curr = pd.read_csv(curr_csv).sort_values(['file_name','chunk_id']).reset_index(drop=True)\n","    prev = pd.read_csv(prev_csv).sort_values(['file_name','chunk_id']).reset_index(drop=True)\n","    if len(curr) != len(prev):\n","        print(\"⚠️ Row count mismatch; cannot compare reliably.\")\n","        return\n","    curr['hash'] = curr['llm_answer'].apply(hash_answer)\n","    prev['hash'] = prev['llm_answer'].apply(hash_answer)\n","    changed_mask = curr['hash'] != prev['hash']\n","    changed = changed_mask.sum()\n","    print(f\"→ {changed}/{len(curr)} answers changed since last reviewer output.\")\n","    diff_df = pd.DataFrame()\n","    if changed > 0:\n","        diff_df = pd.concat([\n","            curr[changed_mask][['file_name', 'chunk_id', 'llm_answer']].rename(columns={'llm_answer':'llm_answer_new'}),\n","            prev[changed_mask][['llm_answer']].rename(columns={'llm_answer':'llm_answer_prev'})\n","        ], axis=1)\n","        print(\"\\nFirst few changed answers (file, chunk_id, old, new):\")\n","        print(diff_df.head(n_show).to_string(index=False))\n","        if out_path is None:\n","            stamp = datetime.now().strftime('%Y%m%d%H%M%S')\n","            out_path = Path(metrics_dir) / f\"{qslug}_reviewer_output_drift_{stamp}.csv\"\n","        diff_df.to_csv(out_path, index=False)\n","        print(f\"Changed output rows exported for review: {out_path}\")\n","    else:\n","        print(\"✔ No output differences detected.\")\n","\n","def find_metrics_versions(metrics_dir, pat=\"generation_metrics_all*.csv\"):\n","    \"\"\"Finds latest and previous version (by mtime) for global metrics drift.\"\"\"\n","    metrics_files = sorted([\n","        (str(f), os.path.getmtime(f))\n","        for f in Path(metrics_dir).glob(pat)\n","    ], key=lambda x: x[1])\n","    if len(metrics_files) < 2:\n","        return None, None\n","    return metrics_files[-1][0], metrics_files[-2][0]\n","\n","def batch_metrics_compare_auto(metrics_dir, out_path=None, n_show=5):\n","    \"\"\"Auto-detect the two latest global all-metrics files and compare.\"\"\"\n","    curr_file, prev_file = find_metrics_versions(metrics_dir)\n","    if not curr_file or not prev_file:\n","        print(\"⚠️ Need at least two 'generation_metrics_all*.csv' files in metrics dir for metrics drift check.\")\n","        print(\"   Present files:\", [str(f) for f in Path(metrics_dir).glob('generation_metrics_all*.csv')])\n","        return\n","    print(f\"\\n[Batch Metrics Drift]\\n  Previous: {prev_file}\\n  Current: {curr_file}\")\n","    curr = pd.read_csv(curr_file)\n","    prev = pd.read_csv(prev_file)\n","    key_cols = ['file_name', 'chunk_id']\n","    metric_cols = [col for col in curr.columns if col not in key_cols]\n","    merged = pd.merge(curr, prev, on=key_cols, suffixes=('_new', '_prev'))\n","    drift_rows = []\n","    for m in metric_cols:\n","        if m+\"_new\" in merged and m+\"_prev\" in merged:\n","            diffs = (merged[m+\"_new\"] != merged[m+\"_prev\"]) & ~(merged[m+\"_new\"].isnull() & merged[m+\"_prev\"].isnull())\n","            drift_sum = diffs.sum()\n","            drift_rows.append({'metric': m, 'changed': drift_sum, 'percent_changed': drift_sum/len(merged) if len(merged)>0 else 0})\n","    drift_df = pd.DataFrame(drift_rows)\n","    print(\"\\nMetric drift summary table:\")\n","    print(drift_df.to_string(index=False))\n","    if out_path is None:\n","        stamp = datetime.now().strftime('%Y%m%d%H%M%S')\n","        out_path = Path(metrics_dir) / f\"batch_metrics_drift_summary_{stamp}.csv\"\n","    drift_df.to_csv(out_path, index=False)\n","    print(f\"Drift summary table exported: {out_path}\")\n","\n","# === MAIN AUTO-COMPARISON AUDIT ===\n","\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","METRICS_DIR = PROJECT_ROOT / \"outputs\" / \"metrics\"\n","REVIEWERS_DIR = PROJECT_ROOT / \"Reviewers\"\n","\n","for qslug in [\"patient_outcomes\", \"nurse_competencies\", \"technical_infrastructure\", \"ethics_regulation\"]:\n","    compare_reviewer_runs_auto(qslug, REVIEWERS_DIR, METRICS_DIR, n_show=5)\n","\n","batch_metrics_compare_auto(METRICS_DIR, n_show=7)\n","\n","print(\"\\nReviewer guidance:\\n\"\n","      \"• Files auto-detected: last 2 reviewer outputs and metrics snapshots (by mtime).\\n\"\n","      \"• If drift not detected, ensure a previous and current output exist for reviewer/metrics.\\n\"\n","      \"• For flagged changes, check exported drift CSVs in the metrics dir. Prioritize for audit.\\n\"\n","      \"• Use this after any major pipeline or data change to ensure traceable, transparent differences.\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"IaqhpO5ustiP","executionInfo":{"status":"ok","timestamp":1746325401468,"user_tz":-720,"elapsed":46,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"a23f0e83-6ab1-45ad-d0d3-ff310ecb3a2a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["⚠️ Reviewer audit skipped: Need >=2 files named 'patient_outcomes_reviewer_output*.csv' in /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers\n","   Found: ['patient_outcomes_reviewer_output.csv']\n","⚠️ Reviewer audit skipped: Need >=2 files named 'nurse_competencies_reviewer_output*.csv' in /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers\n","   Found: ['nurse_competencies_reviewer_output.csv']\n","⚠️ Reviewer audit skipped: Need >=2 files named 'technical_infrastructure_reviewer_output*.csv' in /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers\n","   Found: ['technical_infrastructure_reviewer_output.csv']\n","⚠️ Reviewer audit skipped: Need >=2 files named 'ethics_regulation_reviewer_output*.csv' in /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/Reviewers\n","   Found: ['ethics_regulation_reviewer_output.csv']\n","⚠️ Need at least two 'generation_metrics_all*.csv' files in metrics dir for metrics drift check.\n","   Present files: ['/content/drive/MyDrive/NurseAI_PaperQA2_v5.3/outputs/metrics/generation_metrics_all.csv']\n","\n","Reviewer guidance:\n","• Files auto-detected: last 2 reviewer outputs and metrics snapshots (by mtime).\n","• If drift not detected, ensure a previous and current output exist for reviewer/metrics.\n","• For flagged changes, check exported drift CSVs in the metrics dir. Prioritize for audit.\n","• Use this after any major pipeline or data change to ensure traceable, transparent differences.\n","\n"]}]},{"cell_type":"code","source":["#@title BLOCK 13: Issue & Decision Tracking Utility (v5.3.2, 2025-05-10)\n","# Purpose: Records local code/review bugs, dev/PI decisions, or change requests as markdown tickets\n","# for reproducibility, local audit, and traceability. Integrated into CI/Git.\n","# Changelog:\n","#   - v5.0: Initial tracker, print/export\n","#   - v5.3+: Decision audit, bug tagging, author/date metadata, bulk query support\n","\n","import os\n","from pathlib import Path\n","from datetime import datetime\n","\n","# --- Load config for paths ---\n","import yaml\n","config_path = Path(\"config.yaml\")\n","with open(config_path, \"r\") as f:\n","    config = yaml.safe_load(f)\n","PROJECT_NAME = config['project_name']\n","PROJECT_ROOT = Path(\"/content/drive/MyDrive\") / PROJECT_NAME\n","ISSUES_DIR = PROJECT_ROOT / \"issues\"\n","\n","os.makedirs(ISSUES_DIR, exist_ok=True)\n","\n","def create_issue(\n","    title,\n","    description,\n","    category=\"bug\",    # e.g., \"bug\", \"enhancement\", \"metrics\", \"decision\"\n","    status=\"open\",     # \"open\", \"closed\"\n","    author=\"anonymous\",\n","    tags=None,\n","    markdown_ext=\".md\"\n","):\n","    \"\"\"\n","    Creates a new issue/decision ticket as a markdown file in the local issues/ folder.\n","    Returns ticket path.\n","    \"\"\"\n","    if tags is None:\n","        tags = []\n","    dt = datetime.now()\n","    timestr = dt.strftime('%Y%m%d_%H%M%S')\n","    safe_title = title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")[:40]\n","    filename = f\"{category}_{timestr}_{safe_title}{markdown_ext}\"\n","    file_path = ISSUES_DIR / filename\n","\n","    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(f\"# {title}\\n\")\n","        f.write(f\"- **Category:** {category}\\n\")\n","        f.write(f\"- **Status:** {status}\\n\")\n","        f.write(f\"- **Created:** {dt.isoformat()}\\n\")\n","        f.write(f\"- **Author:** {author}\\n\")\n","        if tags: f.write(f\"- **Tags:** {', '.join(tags)}\\n\")\n","        f.write(\"\\n---\\n\")\n","        f.write(f\"{description}\\n\")\n","    print(f\"Issue/decision saved: {file_path}\")\n","    return file_path\n","\n","def list_issues(category_filter=None, status_filter=None, keyword_filter=None):\n","    \"\"\"\n","    Lists issue/decision files in the issues directory, optionally filtered by category, status, or keyword.\n","    \"\"\"\n","    files = sorted([f for f in ISSUES_DIR.glob(\"*.md\")])\n","    summaries = []\n","    for path in files:\n","        with open(path, \"r\", encoding=\"utf-8\") as f:\n","            md = f.read()\n","        line1 = md.splitlines()[0] if md else \"No Title\"\n","        lines = md.splitlines()\n","        cat = next((l for l in lines if l.startswith(\"- **Category:**\")), \"\")\n","        status = next((l for l in lines if l.startswith(\"- **Status:**\")), \"\")\n","        if category_filter and category_filter.lower() not in cat.lower():\n","            continue\n","        if status_filter and status_filter.lower() not in status.lower():\n","            continue\n","        if keyword_filter and keyword_filter.lower() not in md.lower():\n","            continue\n","        summaries.append({\"file\": path.name, \"title\": line1[2:] if line1.startswith(\"# \") else line1, \"category\": cat[14:], \"status\": status[12:]})\n","    print(\"\\nTracked issues/decisions:\")\n","    for s in summaries:\n","        print(f\"- {s['file']}: [{s['status']}] ({s['category']}) {s['title']}\")\n","    if not summaries:\n","        print(\"No issues or decisions found.\")\n","\n","def print_issue(file_name):\n","    \"\"\"\n","    Pretty-prints the full markdown of a particular issue/decision by file name.\n","    \"\"\"\n","    file_path = ISSUES_DIR / file_name\n","    if not file_path.exists():\n","        print(f\"Issue file not found: {file_path}\")\n","        return\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        print(f.read())\n","\n","# Example usage:\n","create_issue(\n","   title=\"Tokenization bug with 4K+ texts\",\n","   description=\"Observed silent truncation for >4096 token chunks in embedding stage.\",\n","   category=\"bug\",\n","   status=\"open\",\n","   author=\"alice\",\n","   tags=[\"embedding\", \"token-limit\"]\n",")\n","list_issues(category_filter=\"bug\", status_filter=\"open\")\n","print_issue(\"bug_20250710_141530_Tokenization_bug_with_4K+_tex.md\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"HyvMfhPLstmX","executionInfo":{"status":"ok","timestamp":1746325404189,"user_tz":-720,"elapsed":26,"user":{"displayName":"Nurse David","userId":"07956094516023546703"}},"outputId":"239d545d-9761-43db-8c80-eac6eb2f3105"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Issue/decision saved: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/issues/bug_20250504_022324_Tokenization_bug_with_4K+_texts.md\n","\n","Tracked issues/decisions:\n","- bug_20250504_022324_Tokenization_bug_with_4K+_texts.md: [* open] (* bug) Tokenization bug with 4K+ texts\n","Issue file not found: /content/drive/MyDrive/NurseAI_PaperQA2_v5.3/issues/bug_20250710_141530_Tokenization_bug_with_4K+_tex.md\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bXExM8cEstrF"},"execution_count":null,"outputs":[]}]}