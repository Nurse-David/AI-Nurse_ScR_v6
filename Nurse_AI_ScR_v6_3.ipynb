{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yn5FKEFmm52p",
    "outputId": "2d3798d3-0bca-4f7d-aa23-950a3c8e9517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PYTHON ENVIRONMENT FINGERPRINT ---\n",
      "Python version: 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n",
      "Platform      : Linux-6.1.123+-x86_64-with-glibc2.35\n",
      "CWD           : /content\n",
      "Locale        : en_US.UTF-8\n",
      "Datetime UTC  : 2025-05-19T02:10:27.277918Z\n",
      "Datetime NZDT : 2025-05-19T14:10:27.277767+12:00\n",
      "- Required Pip Package Versions -\n",
      "PyMuPDF: 1.25.5\n",
      "pdfplumber: 0.11.6\n",
      "pandas: 2.2.2\n",
      "tqdm: 4.67.1\n",
      "tiktoken: 0.9.0\n",
      "openai: 1.78.1\n",
      "requests: 2.32.3\n",
      "pyyaml: 6.0.2\n",
      "--- End Block 1 ---\n"
     ]
    }
   ],
   "source": [
    "# @title [Block 1] Install & Verify All Pipeline Libraries (+Deprecation-safe, Audit Logging, Timezone)\n",
    "# Version 6.2.5\n",
    "# v6.0: Added tiktoken and improved environment/version printouts.\n",
    "# v6.1: Suppresses noisy pdfminer/pdfplumber warnings, writes requirements.txt, more clear reviewer messages.\n",
    "# v6.2: Audit log, fail-closed preflight, critical package enforcement, more robust error/warning audit output.\n",
    "# v6.2.3: NZDT/UTC timestamp for chain-of-custody, audit-logging enhanced.\n",
    "# v6.2.4: Fully removes pkg_resources, uses importlib.metadata for requirements/version print.\n",
    "# v6.2.5: [BUGFIX] Corrects `required_packages` unpacking (now always 3-element tuples).\n",
    "#\n",
    "# Block Summary\n",
    "#\n",
    "# - Installs/verifies all dependencies, fails closed if any are missing.\n",
    "# - Suppresses pdfminer/pdfplumber UserWarnings.\n",
    "# - Prints all required package versions, Python/platform/locale.\n",
    "# - Records UTC and NZDT timestamps for fixity audit.\n",
    "# - Writes requirements.txt and audit/block1_env_fingerprint.jsonl.\n",
    "# - Uses importlib.metadata (not pkg_resources) everywhere.\n",
    "\n",
    "import sys, platform, subprocess, os, locale, hashlib, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Timezone handling ----\n",
    "try:\n",
    "    import zoneinfo\n",
    "    TZ_NZ = zoneinfo.ZoneInfo(\"Pacific/Auckland\")\n",
    "    now_nzdt = datetime.now(TZ_NZ)\n",
    "except Exception:\n",
    "    TZ_NZ = None\n",
    "    now_nzdt = None\n",
    "now_utc = datetime.utcnow()\n",
    "now_utc_iso = now_utc.isoformat() + \"Z\"\n",
    "now_nzdt_iso = (now_nzdt.isoformat() if now_nzdt else \"Unavailable\")\n",
    "\n",
    "# --------- Audit log helper ----------\n",
    "def sha256_file(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_audit_log(data, path=\"audit/block1_env_fingerprint.jsonl\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# --------- Suppress noisy PDF warnings ----------\n",
    "import logging, warnings\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pdfminer.pdfpage\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --------- REQUIRED/IMPORT names and install names (pip_name, import_name, dist_name) ----------\n",
    "required_packages = [\n",
    "    (\"PyMuPDF\", \"fitz\", \"pymupdf\"),\n",
    "    (\"pdfplumber\", \"pdfplumber\", \"pdfplumber\"),\n",
    "    (\"pandas\", \"pandas\", \"pandas\"),\n",
    "    (\"tqdm\", \"tqdm\", \"tqdm\"),\n",
    "    (\"tiktoken\", \"tiktoken\", \"tiktoken\"),\n",
    "    (\"openai\", \"openai\", \"openai\"),\n",
    "    (\"requests\", \"requests\", \"requests\"),\n",
    "    (\"pyyaml\", \"yaml\", \"pyyaml\"),\n",
    "]\n",
    "\n",
    "# --------- Audit log object ----------\n",
    "block1_audit = {\n",
    "    \"step\": \"block1_install_and_env\",\n",
    "    \"timestamp_utc\": now_utc_iso,\n",
    "    \"timestamp_nzdt\": now_nzdt_iso,\n",
    "    \"python_version\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cwd\": str(Path.cwd()),\n",
    "    \"user\": os.environ.get(\"USER\", os.environ.get(\"USERNAME\", \"unknown\")),\n",
    "    \"locale\": \"\",\n",
    "    \"requirements_txt_hash\": None,\n",
    "    \"errors\": [],\n",
    "    \"warnings\": [],\n",
    "}\n",
    "\n",
    "try:\n",
    "    block1_audit[\"locale\"] = locale.setlocale(locale.LC_ALL, '')\n",
    "except Exception:\n",
    "    block1_audit[\"warnings\"].append(\"Could not detect locale.\")\n",
    "\n",
    "# --------- Preflight: install/verify packages, fail-closed if not present ----------\n",
    "def pip_install_package(pip_name, import_name):\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "            __import__(import_name)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "\n",
    "failed_pkgs = []\n",
    "for pip_name, import_name, dist_name in required_packages:\n",
    "    result = pip_install_package(pip_name, import_name)\n",
    "    if result is not True:\n",
    "        failed_pkgs.append((pip_name, result))\n",
    "if failed_pkgs:\n",
    "    print(\"\\n[CRITICAL ERROR] Could not install required dependencies:\")\n",
    "    for pip_name, err in failed_pkgs:\n",
    "        print(f\" - {pip_name}: {err}\")\n",
    "        block1_audit[\"errors\"].append(f\"{pip_name}: {err}\")\n",
    "    write_audit_log(block1_audit)\n",
    "    print(\"Reviewer Action:\\n - Please verify internet access and pip availability.\\n - Rerun this block when resolved.\\n - Error log is saved to audit/block1_env_fingerprint.jsonl.\")\n",
    "    raise SystemExit(\"Block 1 failed closed due to missing required libraries.\")\n",
    "\n",
    "# --------- Write requirements.txt and hash it for audit trail ----------\n",
    "try:\n",
    "    try:\n",
    "        import importlib.metadata as importlib_metadata\n",
    "    except ImportError:\n",
    "        import importlib_metadata # type: ignore\n",
    "    dists = sorted(importlib_metadata.distributions(), key=lambda d: (d.metadata[\"Name\"] if \"Name\" in d.metadata else d._path.name).lower())\n",
    "    with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for dist in dists:\n",
    "            name = dist.metadata[\"Name\"] if \"Name\" in dist.metadata else dist._path.name\n",
    "            version = dist.version\n",
    "            f.write(f\"{name}=={version}\\n\")\n",
    "    req_hash = sha256_file(\"requirements.txt\")\n",
    "    block1_audit[\"requirements_txt_hash\"] = req_hash\n",
    "except Exception as e:\n",
    "    msg = f\"Could not write requirements.txt: {e}\"\n",
    "    block1_audit[\"warnings\"].append(msg)\n",
    "    print(\"[WARN]\", msg)\n",
    "\n",
    "# --------- Print python/environment/time details ----------\n",
    "print(\"--- PYTHON ENVIRONMENT FINGERPRINT ---\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform      : {platform.platform()}\")\n",
    "print(f\"CWD           : {Path.cwd()}\")\n",
    "try:\n",
    "    print(f\"Locale        : {locale.setlocale(locale.LC_ALL, '')}\")\n",
    "except:\n",
    "    print(\"Locale        : [Could not detect locale]\")\n",
    "print(f\"Datetime UTC  : {now_utc_iso}\")\n",
    "print(f\"Datetime NZDT : {now_nzdt_iso if now_nzdt else '[Zoneinfo not installed]'}\")\n",
    "print(\"- Required Pip Package Versions -\")\n",
    "for pip_name, import_name, dist_name in required_packages:\n",
    "    try:\n",
    "        version = importlib_metadata.version(dist_name)\n",
    "        print(f\"{pip_name}: {version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{pip_name}: NOT INSTALLED or version not found ({e})\")\n",
    "        block1_audit[\"warnings\"].append(f\"{pip_name}: not found ({e})\")\n",
    "print(\"--- End Block 1 ---\")\n",
    "write_audit_log(block1_audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c2LeYQMcnFWG",
    "outputId": "0b4bc47d-26f3-4bac-99b4-a12a7fc58d92"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "[NOTICE] config.yaml was NOT found and a full template was created at:\n",
      "  /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/operational/config.yaml\n",
      "------ config.yaml TEMPLATE CONTENT ------\n",
      "# --- Pipeline Parameters and Tools ---\n",
      "llm_model: gpt-4.1-2025-04-14\n",
      "openai_key_envvar: OPENAI_API_KEY\n",
      "embedding_model: text-embedding-3-large\n",
      "chunk_sizes:\n",
      "- 500\n",
      "- 5000\n",
      "chunk_overlaps:\n",
      "  500: 100\n",
      "  5000: 500\n",
      "tiktoken_encoding: cl100k_base\n",
      "num_pdfs: 4\n",
      "temperature_schedule:\n",
      "- 0.0\n",
      "- 0.0\n",
      "- 0.0\n",
      "- 0.1\n",
      "- 0.2\n",
      "max_context_tokens: 1000000\n",
      "max_synthesis_tokens: 1000000\n",
      "run_timestamp: '2025-05-19T16:42:32.642731+12:00'\n",
      "run_id: Nurse-AI_ScR_250519_1642\n",
      "use_grobid: true\n",
      "grobid_url: http://localhost:8070/api/processHeaderDocument\n",
      "use_crossref: true\n",
      "use_openalex: true\n",
      "allow_internet: true\n",
      "allow_dynamic_expansion: false\n",
      "\n",
      "# --- Directories, Paths, and Filenames ---\n",
      "pdf_dir: /content/drive/My Drive/Pilot/PDFs\n",
      "run_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642\n",
      "project_root: /content/drive/My Drive/Pilot\n",
      "manifest_path: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/operational/manifest.csv\n",
      "operational_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/operational\n",
      "ai_artifacts_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts\n",
      "reviewer_content_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/reviewer_content\n",
      "metrics_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/metrics\n",
      "audit_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/audit\n",
      "issues_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/issues\n",
      "tests_dir: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/tests\n",
      "\n",
      "-----------------------------------------\n",
      "Would you like to edit config.yaml before continuing? [Y/N]: n\n",
      "Continuing with the generated config.yaml as shown above.\n",
      "\n",
      "--- BLOCK 2: PDF Corpus & Directory Setup ---\n",
      "Python working directory now (os.getcwd()): /content\n",
      "Time in NZDT: 2025-05-19T16:42:32.642731+12:00 | UTC: 2025-05-19T04:42:32.642771Z\n",
      "PROJECT_ROOT    : /content/drive/My Drive/Pilot\n",
      "PDF_DIR         : /content/drive/My Drive/Pilot/PDFs\n",
      "SAMPLE RUN_DIR  : /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642\n",
      " [OK] Found 4 PDF file(s) in '/content/drive/My Drive/Pilot/PDFs':\n",
      " [1] Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      " [2] Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      " [3] Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      " [4] Copy of Copy of Dubin et al. - 2024 - Appropriateness of Frequently Asked Patient Questi.pdf\n",
      "\n",
      "Planned run directory for this session: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642\n",
      "\n",
      "[INFO] pipeline_env.json has been written for deterministic use by all future blocks (no search, no prompts).\n",
      "--- End Block 2 ---\n"
     ]
    }
   ],
   "source": [
    "# @title [Block 2] Define Project Paths, Mount Drive, NZDT Run Folder, Print PDF Corpus Summary, Interactive Config Confirmation, Save pipeline_env.json (v6.2.5)\n",
    "#\n",
    "# ----------- [Version 6.2.5] -----------\n",
    "# v6.0: Sets up all pipeline dirs and run/output/audit folders.\n",
    "# v6.1: Reviewer-friendly summary and robust error checks.\n",
    "# v6.2: Audit logging, fail-closed config, explicit directory setup, next-step guidance.\n",
    "# v6.2.3: NZDT time/fixity, prints cd/CONFIG_PATH helper, logs NZDT+UTC.\n",
    "# v6.2.5: On completion, writes all global pipeline folder/ID variables to pipeline_env.json for use in all future blocks (NO search or input needed).\n",
    "#\n",
    "# ----------- [Block Summary] -----------\n",
    "# - Sets up and prints all pipeline directories and run/output/audit folders with NZDT/UTC timestamp for fixity.\n",
    "# - Creates/validates config.yaml (if missing, prints code-grouped YAML, reviewer Y/N pause option).\n",
    "# - Writes all project/run path/globals to pipeline_env.json so all next blocks just reload this\u2014NO searching, prompt, or ambiguity.\n",
    "# - Reviewer guidance, printout, and summary for PI/repro.\n",
    "# - Audit log written with full NZDT/UTC provenance.\n",
    "#\n",
    "# ----------- [Start of Code] -----------\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from src.paths import get_project_root, build_all_dirs\n",
    "\n",
    "# --- Timezone safe run creation ---\n",
    "try:\n",
    "    import zoneinfo\n",
    "    TZ_NZ = zoneinfo.ZoneInfo(\"Pacific/Auckland\")\n",
    "    now_nzdt = datetime.now(TZ_NZ)\n",
    "    NOW_STR = now_nzdt.strftime('%y%m%d_%H%M')\n",
    "    now_utc = datetime.utcnow()\n",
    "    NZDT_LABEL = now_nzdt.isoformat()\n",
    "    UTC_LABEL = now_utc.isoformat() + \"Z\"\n",
    "except Exception:\n",
    "    TZ_NZ = None\n",
    "    now_nzdt = None\n",
    "    NOW_STR = datetime.utcnow().strftime('%y%m%d_%H%M')\n",
    "    now_utc = datetime.utcnow()\n",
    "    NZDT_LABEL = \"Unavailable\"\n",
    "    UTC_LABEL = now_utc.isoformat() + \"Z\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = get_project_root(IN_COLAB)\n",
    "PDF_DIR = PROJECT_ROOT / 'PDFs'\n",
    "RUN_ID = f\"Nurse-AI_ScR_{NOW_STR}\"\n",
    "RUN_DIR = PROJECT_ROOT / RUN_ID\n",
    "DIRS = build_all_dirs(RUN_DIR)\n",
    "OPERATIONAL_DIR      = DIRS['operational']\n",
    "AI_ARTIFACTS_DIR     = DIRS['ai_artifacts']\n",
    "REVIEWER_CONTENT_DIR = DIRS['reviewer_content']\n",
    "METRICS_DIR          = DIRS['metrics']\n",
    "AUDIT_DIR            = DIRS['audit']\n",
    "ISSUES_DIR           = DIRS['issues']\n",
    "TESTS_DIR            = DIRS['tests']\n",
    "\n",
    "pdfs_list = sorted(PDF_DIR.glob('*.pdf')) if PDF_DIR.exists() else []\n",
    "n_pdfs = len(pdfs_list)\n",
    "\n",
    "CONFIG_PATH = OPERATIONAL_DIR / \"config.yaml\"\n",
    "MANIFEST_PATH = OPERATIONAL_DIR / \"manifest.csv\"\n",
    "CONFIG_PARAMETERS = {\n",
    "    'llm_model': 'gpt-4.1-2025-04-14',\n",
    "    'openai_key_envvar': 'OPENAI_API_KEY',\n",
    "    'embedding_model': 'text-embedding-3-large',\n",
    "    'chunk_sizes': [500, 5000],\n",
    "    'chunk_overlaps': {500: 100, 5000: 500},\n",
    "    'tiktoken_encoding': 'cl100k_base',\n",
    "    'num_pdfs': n_pdfs,\n",
    "    'temperature_schedule': [0.0, 0.0, 0.0, 0.1, 0.2],\n",
    "    'max_context_tokens': 1000000,\n",
    "    'max_synthesis_tokens': 1000000,\n",
    "    'run_timestamp': NZDT_LABEL,\n",
    "    'run_id': RUN_ID,\n",
    "    'use_grobid': True,\n",
    "    'grobid_url': \"http://localhost:8070/api/processHeaderDocument\",\n",
    "    'use_crossref': True,\n",
    "    'use_openalex': True,\n",
    "    'allow_internet': True,\n",
    "    'allow_dynamic_expansion': False,\n",
    "}\n",
    "CONFIG_PATHS = {\n",
    "    'pdf_dir': str(PDF_DIR),\n",
    "    'run_dir': str(RUN_DIR),\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "    'manifest_path': str(MANIFEST_PATH),\n",
    "    'operational_dir': str(OPERATIONAL_DIR),\n",
    "    'ai_artifacts_dir': str(AI_ARTIFACTS_DIR),\n",
    "    'reviewer_content_dir': str(REVIEWER_CONTENT_DIR),\n",
    "    'metrics_dir': str(METRICS_DIR),\n",
    "    'audit_dir': str(AUDIT_DIR),\n",
    "    'issues_dir': str(ISSUES_DIR),\n",
    "    'tests_dir': str(TESTS_DIR),\n",
    "}\n",
    "DEFAULT_CONFIG = {**CONFIG_PARAMETERS, **CONFIG_PATHS}\n",
    "config_yaml_content = \"# --- Pipeline Parameters and Tools ---\\n\"\n",
    "config_yaml_content += yaml.dump(CONFIG_PARAMETERS, sort_keys=False, default_flow_style=False)\n",
    "config_yaml_content += \"\\n# --- Directories, Paths, and Filenames ---\\n\"\n",
    "config_yaml_content += yaml.dump(CONFIG_PATHS, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "missing_config = False\n",
    "user_choice = None\n",
    "if not CONFIG_PATH.exists():\n",
    "    missing_config = True\n",
    "    with open(CONFIG_PATH, \"w\") as f:\n",
    "        f.write(config_yaml_content)\n",
    "    print(f\"\\n[NOTICE] config.yaml was NOT found and a full template was created at:\\n  {CONFIG_PATH}\")\n",
    "    print(\"------ config.yaml TEMPLATE CONTENT ------\")\n",
    "    print(config_yaml_content)\n",
    "    print(\"-----------------------------------------\")\n",
    "    try:\n",
    "        user_choice = input(\"Would you like to edit config.yaml before continuing? [Y/N]: \").strip().lower()\n",
    "    except Exception:\n",
    "        user_choice = \"n\"\n",
    "    if user_choice == \"y\":\n",
    "        print(f\"Pause here and edit config.yaml at:\\n  {CONFIG_PATH}\\nThen run this block again when finished.\")\n",
    "        block2_audit = {\n",
    "            \"step\": \"block2_dir_and_config_preflight\",\n",
    "            \"timestamp_nzdt\": NZDT_LABEL,\n",
    "            \"timestamp_utc\": UTC_LABEL,\n",
    "            \"user_decision\": \"pause_to_edit_config\",\n",
    "            \"config_path\": str(CONFIG_PATH),\n",
    "        }\n",
    "        audit_log_path = AUDIT_DIR / \"block2_dirsetup_preflight.jsonl\"\n",
    "        with open(audit_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(block2_audit, ensure_ascii=False) + \"\\n\")\n",
    "        raise RuntimeError(\"[BLOCK 2 PAUSED] Edit config.yaml and rerun this cell.\")\n",
    "    else:\n",
    "        print(\"Continuing with the generated config.yaml as shown above.\")\n",
    "else:\n",
    "    print(f\"[OK] config.yaml found: {CONFIG_PATH}\")\n",
    "\n",
    "print(f\"\\n--- BLOCK 2: PDF Corpus & Directory Setup ---\")\n",
    "print(f\"Python working directory now (os.getcwd()): {os.getcwd()}\")\n",
    "print(f\"Time in NZDT: {NZDT_LABEL} | UTC: {UTC_LABEL}\")\n",
    "print(f\"PROJECT_ROOT    : {PROJECT_ROOT}\")\n",
    "print(f\"PDF_DIR         : {PDF_DIR}\")\n",
    "print(f\"SAMPLE RUN_DIR  : {RUN_DIR}\")\n",
    "\n",
    "if not PDF_DIR.exists():\n",
    "    print(f\" [ERROR] PDF corpus directory does not exist: {PDF_DIR}\")\n",
    "    corpus_ok = False\n",
    "elif not n_pdfs:\n",
    "    print(f\" [WARNING] PDF corpus exists, but contains NO PDF files.\")\n",
    "    corpus_ok = False\n",
    "else:\n",
    "    corpus_ok = True\n",
    "    print(f\" [OK] Found {n_pdfs} PDF file(s) in '{PDF_DIR}':\")\n",
    "    shown_files = [f.name for f in pdfs_list[:5]]\n",
    "    for i, fname in enumerate(shown_files, 1):\n",
    "        print(f\" [{i}] {fname}\")\n",
    "    if n_pdfs > 5:\n",
    "        print(f\" ... ({n_pdfs - 5} more not shown)\")\n",
    "    missing_ext = [f for f in pdfs_list if not f.name.lower().endswith('.pdf')]\n",
    "    if missing_ext:\n",
    "        print(f\" [WARNING] {len(missing_ext)} files missing .pdf extension! Files: {[f.name for f in missing_ext]}\")\n",
    "print(f\"\\nPlanned run directory for this session: {RUN_DIR}\")\n",
    "\n",
    "# ------ [Write pipeline_env.json for ALL further blocks] ------\n",
    "pipeline_env = dict(\n",
    "    PROJECT_ROOT   = str(PROJECT_ROOT),\n",
    "    RUN_ID         = RUN_ID,\n",
    "    RUN_DIR        = str(RUN_DIR),\n",
    "    OPERATIONAL_DIR= str(OPERATIONAL_DIR),\n",
    "    AI_ARTIFACTS_DIR=str(AI_ARTIFACTS_DIR),\n",
    "    REVIEWER_CONTENT_DIR=str(REVIEWER_CONTENT_DIR),\n",
    "    METRICS_DIR    = str(METRICS_DIR),\n",
    "    AUDIT_DIR      = str(AUDIT_DIR),\n",
    "    ISSUES_DIR     = str(ISSUES_DIR),\n",
    "    TESTS_DIR      = str(TESTS_DIR)\n",
    ")\n",
    "with open(\"pipeline_env.json\", \"w\") as f:\n",
    "    json.dump(pipeline_env, f, indent=2)\n",
    "\n",
    "# ------ [AUDIT LOG: directory/config/corpus state] ------\n",
    "block2_audit = {\n",
    "    \"step\": \"block2_dir_and_config_preflight\",\n",
    "    \"timestamp_nzdt\": NZDT_LABEL,\n",
    "    \"timestamp_utc\": UTC_LABEL,\n",
    "    \"env\": \"colab\" if IN_COLAB else \"local\",\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"run_dir\": str(RUN_DIR),\n",
    "    \"pdf_dir\": str(PDF_DIR),\n",
    "    \"n_pdfs\": n_pdfs,\n",
    "    \"config_yaml_exists\": True,\n",
    "    \"config_yaml_path\": str(CONFIG_PATH),\n",
    "    \"corpus_ok\": corpus_ok,\n",
    "    \"user_decision\": user_choice,\n",
    "    \"warnings\": [],\n",
    "    \"errors\": []\n",
    "}\n",
    "if missing_config and user_choice == \"y\":\n",
    "    block2_audit[\"warnings\"].append(\"User paused to edit just-created config.yaml; rerun required.\")\n",
    "if not corpus_ok:\n",
    "    block2_audit[\"errors\"].append(\"PDF directory/corpus unsatisfactory\")\n",
    "audit_log_path = AUDIT_DIR / \"block2_dirsetup_preflight.jsonl\"\n",
    "with open(audit_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(block2_audit, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "if not corpus_ok:\n",
    "    print(\"\\n[BLOCK 2 TERMINATED: Correct the PDF corpus directory and rerun this block.]\\n- No further code will execute in this run.\")\n",
    "    raise SystemExit(\"Block 2 failed closed: PDF corpus missing/empty.\")\n",
    "\n",
    "print(f\"\\n[INFO] pipeline_env.json has been written for deterministic use by all future blocks (no search, no prompts).\")\n",
    "print(\"--- End Block 2 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "NdbVTO28q5ST",
    "outputId": "95f69a8b-6b6c-4656-ceb6-4112d6b04f86"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Grobid] Found existing install.\n",
      "[Grobid] Starting Grobid server in background...\n",
      "[Grobid] Waiting for server to launch (up to 120s):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Waiting for Grobid:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    | 00:42 [01:25]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Grobid] Server is UP at http://localhost:8070/api/processHeaderDocument\n",
      "\n",
      "[GROBID_RUNNING=1] (0 = unavailable, 1 = running)\n",
      "GROBID_URL=http://localhost:8070/api/processHeaderDocument\n",
      "--- End Block 2.5 ---\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title [Block 2.5] Install & Launch Grobid Server (Colab/Local, NZDT Audit, Progress Bar, v6.3.1)\n",
    "#\n",
    "# ----------- [Version Control] -----------\n",
    "# v6.1: Colab/local Grobid install/launch/progress, healthcheck\n",
    "# v6.2: Audit logging, config support, progress bar, reviewer guidance\n",
    "# v6.2.3: Timestamp/zoneinfo in NZDT + UTC, audit file in correct run dir, provenance-logged status/report.\n",
    "# Version 6.3.1 \u2013 Checks, autorestarts, clear logs, NZ English\n",
    "#\n",
    "# ----------- [Block Summary] -----------\n",
    "# - Installs Java/Grobid, builds & launches Grobid server (Colab/local)\n",
    "# - NZDT and UTC audit log; audit file in correct run folder's audit dir.\n",
    "# - Progress bar for launch polling. Clear reviewer instructions and provenance trace.\n",
    "# - Sets envs GROBID_RUNNING, GROBID_URL for all downstream blocks; never fails-closed.\n",
    "#\n",
    "# ----------- [Start of Code] -----------\n",
    "\n",
    "\n",
    "import os, subprocess, time, requests, signal, json\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Audit/Config ---\n",
    "try:\n",
    "    import zoneinfo\n",
    "    TZ_NZ = zoneinfo.ZoneInfo(\"Pacific/Auckland\")\n",
    "    now_nzdt = datetime.now(TZ_NZ).isoformat()\n",
    "    now_utc = datetime.utcnow().isoformat() + \"Z\"\n",
    "except Exception:\n",
    "    now_nzdt = datetime.utcnow().isoformat()\n",
    "    now_utc  = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "# Find config and audit directories\n",
    "CONFIG_PATH = None\n",
    "for candidate in [\n",
    "    Path.cwd() / \"operational\" / \"config.yaml\",\n",
    "    Path.cwd() / \"config.yaml\",\n",
    "    Path.cwd().parent / \"operational\" / \"config.yaml\"]:\n",
    "    if candidate.exists():\n",
    "        CONFIG_PATH = candidate\n",
    "        break\n",
    "\n",
    "GROBID_HOME = Path(\"/content/grobid\")\n",
    "GROBID_URL  = \"http://localhost:8070/api/processHeaderDocument\"\n",
    "AUDIT_DIR   = Path(\"audit\")\n",
    "if CONFIG_PATH:\n",
    "    try:\n",
    "        with open(CONFIG_PATH, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            GROBID_URL = config.get(\"grobid_url\", GROBID_URL)\n",
    "            AUDIT_DIR = Path(config.get(\"audit_dir\", AUDIT_DIR))\n",
    "    except Exception: pass\n",
    "AUDIT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "HEALTH_URL = GROBID_URL.replace(\"/processHeaderDocument\", \"/isalive\")\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "    with open(AUDIT_DIR / \"block2p5_grobid_status.txt\", \"a\") as f:\n",
    "        f.write(str(msg) + \"\\n\")\n",
    "\n",
    "def check_grobid_healthy(url=HEALTH_URL):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=8)\n",
    "        return (r.status_code == 200) and (\"grobid\" in r.text.lower() or \"true\" in r.text.lower())\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def kill_grobid(port=8070):\n",
    "    # On Colab/Unix: kill process on the port if one exists\n",
    "    try:\n",
    "        out = subprocess.check_output(f\"lsof -i:{port} -t\", shell=True).decode().strip().split()\n",
    "        for pid in out:\n",
    "            os.kill(int(pid), signal.SIGKILL)\n",
    "            log(f\"[INFO] Killed previous process on port {port} (pid {pid})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "status_record = {\n",
    "    \"step\": \"block2.5_grobid_setup\",\n",
    "    \"timestamp_nzdt\": now_nzdt,\n",
    "    \"timestamp_utc\": now_utc,\n",
    "    \"grobid_url\": GROBID_URL,\n",
    "    \"already_installed\": False,\n",
    "    \"already_running\": False,\n",
    "    \"server_start_attempted\": False,\n",
    "    \"error\": None\n",
    "}\n",
    "\n",
    "# Step 1: Check for Grobid folder and try to check/rerun the server as needed\n",
    "try:\n",
    "    if not (GROBID_HOME / \"gradlew\").exists():\n",
    "        # Need to install Grobid\n",
    "        log(\"[Grobid] Installing OpenJDK 11 and cloning Grobid...\")\n",
    "        os.system('apt-get update')\n",
    "        os.system('apt-get install -y openjdk-11-jdk')\n",
    "        subprocess.check_call(['git', 'clone', 'https://github.com/kermitt2/grobid.git', str(GROBID_HOME)])\n",
    "        subprocess.check_call(['bash', '-c', f'cd {GROBID_HOME} && ./gradlew clean install'])\n",
    "        status_record[\"already_installed\"] = False\n",
    "    else:\n",
    "        log(\"[Grobid] Found existing install.\")\n",
    "        status_record[\"already_installed\"] = True\n",
    "\n",
    "    # Step 2: Health check\n",
    "    if check_grobid_healthy():\n",
    "        log(\"[Grobid] Grobid is ALREADY running and healthy.\")\n",
    "        status_record[\"already_running\"] = True\n",
    "        os.environ[\"GROBID_RUNNING\"] = \"1\"\n",
    "        os.environ[\"GROBID_URL\"] = GROBID_URL\n",
    "    else:\n",
    "        status_record[\"already_running\"] = False\n",
    "        # Kill any stray process on 8070 (if recoverable)\n",
    "        kill_grobid(8070)\n",
    "        # Attempt relaunch\n",
    "        log(\"[Grobid] Starting Grobid server in background...\")\n",
    "        subprocess.Popen([\"bash\", \"-c\", f\"cd {GROBID_HOME} && ./gradlew run > grobid_run.log 2>&1 &\"])\n",
    "        status_record[\"server_start_attempted\"] = True\n",
    "        # Wait for server launch/healthy\n",
    "        ready = False\n",
    "        log(\"[Grobid] Waiting for server to launch (up to 120s):\")\n",
    "        for i in tqdm(range(12), desc=\"Waiting for Grobid\", ncols=70, bar_format='{l_bar}{bar}| {elapsed} [{remaining}]'):\n",
    "            if check_grobid_healthy():\n",
    "                log(f\"[Grobid] Server is UP at {GROBID_URL}\")\n",
    "                ready = True\n",
    "                break\n",
    "            time.sleep(10)\n",
    "        if ready:\n",
    "            os.environ[\"GROBID_RUNNING\"] = \"1\"\n",
    "            os.environ[\"GROBID_URL\"] = GROBID_URL\n",
    "        else:\n",
    "            log(\"[WARNING] Grobid server did not start in time. Extraction will use fallback methods only.\")\n",
    "            os.environ[\"GROBID_RUNNING\"] = \"0\"\n",
    "            os.environ[\"GROBID_URL\"] = GROBID_URL\n",
    "\n",
    "except Exception as e:\n",
    "    status_record[\"error\"] = str(e)\n",
    "    log(f\"[ERROR] Grobid installation/setup failed: {e}\")\n",
    "    os.environ[\"GROBID_RUNNING\"] = \"0\"\n",
    "    os.environ[\"GROBID_URL\"] = GROBID_URL\n",
    "\n",
    "# Audit/status log\n",
    "with open(AUDIT_DIR / \"block2p5_grobid_setup.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(status_record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "grobid_run_status = os.environ.get(\"GROBID_RUNNING\", \"0\")\n",
    "print(f\"\\n[GROBID_RUNNING={grobid_run_status}] (0 = unavailable, 1 = running)\\nGROBID_URL={os.environ['GROBID_URL']}\")\n",
    "print(\"--- End Block 2.5 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.0] Extraction Pipeline Common Setup (v6.5.0)\n",
    "# Current Version 6.5.0\n",
    "#\n",
    "# Version Control Summaries\n",
    "# v6.5.0: Modularization base. Loads env/config, PDF paths, utilities, normalizers,\n",
    "#         logging helpers. For import or top-of-workspace in method blocks.\n",
    "#\n",
    "# Block Summary\n",
    "# - Loads/validates pipeline_env.json and config.yaml.\n",
    "# - Loads PDF file list as Path objects.\n",
    "# - Defines: normalization functions, print_and_log, hash helpers.\n",
    "# - Exports: OP_DIR, PDF_DIR, pdfs, config, fields, run_id, method_names, etc.\n",
    "# - All downstream extraction blocks should run after this in session.\n",
    "\n",
    "import os, re, json, yaml, hashlib, logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Pipeline Environment Loading ---\n",
    "with open(\"pipeline_env.json\", \"r\") as f: env = json.load(f)\n",
    "\n",
    "OP_DIR = Path(env[\"OPERATIONAL_DIR\"])\n",
    "AI_ARTIFACTS_DIR = Path(env.get(\"AI_ARTIFACTS_DIR\", \"operational/ai_artifacts\"))\n",
    "AI_ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG_PATH = OP_DIR / \"config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f: config = yaml.safe_load(f)\n",
    "\n",
    "PDF_DIR = Path(config[\"pdf_dir\"])\n",
    "pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))  # Always a list of pathlib.Path objects\n",
    "\n",
    "run_id = config.get(\"run_id\", \"unnamed_run\")\n",
    "fields = [\n",
    "    \"title\", \"author\", \"year\", \"doi\",\n",
    "    \"author_keywords\", \"country\", \"source_journal\", \"study_type\"\n",
    "]\n",
    "method_names = [\"llm\", \"grobid\", \"fitz\", \"pdfplumber\", \"filename\", \"crossref\", \"openalex\"]\n",
    "\n",
    "# --- Utility: Logging ---\n",
    "def print_and_log(msg, level=\"info\"):\n",
    "    print(msg)\n",
    "    with open(OP_DIR / \"block3_common_debug_log.txt\", \"a\") as f:\n",
    "        f.write(str(msg) + \"\\n\")\n",
    "\n",
    "# --- Utility: Normalize Fields ---\n",
    "def normalize_doi(x):\n",
    "    if not x or not isinstance(x, str): return \"\"\n",
    "    x = x.strip().lower().replace(' ', '')\n",
    "    x = re.sub(r\"^(https?://(dx\\.)?doi\\.org/)\", \"\", x)\n",
    "    x = re.sub(r\"\\s\", \"\", x)\n",
    "    return x\n",
    "\n",
    "def normalize_author(raw):\n",
    "    if not raw: return \"\"\n",
    "    if isinstance(raw, list): vals = raw\n",
    "    else: vals = [raw]\n",
    "    def norm_piece(x):\n",
    "        if not x: return \"\"\n",
    "        x = str(x)\n",
    "        x = x.replace(\",\", \"\")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "        return x\n",
    "    flat = []\n",
    "    for v in vals:\n",
    "        if isinstance(v, str):\n",
    "            flat += [norm_piece(w) for w in re.split(r\";|,|&| and \", v) if w.strip()]\n",
    "        else:\n",
    "            flat.append(norm_piece(v))\n",
    "    seen, out = set(), []\n",
    "    for a in flat:\n",
    "        a_lc = a.lower()\n",
    "        if a_lc and a_lc not in seen:\n",
    "            out.append(a)\n",
    "            seen.add(a_lc)\n",
    "    return \"; \".join(out)\n",
    "\n",
    "def normalize_country(raw):\n",
    "    ISO_MAP = {'us': 'United States', 'gb':'United Kingdom', 'uk':'United Kingdom', 'au':'Australia', 'nz':'New Zealand', 'ca':'Canada'}\n",
    "    if not raw: return \"\"\n",
    "    vals = [w.strip() for w in str(raw).split(';') if w.strip()]\n",
    "    names = []\n",
    "    for v in vals:\n",
    "        v_lc = v.lower()\n",
    "        n = ISO_MAP.get(v_lc)\n",
    "        if n: names.append(n)\n",
    "        else:\n",
    "            c = re.sub(r'[^a-zA-Z ]+', '', v).strip()\n",
    "            if c and c.lower() not in [n.lower() for n in names]:\n",
    "                names.append(c)\n",
    "    return \"; \".join(dict.fromkeys(names))\n",
    "\n",
    "def normalize_keywords(raw):\n",
    "    if not raw: return \"\"\n",
    "    if isinstance(raw, list): vals = raw\n",
    "    else: vals = [raw]\n",
    "    flat = []\n",
    "    for v in vals:\n",
    "        if isinstance(v, str):\n",
    "            flat += [k.strip() for k in re.split(r\";|,|/|\\|\", v) if k.strip()]\n",
    "        else:\n",
    "            flat.append(str(v).strip())\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for k in flat:\n",
    "        kl = k.lower()\n",
    "        if kl and kl not in seen:\n",
    "            uniq.append(k)\n",
    "            seen.add(kl)\n",
    "    return \"; \".join(sorted(uniq, key=str.lower))\n",
    "\n",
    "# --- File Hash Helper (Useful for Version/Audit) ---\n",
    "def sha256_file(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Export: Common Config Info ---\n",
    "print(f\"[Block 3.0] Loaded {len(pdfs)} PDF(s) from: {PDF_DIR}\")\n",
    "print(f\"[Block 3.0] Available fields: {fields}\")\n",
    "print(f\"[Block 3.0] AI_ARTIFACTS_DIR: {AI_ARTIFACTS_DIR}\")\n",
    "print(f\"[Block 3.0] run_id: {run_id}\")\n",
    "\n",
    "# --- No main code: this block is for session setup/exports, not per-paper logic ---"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eNCQ9DmQvsQW",
    "outputId": "7206c9b4-cbb9-4829-ac75-022f2dc73dc5"
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Block 3.0] Loaded 4 PDF(s) from: /content/drive/My Drive/Pilot/PDFs\n",
      "[Block 3.0] Available fields: ['title', 'author', 'year', 'doi', 'author_keywords', 'country', 'source_journal', 'study_type']\n",
      "[Block 3.0] AI_ARTIFACTS_DIR: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts\n",
      "[Block 3.0] run_id: Nurse-AI_ScR_250519_1642\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.1] LLM Metadata Extraction Only (Colab/Env Secure Input, v6.5.2)\n",
    "# Version 6.5.2\n",
    "#\n",
    "# - Securely prompts for API key on Colab if not set (never prints or stores in code).\n",
    "# - Works everywhere, no Google Colab Pro features required.\n",
    "\n",
    "import json, hashlib, os, re\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "def extract_first_json(text):\n",
    "    text = re.sub(r\"^```(?:json)?\", \"\", text, flags=re.MULTILINE).strip('` \\n')\n",
    "    depth = 0\n",
    "    start, end = -1, -1\n",
    "    for i, c in enumerate(text):\n",
    "        if c == '{':\n",
    "            if depth == 0:\n",
    "                start = i\n",
    "            depth += 1\n",
    "        elif c == '}':\n",
    "            if depth > 0:\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    end = i\n",
    "                    break\n",
    "    if start != -1 and end != -1:\n",
    "        return text[start:end+1]\n",
    "    return \"\"\n",
    "\n",
    "def flatten(val): return \"; \".join(str(x) for x in val) if isinstance(val, list) else str(val)\n",
    "\n",
    "# --- Prompt user for secret if needed (always secure, never logs the secret itself) ---\n",
    "def get_openai_api_key():\n",
    "    keyname = config.get(\"openai_key_envvar\", \"OPENAI_API_KEY\")\n",
    "    # 1. Try environment variable\n",
    "    val = os.environ.get(keyname, \"\")\n",
    "    if val and val.strip():\n",
    "        return val\n",
    "    # 2. Try Colab userdata.get (if available)\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        val2 = userdata.get(keyname)\n",
    "        if val2 and val2.strip():\n",
    "            return val2\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3. Prompt interactively for key (never echoed, never stored in notebook)\n",
    "    try:\n",
    "        import getpass\n",
    "        val3 = getpass.getpass(f'Paste your OpenAI API key for {keyname}: ')\n",
    "        if val3 and val3.strip():\n",
    "            os.environ[keyname] = val3.strip()  # So it works for rest of pipeline\n",
    "            return val3.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def extract_ai_llm_full(first_page, api_key=None, model=None, debug=False):\n",
    "    error = None\n",
    "    # Use the get_openai_api_key logic above by default\n",
    "    api_key = api_key or get_openai_api_key()\n",
    "    if not api_key or not api_key.strip():\n",
    "        error = (\n",
    "            \"[CRIT] OpenAI API key missing/blank for LLM calls!\\n\"\n",
    "            \"If in Colab, you will be prompted for your key securely.\\n\"\n",
    "            \"Else, set envvar OPENAI_API_KEY.\"\n",
    "        )\n",
    "        print_and_log(error)\n",
    "        return {k:\"\" for k in fields}, error\n",
    "    try:\n",
    "        import openai\n",
    "        openai.api_key = api_key\n",
    "        prompt = (\n",
    "            \"Extract the following metadata as a JSON object from the text provided: \"\n",
    "            \"title, author, year, doi, author_keywords, country, source_journal, study_type. \"\n",
    "            \"If a field is missing, leave blank or use null. Text follows:\\n\" + first_page\n",
    "        )\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=model or config.get(\"llm_model\"),\n",
    "            messages=[{\"role\":\"user\", \"content\": prompt}],\n",
    "            temperature=0, max_tokens=384\n",
    "        )\n",
    "        txt = resp.choices[0].message.content.strip()\n",
    "        if debug:\n",
    "            print_and_log(f\"[DEBUG] LLM RAW RESPONSE:\\n{txt[:400]}\\n---\")\n",
    "        raw_json = extract_first_json(txt)\n",
    "        if not raw_json:\n",
    "            raise ValueError(\"No JSON block found in LLM output.\")\n",
    "        try:\n",
    "            result = json.loads(raw_json)\n",
    "        except Exception as e_inner:\n",
    "            raw_json2 = re.sub(r',\\s*([}\\]])', r'\\1', raw_json)\n",
    "            try:\n",
    "                result = json.loads(raw_json2)\n",
    "            except Exception as e2:\n",
    "                error = f\"[ERROR] LLM JSON parsing failed: {e_inner} | {e2}\"\n",
    "                print_and_log(error)\n",
    "                fname = OP_DIR / f\"llm_raw_{hashlib.sha1(txt.encode('utf-8')).hexdigest()[:8]}.txt\"\n",
    "                with open(fname, \"w\") as f:\n",
    "                    f.write(txt)\n",
    "                return {k: \"\" for k in fields}, f\"{error} | RAW SAVED to {fname}\"\n",
    "        output = {k: flatten(result.get(k, \"\")) if isinstance(result, dict) else \"\" for k in fields}\n",
    "        return output, None\n",
    "    except Exception as e:\n",
    "        error = f\"[ERROR] LLM extraction failed: {e}\"\n",
    "        print_and_log(error)\n",
    "        fname = OP_DIR / f\"llm_raw_fail_{hashlib.sha1(str(e).encode('utf-8')).hexdigest()[:8]}.txt\"\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(str(e))\n",
    "        return {k: \"\" for k in fields}, error\n",
    "\n",
    "llm_results = []\n",
    "for idx, pdf in enumerate(pdfs):\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    errors = []\n",
    "    try:\n",
    "        import fitz\n",
    "        doc = fitz.open(pdf)\n",
    "        first_page = doc[0].get_text()[:3500]\n",
    "    except Exception as e:\n",
    "        first_page = \"\"\n",
    "        errors.append(f\"[ERROR] fitz text extraction: {e}\")\n",
    "    llm, err_llm = extract_ai_llm_full(first_page)\n",
    "    if err_llm: errors.append(err_llm)\n",
    "    normd = {\n",
    "        \"pdf_id\": pdf_id,\n",
    "        \"pdf_filename\": str(pdf.name),\n",
    "        \"llm_extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\"\n",
    "    }\n",
    "    for k, v in llm.items():\n",
    "        if k == \"doi\":\n",
    "            normd[k] = normalize_doi(v)\n",
    "        elif k == \"author\":\n",
    "            normd[k] = normalize_author(v)\n",
    "        elif k == \"country\":\n",
    "            normd[k] = normalize_country(v)\n",
    "        elif k == \"author_keywords\":\n",
    "            normd[k] = normalize_keywords(v)\n",
    "        else:\n",
    "            normd[k] = v or \"\"\n",
    "    normd[\"error_log\"] = errors\n",
    "    llm_results.append(normd)\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"llm_results_{run_id}.json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(llm_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[Block 3.1] Wrote LLM results to {outfile} for {len(llm_results)} papers.\")\n",
    "\n",
    "for r in llm_results[:3]:\n",
    "    print(f\"\\n{r['pdf_filename']}\\nTitle: {r['title']} | Author(s): {r['author']} | Year: {r['year']} | DOI: {r['doi']}\")\n",
    "\n",
    "print(\"--- End Block 3.1 (LLM Extraction Only, Colab/Env Secure Prompt) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sFtOzo5JvteA",
    "outputId": "3fcf86c1-45f1-43a6-9687-81fd53b86bb2"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Block 3.1] Wrote LLM results to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/llm_results_Nurse-AI_ScR_250519_1642.json for 4 papers.\n",
      "\n",
      "Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "Title: Machine Learning Model to Extract Malnutrition Data from Nursing Notes | Author(s): Mohammad ALKHALAF; Mengyang YIN; Chao DENG; Hui-Chen (Rita) CHANG; Ping YU | Year: 2024 | DOI: 10.3233/shti231240\n",
      "\n",
      "Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "Title: Human-AI Teaming in the ICU: A Comparative Analysis of Data Scientists' and Clinicians\u2019 Assessments on AI Augmentation and Automation at Work | Author(s): Nadine Bienefeld; Emanuela Keller; Gudela Grote | Year: 2023 | DOI: none\n",
      "\n",
      "Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      "Title: AI-Based Tailored Mobile Intervention for Nurse Burnout: Development Study | Author(s): Chiyoung Cha; Aram Cho; Gumhee Baek | Year: 2023 | DOI: none\n",
      "--- End Block 3.1 (LLM Extraction Only, Colab/Env Secure Prompt) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.2] Grobid Extraction with TEI Namespace Correction (v6.5.4)\n",
    "# Version 6.5.4\n",
    "#\n",
    "# - Correctly parses Grobid TEI XML using xmlns namespace aware XPath.\n",
    "# - All major fields should now be extracted if present in the TEI.\n",
    "# - TEI debug (preview/sample) retained for diagnosis.\n",
    "\n",
    "import json, hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "grobid_url = config.get(\"grobid_url\", \"http://localhost:8070/api/processHeaderDocument\")\n",
    "DEBUG_TEI_N = 3    # Save/print TEI debug for first N PDFs\n",
    "\n",
    "NS = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "def extract_grobid_full(pdf_file, pdf_id, tei_debug_dir=None, verbose=False):\n",
    "    meta, err, tei = {f:\"\" for f in fields}, None, \"\"\n",
    "    tei_path = None\n",
    "    try:\n",
    "        with open(pdf_file, \"rb\") as f:\n",
    "            resp = requests.post(grobid_url, files={'input': f}, timeout=60)\n",
    "        tei = resp.text\n",
    "        if verbose:\n",
    "            print(f\"PDF {pdf_file.name}: TEI length {len(tei)}\")\n",
    "        if tei_debug_dir is not None and tei and '<TEI' in tei:\n",
    "            tei_path = tei_debug_dir / f\"{pdf_id}_tei.xml\"\n",
    "            with open(tei_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(tei)\n",
    "        if tei and '<TEI' in tei:\n",
    "            tree = ET.fromstring(tei)\n",
    "            # --- Namespace-aware field extraction ---\n",
    "            title_el = tree.find('.//tei:titleStmt/tei:title', NS)\n",
    "            meta[\"title\"] = title_el.text.strip() if title_el is not None and title_el.text else \"\"\n",
    "            authors = []\n",
    "            for pers in tree.findall('.//tei:author/tei:persName', NS):\n",
    "                surname = pers.find('tei:surname', NS)\n",
    "                forename = pers.find('tei:forename', NS)\n",
    "                a = (forename.text if forename is not None else \"\")\n",
    "                b = (surname.text if surname is not None else \"\")\n",
    "                val = (a + \" \" + b).strip()\n",
    "                if val: authors.append(val)\n",
    "            meta[\"author\"] = \"; \".join(authors)\n",
    "            doi_el = tree.find('.//tei:idno[@type=\"DOI\"]', NS)\n",
    "            meta[\"doi\"] = doi_el.text.strip() if doi_el is not None and doi_el.text else \"\"\n",
    "            date_el = tree.find('.//tei:publicationStmt/tei:date', NS)\n",
    "            meta[\"year\"] = date_el.attrib['when'][:4] if date_el is not None and 'when' in date_el.attrib else \"\"\n",
    "            j = tree.find('.//tei:monogr/tei:title', NS)\n",
    "            meta[\"source_journal\"] = j.text.strip() if j is not None and j.text else \"\"\n",
    "            kws = [k.text.strip() for k in tree.findall('.//tei:keywords/tei:term', NS) if k.text]\n",
    "            meta[\"author_keywords\"] = \"; \".join(kws)\n",
    "            ptype = tree.find('.//tei:note[@type=\"studyType\"]', NS)\n",
    "            meta[\"study_type\"] = ptype.text.strip() if ptype is not None and ptype.text else \"\"\n",
    "            countries = []\n",
    "            for aff in tree.findall('.//tei:affiliation', NS):\n",
    "                country_el = aff.find('.//tei:country', NS)\n",
    "                if country_el is not None and country_el.text:\n",
    "                    countries.append(country_el.text.strip())\n",
    "            meta[\"country\"] = \"; \".join(set(countries))\n",
    "        else:\n",
    "            err = f\"[WARN] Malformed or empty TEI from Grobid (length={len(tei)})\"\n",
    "            print_and_log(err)\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] Grobid XML: {e}\"\n",
    "        print_and_log(err)\n",
    "    return meta, err, tei, tei_path\n",
    "\n",
    "tei_debug_dir = AI_ARTIFACTS_DIR / \"grobid_tei_debug\"\n",
    "tei_debug_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "grobid_results = []\n",
    "for idx, pdf in enumerate(pdfs):\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    errors, tei_path = [], None\n",
    "    meta, err_grobid, tei, saved_tei_path = extract_grobid_full(pdf, pdf_id, tei_debug_dir if idx < DEBUG_TEI_N else None, verbose=True)\n",
    "    if err_grobid: errors.append(err_grobid)\n",
    "    if saved_tei_path:\n",
    "        errors.append(f\"[INFO] TEI saved in {saved_tei_path}\")\n",
    "    normd = {\n",
    "        \"pdf_id\": pdf_id,\n",
    "        \"pdf_filename\": str(pdf.name),\n",
    "        \"grobid_extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"tei_length\": len(tei),\n",
    "        \"tei_sample\": (tei[:450] + \"...\") if tei else \"\",\n",
    "    }\n",
    "    for k, v in meta.items():\n",
    "        if k == \"doi\":\n",
    "            normd[k] = normalize_doi(v)\n",
    "        elif k == \"author\":\n",
    "            normd[k] = normalize_author(v)\n",
    "        elif k == \"country\":\n",
    "            normd[k] = normalize_country(v)\n",
    "        elif k == \"author_keywords\":\n",
    "            normd[k] = normalize_keywords(v)\n",
    "        else:\n",
    "            normd[k] = v or \"\"\n",
    "    normd[\"error_log\"] = errors\n",
    "    grobid_results.append(normd)\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"grobid_results_{run_id}.json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grobid_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[Block 3.2] Wrote Grobid results to {outfile} for {len(grobid_results)} papers.\")\n",
    "for r in grobid_results[:3]:\n",
    "    print(f\"\\n{r['pdf_filename']}\\nTitle: {r['title']} | Author(s): {r['author']} | Year: {r['year']} | DOI: {r['doi']}\\nTEI preview: {r['tei_sample'][:180]} ...\")\n",
    "print(\"\\nCheck ai_artifacts/grobid_tei_debug/{pdf_id}_tei.xml for raw TEI from first few papers.\")\n",
    "print(\"--- End Block 3.2 (Grobid Extraction, Namespace-corrected) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dSOS2aIHvtwM",
    "outputId": "ff0c5844-5b30-4cd8-e47b-39b072fd98ed"
   },
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PDF Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf: TEI length 5131\n",
      "PDF Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf: TEI length 2538\n",
      "PDF Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf: TEI length 2262\n",
      "PDF Copy of Copy of Dubin et al. - 2024 - Appropriateness of Frequently Asked Patient Questi.pdf: TEI length 9806\n",
      "[Block 3.2] Wrote Grobid results to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/grobid_results_Nurse-AI_ScR_250519_1642.json for 4 papers.\n",
      "\n",
      "Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "Title: Machine Learning Model to Extract Malnutrition Data from Nursing Notes | Author(s): Mohammad Alkhalaf; Mengyang Yin; Chao Deng; Rita Chang; Ping Yu | Year:  | DOI: \n",
      "TEI preview: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" \n",
      "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n",
      "xsi:schemaLocation=\"htt ...\n",
      "\n",
      "Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "Title: Human-AI Teaming in the ICU: A Comparative Analysis of Data Scientists' and Clinicians' Assessments on AI Augmentation and Automation at Work | Author(s): Nadine Bienefeld; Emanuela Keller; Gudela Grote | Year: 2023 | DOI: \n",
      "TEI preview: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" \n",
      "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n",
      "xsi:schemaLocation=\"htt ...\n",
      "\n",
      "Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      "Title: AI-Based Tailored Mobile Intervention for Nurse Burnout: Development Study | Author(s): Chiyoung Cha; Aram Cho; Gumhee Baek | Year:  | DOI: \n",
      "TEI preview: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" \n",
      "xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n",
      "xsi:schemaLocation=\"htt ...\n",
      "\n",
      "Check ai_artifacts/grobid_tei_debug/{pdf_id}_tei.xml for raw TEI from first few papers.\n",
      "--- End Block 3.2 (Grobid Extraction, Namespace-corrected) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.3] Embedded PDF Metadata Extraction (fitz + pdfplumber) (v6.5.0)\n",
    "# Version 6.5.0\n",
    "#\n",
    "# - Extracts embedded metadata via fitz (PyMuPDF) and pdfplumber for each PDF.\n",
    "# - Normalizes results, logs errors, and provides per-paper, per-method results.\n",
    "# - Output: ai_artifacts/pdfmeta_results_{run_id}.json\n",
    "\n",
    "import json, hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "try:\n",
    "    import fitz\n",
    "except ImportError:\n",
    "    raise ImportError(\"fitz (PyMuPDF) not found. Please install via pip.\")\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "except ImportError:\n",
    "    raise ImportError(\"pdfplumber not found. Please install via pip.\")\n",
    "\n",
    "def extract_fitz_metadata(pdf_path):\n",
    "    meta, err = {f:\"\" for f in fields}, None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        m = doc.metadata\n",
    "        meta[\"title\"] = m.get(\"title\", \"\") or m.get(\"Title\", \"\")\n",
    "        meta[\"author\"] = m.get(\"author\", \"\") or m.get(\"Author\", \"\")\n",
    "        meta[\"year\"] = str(m.get(\"modDate\", \"\")[2:6]) if m.get(\"modDate\", \"\") else \"\"\n",
    "        meta[\"author_keywords\"] = m.get(\"keywords\", \"\") or m.get(\"Keywords\", \"\")\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] fitz metadata: {e}\"\n",
    "        print_and_log(err)\n",
    "    return meta, err\n",
    "\n",
    "def extract_pdfplumber_metadata(pdf_path):\n",
    "    meta, err = {f:\"\" for f in fields}, None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as p:\n",
    "            info = p.metadata or {}\n",
    "            meta[\"title\"] = info.get(\"Title\", \"\")\n",
    "            meta[\"author\"] = info.get(\"Author\", \"\")\n",
    "            meta[\"year\"] = info.get(\"ModDate\", \"\")[2:6] if info.get(\"ModDate\",\"\") else \"\"\n",
    "            meta[\"author_keywords\"] = info.get(\"Keywords\", \"\")\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] pdfplumber metadata: {e}\"\n",
    "        print_and_log(err)\n",
    "    return meta, err\n",
    "\n",
    "pdfmeta_results = []\n",
    "for idx, pdf in enumerate(pdfs):\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    errors = []\n",
    "\n",
    "    fitz_m, err_fitz = extract_fitz_metadata(pdf)\n",
    "    if err_fitz: errors.append(err_fitz)\n",
    "\n",
    "    pdfplumber_m, err_pdfplumber = extract_pdfplumber_metadata(pdf)\n",
    "    if err_pdfplumber: errors.append(err_pdfplumber)\n",
    "\n",
    "    normd = {\n",
    "        \"pdf_id\": pdf_id,\n",
    "        \"pdf_filename\": str(pdf.name),\n",
    "        \"fitz_title\": normalize_author(fitz_m[\"title\"]),\n",
    "        \"fitz_author\": normalize_author(fitz_m[\"author\"]),\n",
    "        \"fitz_year\": fitz_m[\"year\"],\n",
    "        \"fitz_author_keywords\": normalize_keywords(fitz_m[\"author_keywords\"]),\n",
    "        \"pdfplumber_title\": normalize_author(pdfplumber_m[\"title\"]),\n",
    "        \"pdfplumber_author\": normalize_author(pdfplumber_m[\"author\"]),\n",
    "        \"pdfplumber_year\": pdfplumber_m[\"year\"],\n",
    "        \"pdfplumber_author_keywords\": normalize_keywords(pdfplumber_m[\"author_keywords\"]),\n",
    "        \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"error_log\": errors\n",
    "    }\n",
    "    pdfmeta_results.append(normd)\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"pdfmeta_results_{run_id}.json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pdfmeta_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[Block 3.3] Wrote PDF embedded metadata results to {outfile} for {len(pdfmeta_results)} papers.\")\n",
    "for r in pdfmeta_results[:3]:\n",
    "    print(f\"\\n{r['pdf_filename']}\\n[fitz] Title: {r['fitz_title']} | Author(s): {r['fitz_author']} | Year: {r['fitz_year']}\\n[pdfplumber] Title: {r['pdfplumber_title']} | Author(s): {r['pdfplumber_author']} | Year: {r['pdfplumber_year']}\")\n",
    "print(\"--- End Block 3.3 (PDF Embedded Metadata Extraction) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vyHlZC8VvtzT",
    "outputId": "77701f09-101b-4cf7-89c7-60f21329e8b2"
   },
   "execution_count": 54,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Block 3.3] Wrote PDF embedded metadata results to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/pdfmeta_results_Nurse-AI_ScR_250519_1642.json for 4 papers.\n",
      "\n",
      "Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "[fitz] Title:  | Author(s):  | Year: 2024\n",
      "[pdfplumber] Title:  | Author(s):  | Year: 2024\n",
      "\n",
      "Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "[fitz] Title: Preprint - 10.2196/50130 | Author(s):  | Year: 2024\n",
      "[pdfplumber] Title: Preprint - 10.2196/50130 | Author(s):  | Year: 2024\n",
      "\n",
      "Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      "[fitz] Title: Preprint - 10.2196/54029 | Author(s):  | Year: 2024\n",
      "[pdfplumber] Title: Preprint - 10.2196/54029 | Author(s):  | Year: 2024\n",
      "--- End Block 3.3 (PDF Embedded Metadata Extraction) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.4] Improved Filename/Regex Metadata Extraction (v6.5.1)\n",
    "# Version 6.5.1\n",
    "#\n",
    "# - Smarter \"author\" extraction: Removes 'Copy of ' and other common prefixes, normalizes\n",
    "# - Tolerant to spaces, underscores, multiple \"Copy of\" repeats, \"v1 -\" or \"Final -\"\n",
    "# - Output: ai_artifacts/filename_results_{run_id}.json\n",
    "\n",
    "import json, hashlib, re\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "# Helper to strip common prefix junk before 'author'\n",
    "def clean_author_string(author_raw):\n",
    "    # Remove leading \"Copy of\", \"Final\", \"v1\", or multiples thereof\n",
    "    author = author_raw\n",
    "    author = re.sub(r\"^(copy of\\s*|\\s*final\\s*|\\s*v\\d+\\s*|\\s*-+\\s*)+\", \"\", author, flags=re.IGNORECASE)\n",
    "    author = author.replace(\"_\", \" \").replace(\"-\", \" \").strip()\n",
    "    # Remove trailing repeated spaces, dots or junk\n",
    "    author = re.sub(r'\\.+$', \"\", author).strip()\n",
    "    # Make sure it's not just empty or a number\n",
    "    return author if (len(author) > 2 and not author.strip().isdigit()) else \"\"\n",
    "\n",
    "def extract_from_filename(pdf_path):\n",
    "    # More robust: cleans at start, matches main academic export pattern\n",
    "    base = str(pdf_path.name)\n",
    "    result = {\n",
    "        \"author\": \"\",\n",
    "        \"year\": \"\",\n",
    "        \"title\": \"\",\n",
    "        \"doi\": \"\",\n",
    "    }\n",
    "    error = None\n",
    "    m = re.match(r\"(.+?)\\s*-\\s*(\\d{4})\\s*-\\s*(.+)\\.pdf$\", base, re.IGNORECASE)\n",
    "    if m:\n",
    "        raw_author, result[\"year\"], result[\"title\"] = m.groups()\n",
    "        result[\"author\"] = clean_author_string(raw_author)\n",
    "    else:\n",
    "        # Try backup, less strict\n",
    "        m2 = re.match(r\"(.+?)\\s*-\\s*(\\d{4})\\s*-\\s*(.+)\", base, re.IGNORECASE)\n",
    "        if m2:\n",
    "            raw_author, result[\"year\"], result[\"title\"] = m2.groups()\n",
    "            result[\"author\"] = clean_author_string(raw_author)\n",
    "        else:\n",
    "            error = f\"[WARN] Filename did not match expected pattern: '{base}'\"\n",
    "    # Try extracting DOI from base/filename (remove .pdf for search)\n",
    "    base_noext = re.sub(r\"\\.pdf$\", \"\", base, flags=re.IGNORECASE)\n",
    "    m_doi = re.search(r\"(10\\.\\d{4,9}/[\\w\\.\\-\\/]+)\", base_noext, re.IGNORECASE)\n",
    "    if m_doi:\n",
    "        result[\"doi\"] = m_doi.group(1)\n",
    "    return result, error\n",
    "\n",
    "filename_results = []\n",
    "for idx, pdf in enumerate(pdfs):\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    fnres, warn = extract_from_filename(pdf)\n",
    "    errors = []\n",
    "    if warn:\n",
    "        errors.append(warn)\n",
    "    normd = {\n",
    "        \"pdf_id\": pdf_id,\n",
    "        \"pdf_filename\": str(pdf.name),\n",
    "        \"fn_author\": normalize_author(fnres[\"author\"]),\n",
    "        \"fn_year\": fnres[\"year\"],\n",
    "        \"fn_title\": fnres[\"title\"].replace(\"_\", \" \").strip(),\n",
    "        \"fn_doi\": normalize_doi(fnres[\"doi\"]),\n",
    "        \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"error_log\": errors\n",
    "    }\n",
    "    filename_results.append(normd)\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"filename_results_{run_id}.json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filename_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[Block 3.4] Wrote filename/regex extraction results to {outfile} for {len(filename_results)} papers.\")\n",
    "for r in filename_results[:3]:\n",
    "    print(f\"\\n{r['pdf_filename']}\\nAuthor(s): {r['fn_author']} | Year: {r['fn_year']} | Title: {r['fn_title']} | DOI: {r['fn_doi']} | ERROR: {r['error_log']}\")\n",
    "print(\"--- End Block 3.4 (Improved Filename/Regex Metadata Extraction) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "z8krlN470ELs",
    "outputId": "479e3865-a10c-447b-ddc6-8730a02b45fd"
   },
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Block 3.4] Wrote filename/regex extraction results to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/filename_results_Nurse-AI_ScR_250519_1642.json for 4 papers.\n",
      "\n",
      "Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "Author(s): Alkhalaf et al | Year: 2024 | Title: Machine Learning Model to Extract Malnutrition Dat | DOI:  | ERROR: []\n",
      "\n",
      "Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "Author(s): Bienefeld et al | Year: 2024 | Title: Human-AI Teaming in Critical Care A Comparative A | DOI:  | ERROR: []\n",
      "\n",
      "Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      "Author(s): Cho et al | Year: 2024 | Title: Development of an Artificial Intelligence-Based Ta | DOI:  | ERROR: []\n",
      "--- End Block 3.4 (Improved Filename/Regex Metadata Extraction) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.5] CrossRef Metadata Extraction (auto-detect missing inputs, v6.5.1)\n",
    "# Version 6.5.1\n",
    "#\n",
    "# - Tries to load all prior method outputs but will proceed and warn even if some are missing\n",
    "# - Prioritizes: Grobid > LLM > Filename for DOI/title/author query\n",
    "# - Skips gracefully based on allow_internet\n",
    "# - Output: ai_artifacts/crossref_results_{run_id}.json\n",
    "\n",
    "import json, hashlib, requests\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "def try_load(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            out = json.load(f)\n",
    "        print(f\"[INFO] Loaded {path.name}, {len(out)} records.\")\n",
    "        return {r[\"pdf_id\"]: r for r in out}\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not load {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "allow_internet = config.get(\"allow_internet\", True)\n",
    "grobid_path = AI_ARTIFACTS_DIR / f\"grobid_results_{run_id}.json\"\n",
    "llm_path = AI_ARTIFACTS_DIR / f\"llm_results_{run_id}.json\"\n",
    "filename_path = AI_ARTIFACTS_DIR / f\"filename_results_{run_id}.json\"\n",
    "\n",
    "grobid_res = try_load(grobid_path)\n",
    "llm_res = try_load(llm_path)\n",
    "fn_res = try_load(filename_path)\n",
    "\n",
    "def extract_crossref_metadata(doi, title, author):\n",
    "    out, err = {f: \"\" for f in fields}, None\n",
    "    try:\n",
    "        if doi:\n",
    "            url = f\"https://api.crossref.org/works/{normalize_doi(doi)}\"\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code != 200:\n",
    "                err = f\"[WARN] CrossRef DOI {doi} query failed: HTTP {r.status_code}\"\n",
    "                return out, err\n",
    "            dat = r.json().get(\"message\", {})\n",
    "        elif title and author:\n",
    "            q = f\"{title} {author}\"\n",
    "            url = f\"https://api.crossref.org/works?query.bibliographic={requests.utils.quote(q)}&rows=1\"\n",
    "            r = requests.get(url, timeout=15)\n",
    "            if r.status_code != 200:\n",
    "                err = f\"[WARN] CrossRef bibliographic query ('{q}') failed: HTTP {r.status_code}\"\n",
    "                return out, err\n",
    "            items = r.json().get(\"message\", {}).get(\"items\", [])\n",
    "            dat = items[0] if items else {}\n",
    "        else:\n",
    "            err = \"[WARN] No DOI or title/author, cannot query CrossRef\"\n",
    "            return out, err\n",
    "        out[\"doi\"] = dat.get(\"DOI\", \"\")\n",
    "        out[\"title\"] = dat.get(\"title\", [\"\"])[0] if dat.get(\"title\") else \"\"\n",
    "        out[\"author\"] = \"; \".join(\n",
    "            \"{}, {}\".format(a.get(\"family\", \"\").strip(), a.get(\"given\", \"\").strip())\n",
    "            for a in dat.get(\"author\", [])\n",
    "        ) if dat.get(\"author\") else \"\"\n",
    "        out[\"year\"] = str(dat.get(\"issued\", {}).get(\"date-parts\", [[None]])[0][0]) if dat.get(\"issued\") else \"\"\n",
    "        out[\"author_keywords\"] = \"; \".join(dat.get(\"subject\", [])) if dat.get(\"subject\") else \"\"\n",
    "        out[\"source_journal\"] = dat.get(\"container-title\", [\"\"])[0] if dat.get(\"container-title\") else \"\"\n",
    "        out[\"study_type\"] = dat.get(\"type\", \"\")\n",
    "        out[\"country\"] = \"\"\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] CrossRef error: {e}\"\n",
    "        print_and_log(err)\n",
    "    return out, err\n",
    "\n",
    "crossref_results = []\n",
    "for pdf in pdfs:\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    errors = []\n",
    "    grob = grobid_res.get(pdf_id, {})\n",
    "    llm = llm_res.get(pdf_id, {})\n",
    "    fn = fn_res.get(pdf_id, {})\n",
    "    candidates = [\n",
    "        dict(doi=grob.get(\"doi\") or \"\", title=grob.get(\"title\") or \"\", author=grob.get(\"author\") or \"\"),\n",
    "        dict(doi=llm.get(\"doi\") or \"\", title=llm.get(\"title\") or \"\", author=llm.get(\"author\") or \"\"),\n",
    "        dict(doi=fn.get(\"fn_doi\") or \"\", title=fn.get(\"fn_title\") or \"\", author=fn.get(\"fn_author\") or \"\")\n",
    "    ]\n",
    "    candidate = next((c for c in candidates if c[\"doi\"] or (c[\"title\"] and c[\"author\"])), None)\n",
    "    if not allow_internet:\n",
    "        normd = {\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"pdf_filename\": str(pdf.name),\n",
    "            \"crossref_doi\": \"\",\n",
    "            \"crossref_title\": \"\",\n",
    "            \"crossref_author\": \"\",\n",
    "            \"crossref_year\": \"\",\n",
    "            \"crossref_journal\": \"\",\n",
    "            \"crossref_keywords\": \"\",\n",
    "            \"crossref_type\": \"\",\n",
    "            \"crossref_country\": \"\",\n",
    "            \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"error_log\": [\"[INFO] Skipped: allow_internet is False\"]\n",
    "        }\n",
    "    elif candidate:\n",
    "        cr, err = extract_crossref_metadata(candidate[\"doi\"], candidate[\"title\"], candidate[\"author\"])\n",
    "        normd = {\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"pdf_filename\": str(pdf.name),\n",
    "            \"crossref_doi\": normalize_doi(cr[\"doi\"]),\n",
    "            \"crossref_title\": cr[\"title\"],\n",
    "            \"crossref_author\": normalize_author(cr[\"author\"]),\n",
    "            \"crossref_year\": cr[\"year\"],\n",
    "            \"crossref_journal\": cr[\"source_journal\"],\n",
    "            \"crossref_keywords\": normalize_keywords(cr[\"author_keywords\"]),\n",
    "            \"crossref_type\": cr[\"study_type\"],\n",
    "            \"crossref_country\": normalize_country(cr[\"country\"]),\n",
    "            \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"error_log\": [err] if err else []\n",
    "        }\n",
    "    else:\n",
    "        normd = {\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"pdf_filename\": str(pdf.name),\n",
    "            \"crossref_doi\": \"\",\n",
    "            \"crossref_title\": \"\",\n",
    "            \"crossref_author\": \"\",\n",
    "            \"crossref_year\": \"\",\n",
    "            \"crossref_journal\": \"\",\n",
    "            \"crossref_keywords\": \"\",\n",
    "            \"crossref_type\": \"\",\n",
    "            \"crossref_country\": \"\",\n",
    "            \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"error_log\": [\"[WARN] Could not assemble lookup info from any extractor.\"]\n",
    "        }\n",
    "    crossref_results.append(normd)\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"crossref_results_{run_id}.json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(crossref_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[Block 3.5] Wrote CrossRef extraction results to {outfile} for {len(crossref_results)} papers.\")\n",
    "for r in crossref_results[:3]:\n",
    "    print(f\"\\n{r['pdf_filename']}\\nTitle: {r['crossref_title']} | Author(s): {r['crossref_author']} | Year: {r['crossref_year']} | DOI: {r['crossref_doi']}\\nERROR: {r['error_log']}\")\n",
    "print(\"--- End Block 3.5 (CrossRef Metadata Extraction, resilient) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "rvwlb5lWvt2u",
    "outputId": "ad23dea0-94f0-4624-e5f9-348f47209c0c"
   },
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Loaded grobid_results_Nurse-AI_ScR_250519_1642.json, 4 records.\n",
      "[INFO] Loaded llm_results_Nurse-AI_ScR_250519_1642.json, 4 records.\n",
      "[INFO] Loaded filename_results_Nurse-AI_ScR_250519_1642.json, 4 records.\n",
      "[Block 3.5] Wrote CrossRef extraction results to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/crossref_results_Nurse-AI_ScR_250519_1642.json for 4 papers.\n",
      "\n",
      "Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "Title: Machine Learning Model to Extract Malnutrition Data from Nursing Notes | Author(s): Alkhalaf; Mohammad; Yin; Mengyang; Deng; Chao; Chang; Hui-Chen (Rita); Yu; Ping | Year: 2024 | DOI: 10.3233/shti231240\n",
      "ERROR: []\n",
      "\n",
      "Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "Title: Human-AI Teaming in Critical Care: A Comparative Analysis of Data Scientists\u2019 and Clinicians\u2019 Perspectives on AI Augmentation and Automation (Preprint) | Author(s): Bienefeld; Nadine; Keller; Emanuela; Grote; Gudela | Year: 2023 | DOI: 10.2196/preprints.50130\n",
      "ERROR: []\n",
      "\n",
      "Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      "Title: Development of an Artificial Intelligence\u2013Based Tailored Mobile Intervention for Nurse Burnout: Single-Arm Trial (Preprint) | Author(s): Cho; Aram; Cha; Chiyoung; Baek; Gumhee | Year: 2023 | DOI: 10.2196/preprints.54029\n",
      "ERROR: []\n",
      "--- End Block 3.5 (CrossRef Metadata Extraction, resilient) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.6] OpenAlex Metadata Extraction (Bulletproof Author Join, v6.5.2)\n",
    "# Version 6.5.2\n",
    "#\n",
    "# - Handles all OpenAlex \"display_name\" field variants (joins only if actual string).\n",
    "# - Outputs: ai_artifacts/openalex_results_{run_id}.json\n",
    "\n",
    "import json, hashlib, requests\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "def try_load(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"[INFO] Loaded {path.name}, {len(results)} records.\")\n",
    "        return {r[\"pdf_id\"]: r for r in results}\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not load {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "allow_internet = config.get(\"allow_internet\", True)\n",
    "grobid_path = AI_ARTIFACTS_DIR / f\"grobid_results_{run_id}.json\"\n",
    "llm_path = AI_ARTIFACTS_DIR / f\"llm_results_{run_id}.json\"\n",
    "filename_path = AI_ARTIFACTS_DIR / f\"filename_results_{run_id}.json\"\n",
    "\n",
    "grobid_res = try_load(grobid_path)\n",
    "llm_res = try_load(llm_path)\n",
    "fn_res = try_load(filename_path)\n",
    "\n",
    "def extract_openalex_metadata(doi, title):\n",
    "    out, err = {f: \"\" for f in fields}, None\n",
    "    try:\n",
    "        if doi:\n",
    "            url = f\"https://api.openalex.org/works/https://doi.org/{normalize_doi(doi)}\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.status_code != 200:\n",
    "                err = f\"[WARN] OpenAlex DOI {doi} query failed: HTTP {r.status_code}\"\n",
    "                return out, err\n",
    "            dat = r.json()\n",
    "        elif title:\n",
    "            url = f\"https://api.openalex.org/works?title.search={requests.utils.quote(title)}\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.status_code != 200:\n",
    "                err = f\"[WARN] OpenAlex title.search ('{title}') failed: HTTP {r.status_code}\"\n",
    "                return out, err\n",
    "            dat = r.json()[\"results\"][0] if \"results\" in r.json() and r.json()[\"results\"] else {}\n",
    "        else:\n",
    "            err = \"[WARN] No DOI or title, cannot query OpenAlex\"\n",
    "            return out, err\n",
    "        doi_val = dat.get(\"doi\", \"\")\n",
    "        if doi_val and doi_val.startswith(\"https://doi.org/\"):\n",
    "            doi_val = doi_val[len(\"https://doi.org/\") :]\n",
    "        out[\"doi\"] = doi_val\n",
    "        out[\"title\"] = dat.get(\"title\", \"\")\n",
    "        # Robust! Only join proper strings. Skip any non-string, non-present\n",
    "        out[\"author\"] = \"; \".join([\n",
    "            disp for a in dat.get(\"authorships\", [])\n",
    "            for disp in [a.get(\"author\",{}).get(\"display_name\")]\n",
    "            if isinstance(disp, str)\n",
    "        ])\n",
    "        out[\"year\"] = str(dat.get(\"publication_year\", \"\"))\n",
    "        out[\"author_keywords\"] = \"; \".join(dat.get(\"keywords\", [])) if dat.get(\"keywords\") else \"\"\n",
    "        out[\"source_journal\"] = dat.get(\"host_venue\", {}).get(\"display_name\", \"\") if dat.get(\"host_venue\") else \"\"\n",
    "        out[\"study_type\"] = dat.get(\"type\", \"\")\n",
    "        ISO_MAP = {'us': 'United States', 'gb':'United Kingdom', 'uk':'United Kingdom', 'au':'Australia', 'nz':'New Zealand', 'ca':'Canada'}\n",
    "        countries = []\n",
    "        for a in dat.get(\"authorships\", []):\n",
    "            for inst in a.get(\"institutions\", []):\n",
    "                cc = inst.get(\"country_code\")\n",
    "                if cc:\n",
    "                    countries.append(ISO_MAP.get(cc.lower(), cc.upper()))\n",
    "        out[\"country\"] = \"; \".join(sorted(set(countries)))\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] OpenAlex error: {e}\"\n",
    "        print_and_log(err)\n",
    "    return out, err\n",
    "\n",
    "openalex_results = []\n",
    "for pdf in pdfs:\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    errors = []\n",
    "    grob = grobid_res.get(pdf_id, {})\n",
    "    llm = llm_res.get(pdf_id, {})\n",
    "    fn = fn_res.get(pdf_id, {})\n",
    "    candidates = [\n",
    "        dict(doi=grob.get(\"doi\") or \"\", title=grob.get(\"title\") or \"\"),\n",
    "        dict(doi=llm.get(\"doi\") or \"\", title=llm.get(\"title\") or \"\"),\n",
    "        dict(doi=fn.get(\"fn_doi\") or \"\", title=fn.get(\"fn_title\") or \"\")\n",
    "    ]\n",
    "    candidate = next((c for c in candidates if c[\"doi\"] or c[\"title\"]), None)\n",
    "    if not allow_internet:\n",
    "        normd = {\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"pdf_filename\": str(pdf.name),\n",
    "            \"openalex_doi\": \"\",\n",
    "            \"openalex_title\": \"\",\n",
    "            \"openalex_author\": \"\",\n",
    "            \"openalex_year\": \"\",\n",
    "            \"openalex_journal\": \"\",\n",
    "            \"openalex_keywords\": \"\",\n",
    "            \"openalex_type\": \"\",\n",
    "            \"openalex_country\": \"\",\n",
    "            \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"error_log\": [\"[INFO] Skipped: allow_internet is False\"]\n",
    "        }\n",
    "    elif candidate:\n",
    "        cr, err = extract_openalex_metadata(candidate[\"doi\"], candidate[\"title\"])\n",
    "        normd = {\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"pdf_filename\": str(pdf.name),\n",
    "            \"openalex_doi\": normalize_doi(cr[\"doi\"]),\n",
    "            \"openalex_title\": cr[\"title\"],\n",
    "            \"openalex_author\": normalize_author(cr[\"author\"]),\n",
    "            \"openalex_year\": cr[\"year\"],\n",
    "            \"openalex_journal\": cr[\"source_journal\"],\n",
    "            \"openalex_keywords\": normalize_keywords(cr[\"author_keywords\"]),\n",
    "            \"openalex_type\": cr[\"study_type\"],\n",
    "            \"openalex_country\": normalize_country(cr[\"country\"]),\n",
    "            \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"error_log\": [err] if err else []\n",
    "        }\n",
    "    else:\n",
    "        normd = {\n",
    "            \"pdf_id\": pdf_id,\n",
    "            \"pdf_filename\": str(pdf.name),\n",
    "            \"openalex_doi\": \"\",\n",
    "            \"openalex_title\": \"\",\n",
    "            \"openalex_author\": \"\",\n",
    "            \"openalex_year\": \"\",\n",
    "            \"openalex_journal\": \"\",\n",
    "            \"openalex_keywords\": \"\",\n",
    "            \"openalex_type\": \"\",\n",
    "            \"openalex_country\": \"\",\n",
    "            \"extraction_time_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"error_log\": [\"[WARN] Could not assemble lookup info from any extractor.\"]\n",
    "        }\n",
    "    openalex_results.append(normd)\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"openalex_results_{run_id}.json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(openalex_results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"[Block 3.6] Wrote OpenAlex extraction results to {outfile} for {len(openalex_results)} papers.\")\n",
    "for r in openalex_results[:3]:\n",
    "    print(f\"\\n{r['pdf_filename']}\\nTitle: {r['openalex_title']} | Author(s): {r['openalex_author']} | Year: {r['openalex_year']} | DOI: {r['openalex_doi']}\\nERROR: {r['error_log']}\")\n",
    "print(\"--- End Block 3.6 (OpenAlex Metadata Extraction, bulletproof) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "G1Bio14Qvt50",
    "outputId": "e9eb4159-90ef-4466-a58f-903baa5688c2"
   },
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] Loaded grobid_results_Nurse-AI_ScR_250519_1642.json, 4 records.\n",
      "[INFO] Loaded llm_results_Nurse-AI_ScR_250519_1642.json, 4 records.\n",
      "[INFO] Loaded filename_results_Nurse-AI_ScR_250519_1642.json, 4 records.\n",
      "[WARN] OpenAlex error: sequence item 0: expected str instance, dict found\n",
      "[Block 3.6] Wrote OpenAlex extraction results to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/openalex_results_Nurse-AI_ScR_250519_1642.json for 4 papers.\n",
      "\n",
      "Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "Title:  | Author(s):  | Year:  | DOI: \n",
      "ERROR: [\"[WARN] OpenAlex title.search ('Machine Learning Model to Extract Malnutrition Data from Nursing Notes') failed: HTTP 403\"]\n",
      "\n",
      "Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "Title:  | Author(s):  | Year:  | DOI: \n",
      "ERROR: [\"[WARN] OpenAlex title.search ('Human-AI Teaming in the ICU: A Comparative Analysis of Data Scientists' and Clinicians' Assessments on AI Augmentation and Automation at Work') failed: HTTP 403\"]\n",
      "\n",
      "Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf\n",
      "Title:  | Author(s):  | Year:  | DOI: \n",
      "ERROR: [\"[WARN] OpenAlex title.search ('AI-Based Tailored Mobile Intervention for Nurse Burnout: Development Study') failed: HTTP 403\"]\n",
      "--- End Block 3.6 (OpenAlex Metadata Extraction, bulletproof) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title [Block 3.7] Extraction Method Comparison/Diagnostics Table (v6.6.0)\n",
    "# Version 6.6.0\n",
    "#\n",
    "# - Table 1: Null/blank count matrix \u2014 field (row) \u00d7 method (col)\n",
    "# - Table 2+: For each paper: method (row) \u00d7 field (col), shows each returned value.\n",
    "# - Lists per-method error logs grouped for audit.\n",
    "# - Output: ai_artifacts/master_nullcount_{run_id}.csv, ai_artifacts/master_perpaper_{run_id}.json\n",
    "\n",
    "import json, hashlib, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    OP_DIR\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Block 3.0 (common setup) must be run first!\")\n",
    "\n",
    "# --- Method to file mapping\n",
    "method_to_jsonfile = {\n",
    "    \"llm\":         f\"llm_results_{run_id}.json\",\n",
    "    \"grobid\":      f\"grobid_results_{run_id}.json\",\n",
    "    \"filename\":    f\"filename_results_{run_id}.json\",\n",
    "    \"fitz\":        f\"pdfmeta_results_{run_id}.json\",\n",
    "    \"pdfplumber\":  f\"pdfmeta_results_{run_id}.json\",  # both in same file\n",
    "    \"crossref\":    f\"crossref_results_{run_id}.json\",\n",
    "    \"openalex\":    f\"openalex_results_{run_id}.json\"\n",
    "}\n",
    "\n",
    "def load_method_results(method, colmap=None, subkey=None):\n",
    "    path = AI_ARTIFACTS_DIR / method_to_jsonfile[method]\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            dat = json.load(f)\n",
    "        results = {}\n",
    "        for obj in dat:\n",
    "            pdf_id = obj[\"pdf_id\"]\n",
    "            if subkey:  # for fitz/pdfplumber \"pdfmeta_results\"\n",
    "                use = obj.get(f\"{subkey}_{colmap}\", \"\") if colmap else obj.get(f\"{subkey}\", \"\")\n",
    "            else:\n",
    "                use = {field: obj.get(f\"{colmap}_{field}\", obj.get(field, \"\")) for field in fields} if colmap else {field: obj.get(field, \"\") for field in fields}\n",
    "            results[pdf_id] = obj\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not load {method}: {path}! {e}\")\n",
    "        return {}\n",
    "\n",
    "fields = [\n",
    "    \"title\", \"author\", \"year\", \"doi\",\n",
    "    \"author_keywords\", \"country\", \"source_journal\", \"study_type\"\n",
    "]\n",
    "methods = [\"llm\", \"grobid\", \"filename\", \"fitz\", \"pdfplumber\", \"crossref\", \"openalex\"]\n",
    "\n",
    "# --- Load data\n",
    "method_results = {}\n",
    "for method in methods:\n",
    "    if method in [\"fitz\", \"pdfplumber\"]:\n",
    "        colmap = \"title\"  # dummy; all handled in next block\n",
    "        subkey = method\n",
    "    else:\n",
    "        colmap, subkey = None, None\n",
    "    method_results[method] = load_method_results(method, colmap=subkey, subkey=subkey)\n",
    "\n",
    "# --- Table 1: Null/Blank Count Table (field x method) ---\n",
    "nullcount = pd.DataFrame(0, index=fields, columns=methods)\n",
    "for pdf in pdfs:\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    for m in methods:\n",
    "        res = method_results[m].get(pdf_id, {})\n",
    "        # For fitz/pdfplumber, handle alternate field layout\n",
    "        for f in fields:\n",
    "            if m == \"fitz\":\n",
    "                val = res.get(\"fitz_\" + f, \"\")\n",
    "            elif m == \"pdfplumber\":\n",
    "                val = res.get(\"pdfplumber_\" + f, \"\")\n",
    "            else:\n",
    "                val = res.get(f, \"\")\n",
    "            if not val or str(val).strip() == \"\" or str(val).strip().lower() == \"none\":\n",
    "                nullcount.loc[f, m] += 1\n",
    "\n",
    "outfile = AI_ARTIFACTS_DIR / f\"master_nullcount_{run_id}.csv\"\n",
    "nullcount.to_csv(outfile)\n",
    "print(\"\\n=== Table 1: Null/Blank Count Table (field x method) ===\")\n",
    "print(nullcount)\n",
    "print(f\"Table written to {outfile}\")\n",
    "\n",
    "# --- Table 2+: Per-paper method \u00d7 field value map ---\n",
    "perpaper_output = {}\n",
    "for pdf in pdfs:\n",
    "    pdf_id = f\"paper_ID_{hashlib.sha1(str(pdf).encode('utf-8')).hexdigest()[:8]}\"\n",
    "    paper_tab = pd.DataFrame(\n",
    "        columns=fields, index=methods\n",
    "    )\n",
    "    for m in methods:\n",
    "        res = method_results[m].get(pdf_id, {})\n",
    "        for f in fields:\n",
    "            if m == \"fitz\":\n",
    "                val = res.get(\"fitz_\" + f, \"\")\n",
    "            elif m == \"pdfplumber\":\n",
    "                val = res.get(\"pdfplumber_\" + f, \"\")\n",
    "            elif m == \"filename\":\n",
    "                # For filename, map to fn_title/fn_author/fn_year etc.\n",
    "                mapfield = {\"title\": \"fn_title\", \"author\": \"fn_author\", \"year\": \"fn_year\", \"doi\": \"fn_doi\"}\n",
    "                val = res.get(mapfield.get(f, \"fn_\" + f), \"\")\n",
    "            elif m == \"crossref\":\n",
    "                val = res.get(\"crossref_\" + f, \"\")\n",
    "            elif m == \"openalex\":\n",
    "                val = res.get(\"openalex_\" + f, \"\")\n",
    "            else:\n",
    "                val = res.get(f, \"\")\n",
    "            paper_tab.loc[m, f] = val\n",
    "    perpaper_output[pdf_id] = {\n",
    "        \"pdf_filename\": pdf.name,\n",
    "        \"comparison_table\": paper_tab.to_dict(),\n",
    "    }\n",
    "\n",
    "outfile_json = AI_ARTIFACTS_DIR / f\"master_perpaper_{run_id}.json\"\n",
    "with open(outfile_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(perpaper_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n=== Table 2+: Per-paper Extraction Comparison ===\")\n",
    "for k, v in list(perpaper_output.items())[:2]:\n",
    "    print(f\"\\nPaper: {v['pdf_filename']}\")\n",
    "    print(pd.DataFrame(v['comparison_table']))\n",
    "\n",
    "# --- Per-method error logs grouped ---\n",
    "print(\"\\n=== Per-method Error Logs Grouped ===\")\n",
    "for m in methods:\n",
    "    errlogs = []\n",
    "    objs = method_results[m].values()\n",
    "    for o in objs:\n",
    "        errs = o.get(\"error_log\", [])\n",
    "        if errs:\n",
    "            errlogs.extend(errs if isinstance(errs, list) else [errs])\n",
    "    print(f\"\\nMethod: {m}\")\n",
    "    if errlogs:\n",
    "        for e in errlogs:\n",
    "            print(\"  \", e)\n",
    "    else:\n",
    "        print(\"  [no errors logged]\")\n",
    "\n",
    "print(\"--- End Block 3.7 (Method Comparison + Per-paper Review Tables) ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sZSulcRHvt8a",
    "outputId": "b181d0d8-32da-497c-b64c-7c478ad19e17"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Table 1: Null/Blank Count Table (field x method) ===\n",
      "                 llm  grobid  filename  fitz  pdfplumber  crossref  openalex\n",
      "title              0       0         4     1           1         4         4\n",
      "author             0       0         4     3           3         4         4\n",
      "year               0       2         4     0           0         4         4\n",
      "doi                3       3         4     4           4         4         4\n",
      "author_keywords    2       3         4     4           4         4         4\n",
      "country            2       2         4     4           4         4         4\n",
      "source_journal     0       4         4     4           4         4         4\n",
      "study_type         1       4         4     4           4         4         4\n",
      "Table written to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/master_nullcount_Nurse-AI_ScR_250519_1642.csv\n",
      "\n",
      "=== Table 2+: Per-paper Extraction Comparison ===\n",
      "\n",
      "Paper: Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf\n",
      "                                                        title  \\\n",
      "llm         Machine Learning Model to Extract Malnutrition...   \n",
      "grobid      Machine Learning Model to Extract Malnutrition...   \n",
      "filename    Machine Learning Model to Extract Malnutrition...   \n",
      "fitz                                                            \n",
      "pdfplumber                                                      \n",
      "crossref    Machine Learning Model to Extract Malnutrition...   \n",
      "openalex                                                        \n",
      "\n",
      "                                                       author  year  \\\n",
      "llm         Mohammad ALKHALAF; Mengyang YIN; Chao DENG; Hu...  2024   \n",
      "grobid      Mohammad Alkhalaf; Mengyang Yin; Chao Deng; Ri...         \n",
      "filename                                       Alkhalaf et al  2024   \n",
      "fitz                                                           2024   \n",
      "pdfplumber                                                     2024   \n",
      "crossref    Alkhalaf; Mohammad; Yin; Mengyang; Deng; Chao;...  2024   \n",
      "openalex                                                              \n",
      "\n",
      "                           doi  \\\n",
      "llm         10.3233/shti231240   \n",
      "grobid                           \n",
      "filename                         \n",
      "fitz                             \n",
      "pdfplumber                       \n",
      "crossref    10.3233/shti231240   \n",
      "openalex                         \n",
      "\n",
      "                                              author_keywords  \\\n",
      "llm         malnutrition; Natural language processing; nur...   \n",
      "grobid      malnutrition; Natural language processing; nur...   \n",
      "filename                                                        \n",
      "fitz                                                            \n",
      "pdfplumber                                                      \n",
      "crossref                                                        \n",
      "openalex                                                        \n",
      "\n",
      "                            country                           source_journal  \\\n",
      "llm                       Australia  MEDINFO 2023 \u2014 The Future Is Accessible   \n",
      "grobid      Saudi Arabia; Australia                                            \n",
      "filename                                                                       \n",
      "fitz                                                                           \n",
      "pdfplumber                                                                     \n",
      "crossref                                                                       \n",
      "openalex                                                                       \n",
      "\n",
      "           study_type  \n",
      "llm              None  \n",
      "grobid                 \n",
      "filename               \n",
      "fitz                   \n",
      "pdfplumber             \n",
      "crossref               \n",
      "openalex               \n",
      "\n",
      "Paper: Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf\n",
      "                                                        title  \\\n",
      "llm         Human-AI Teaming in the ICU: A Comparative Ana...   \n",
      "grobid      Human-AI Teaming in the ICU: A Comparative Ana...   \n",
      "filename    Human-AI Teaming in Critical Care A Comparative A   \n",
      "fitz                                 Preprint - 10.2196/50130   \n",
      "pdfplumber                           Preprint - 10.2196/50130   \n",
      "crossref    Human-AI Teaming in Critical Care: A Comparati...   \n",
      "openalex                                                        \n",
      "\n",
      "                                                       author  year  \\\n",
      "llm           Nadine Bienefeld; Emanuela Keller; Gudela Grote  2023   \n",
      "grobid        Nadine Bienefeld; Emanuela Keller; Gudela Grote  2023   \n",
      "filename                                      Bienefeld et al  2024   \n",
      "fitz                                                           2024   \n",
      "pdfplumber                                                     2024   \n",
      "crossref    Bienefeld; Nadine; Keller; Emanuela; Grote; Gu...  2023   \n",
      "openalex                                                              \n",
      "\n",
      "                                doi author_keywords country  \\\n",
      "llm                            none            None    None   \n",
      "grobid                                                        \n",
      "filename                                                      \n",
      "fitz                                                          \n",
      "pdfplumber                                                    \n",
      "crossref    10.2196/preprints.50130                           \n",
      "openalex                                                      \n",
      "\n",
      "                                  source_journal            study_type  \n",
      "llm         Journal of Medical Internet Research  Comparative Analysis  \n",
      "grobid                                                                  \n",
      "filename                                                                \n",
      "fitz                                                                    \n",
      "pdfplumber                                                              \n",
      "crossref                                                                \n",
      "openalex                                                                \n",
      "\n",
      "=== Per-method Error Logs Grouped ===\n",
      "\n",
      "Method: llm\n",
      "  [no errors logged]\n",
      "\n",
      "Method: grobid\n",
      "   [INFO] TEI saved in /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/grobid_tei_debug/paper_ID_34213f62_tei.xml\n",
      "   [INFO] TEI saved in /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/grobid_tei_debug/paper_ID_c9685dcc_tei.xml\n",
      "   [INFO] TEI saved in /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1642/ai_artifacts/grobid_tei_debug/paper_ID_accd7a48_tei.xml\n",
      "\n",
      "Method: filename\n",
      "  [no errors logged]\n",
      "\n",
      "Method: fitz\n",
      "  [no errors logged]\n",
      "\n",
      "Method: pdfplumber\n",
      "  [no errors logged]\n",
      "\n",
      "Method: crossref\n",
      "  [no errors logged]\n",
      "\n",
      "Method: openalex\n",
      "   [WARN] OpenAlex title.search ('Machine Learning Model to Extract Malnutrition Data from Nursing Notes') failed: HTTP 403\n",
      "   [WARN] OpenAlex title.search ('Human-AI Teaming in the ICU: A Comparative Analysis of Data Scientists' and Clinicians' Assessments on AI Augmentation and Automation at Work') failed: HTTP 403\n",
      "   [WARN] OpenAlex title.search ('AI-Based Tailored Mobile Intervention for Nurse Burnout: Development Study') failed: HTTP 403\n",
      "   [WARN] OpenAlex error: sequence item 0: expected str instance, dict found\n",
      "--- End Block 3.7 (Method Comparison + Per-paper Review Tables) ---\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gCGVQ7VCvt-e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YL4MEkwVz7rO",
    "outputId": "36b09d63-0dfa-4bfe-f14f-6622716a4040"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[DEBUG] LLM RAW RESPONSE:\n",
      "{\n",
      "  \"title\": \"Machine Learning Model to Extract Malnutrition Data from Nursing Notes\",\n",
      "  \"author\": [\n",
      "    \"Mohammad ALKHALAF\",\n",
      "    \"Mengyang YIN\",\n",
      "    \"Chao DENG\",\n",
      "    \"Hui-Chen (Rita) CHANG\",\n",
      "    \"Ping YU\"\n",
      "  ],\n",
      "  \"year\": 2024,\n",
      "  \"doi\": \"10.3233/SHTI231240\",\n",
      "  \"author_keywords\": [\n",
      "    \"Natural language processing\",\n",
      "    \"malnutrition\",\n",
      "    \"nursing progress notes\"\n",
      "  ],\n",
      "  \"country\": \"Australia\",\n",
      "  \"source_journal\": \"MEDINFO 2023 \u2014 The Future Is Accessible\",\n",
      "  \"study_type\": null\n",
      "}\n",
      "---\n",
      "[ERROR] LLM extraction failed: Extra data: line 26 column 1 (char 695)\n",
      "\n",
      "--- Null/Blank Value Table (methods \u00d7 fields, post-normalization) ---\n",
      "            title  author  year  doi  author_keywords  country  \\\n",
      "llm             1       1     1    1                1        1   \n",
      "grobid          4       4     4    4                4        4   \n",
      "fitz            1       3     0    4                4        4   \n",
      "pdfplumber      1       3     0    4                4        4   \n",
      "filename        0       0     0    2                4        4   \n",
      "crossref        4       4     4    4                4        4   \n",
      "openalex        4       4     4    4                4        4   \n",
      "\n",
      "            source_journal  study_type  \n",
      "llm                      1           1  \n",
      "grobid                   4           4  \n",
      "fitz                     4           4  \n",
      "pdfplumber               4           4  \n",
      "filename                 4           4  \n",
      "crossref                 4           4  \n",
      "openalex                 4           4  \n",
      "\n",
      "\n",
      "======== PAPER: paper_ID_34213f62 / /content/drive/My Drive/Pilot/PDFs/Copy of Copy of Alkhalaf et al. - 2024 - Machine Learning Model to Extract Malnutrition Dat.pdf ========\n",
      "                                                                             title                                                                      author  year                 doi                                                    author_keywords    country                           source_journal study_type\n",
      "llm         Machine Learning Model to Extract Malnutrition Data from Nursing Notes  Mohammad ALKHALAF; Mengyang YIN; Chao DENG; Hui-Chen (Rita) CHANG; Ping YU  2024  10.3233/shti231240  malnutrition; Natural language processing; nursing progress notes  Australia  MEDINFO 2023 \u2014 The Future Is Accessible       None\n",
      "grobid                                                                                                                                                                                                                                                                                                                    \n",
      "fitz                                                                                                                                                            2024                                                                                                                                                      \n",
      "pdfplumber                                                                                                                                                      2024                                                                                                                                                      \n",
      "filename                        Machine Learning Model to Extract Malnutrition Dat                                             Copy of Copy of Alkhalaf et al.  2024  10.3233/shti231240                                                                                                                                  \n",
      "crossref                                                                                                                                                                                                                                                                                                                  \n",
      "openalex                                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "======== PAPER: paper_ID_c9685dcc / /content/drive/My Drive/Pilot/PDFs/Copy of Copy of Bienefeld et al. - 2024 - Human-AI Teaming in Critical Care A Comparative A.pdf ========\n",
      "                                                                                                                                                    title                                           author  year   doi author_keywords country                        source_journal            study_type\n",
      "llm         Human-AI Teaming in the ICU: A Comparative Analysis of Data Scientists' and Clinicians\u2019 Assessments on AI Augmentation and Automation at Work  Nadine Bienefeld; Emanuela Keller; Gudela Grote  2023  none            None    None  Journal of Medical Internet Research  Comparative Analysis\n",
      "grobid                                                                                                                                                                                                                                                                                                    \n",
      "fitz                                                                                                                             Preprint - 10.2196/50130                                                   2024                                                                                          \n",
      "pdfplumber                                                                                                                       Preprint - 10.2196/50130                                                   2024                                                                                          \n",
      "filename                                                                                                Human-AI Teaming in Critical Care A Comparative A                 Copy of Copy of Bienefeld et al.  2024                                                                                          \n",
      "crossref                                                                                                                                                                                                                                                                                                  \n",
      "openalex                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "======== PAPER: paper_ID_accd7a48 / /content/drive/My Drive/Pilot/PDFs/Copy of Copy of Cho et al. - 2024 - Development of an Artificial Intelligence-Based Ta.pdf ========\n",
      "                                                                                 title                               author  year   doi author_keywords country                        source_journal         study_type\n",
      "llm         AI-Based Tailored Mobile Intervention for Nurse Burnout: Development Study  Chiyoung Cha; Aram Cho; Gumhee Baek  2023  none            None    None  Journal of Medical Internet Research  Development Study\n",
      "grobid                                                                                                                                                                                                                  \n",
      "fitz                                                          Preprint - 10.2196/54029                                       2024                                                                                       \n",
      "pdfplumber                                                    Preprint - 10.2196/54029                                       2024                                                                                       \n",
      "filename                            Development of an Artificial Intelligence-Based Ta           Copy of Copy of Cho et al.  2024                                                                                       \n",
      "crossref                                                                                                                                                                                                                \n",
      "openalex                                                                                                                                                                                                                \n",
      "\n",
      "\n",
      "======== PAPER: paper_ID_884fd959 / /content/drive/My Drive/Pilot/PDFs/Copy of Copy of Dubin et al. - 2024 - Appropriateness of Frequently Asked Patient Questi.pdf ========\n",
      "                                                                                                                                                  title                        author  year                          doi author_keywords country source_journal study_type\n",
      "llm                                                                                                                                                                                                                                                                       \n",
      "grobid                                                                                                                                                                                                                                                                    \n",
      "fitz        Appropriateness of Frequently Asked Patient Questions Following Total Hip Arthroplasty From ChatGPT Compared to Arthroplasty-Trained Nurses            Jeremy A. Dubin BA  2024                                                                               \n",
      "pdfplumber  Appropriateness of Frequently Asked Patient Questions Following Total Hip Arthroplasty From ChatGPT Compared to Arthroplasty-Trained Nurses            Jeremy A. Dubin BA  2024                                                                               \n",
      "filename                                                                                             Appropriateness of Frequently Asked Patient Questi  Copy of Copy of Dubin et al.  2024  10.1016/j.arth.2024.04.020.                                                  \n",
      "crossref                                                                                                                                                                                                                                                                  \n",
      "openalex                                                                                                                                                                                                                                                                  \n",
      "\n",
      "[INFO] Raw method+error audit JSON written for each PDF in /content/drive/My Drive/Pilot/Nurse-AI_ScR_250519_1422/operational/audit/raw_meta\n",
      "--- End Block 3 v6.4.1 ---\n"
     ]
    }
   ],
   "source": [
    "# @title [Block 3] Forensic Metadata Extraction \u2013 Normalized, Forensic Audit, Enhanced DOI (v6.4.0)\n",
    "# Current Version 6.4.0\n",
    "#\n",
    "# Version Control Summaries\n",
    "# v6.0: Multi-source extraction and manifest voting (fitz, pdfplumber, GROBID, CrossRef, OpenAlex, filename, LLM).\n",
    "# v6.1: LLM-based metadata, review-needed flag, fuzzy matching for missing/ambiguous fields.\n",
    "# v6.2: Added consensus logic, CrossRef/OpenAlex normalization, audit trail, reviewer trace.\n",
    "# v6.2.4: Extended fields, audit-ready outputs.\n",
    "# v6.3.0: Refactor - forensic extraction, no voting.\n",
    "# v6.3.1: Consistent paper IDs, match analytics, outputs audit/stable.\n",
    "# v6.3.2: Pure ID-table output, no filenames, exact/approx tables.\n",
    "# v6.3.3: More robust error tracing, LLM key via colab.userdata, type diagnostics.\n",
    "# v6.3.5: Full XML parsing for Grobid, safe LLM flattening, NZ English, method error log.\n",
    "# v6.4.0: [SPRINT 1] - Normalization applied to all method outputs (DOI, authors, country, keywords).\n",
    "#         - DOI regex scan on first 2 and last page (fallback).\n",
    "#         - CrossRef bibliographic search if no DOI and internet enabled.\n",
    "#         - All method errors captured per PDF. Raw audit JSON (all methods+errors) written per-paper.\n",
    "#         - API calls conditional on allow_internet.\n",
    "#         - Null summary, consensus logic preps for normalized fields. Review/PI diagnostics improved.\n",
    "#\n",
    "# Block Summary\n",
    "# - Extracts metadata via: LLM, Grobid (XML), fitz, pdfplumber, filename, CrossRef, OpenAlex\n",
    "# - Runs all field values through normalization: DOI, author, country, keywords\n",
    "# - Fallback: DOI regex search (first 2 + last page); CrossRef 'bibliographic' if still missing\n",
    "# - All method outputs, errors, and debug logs written to per-paper raw_meta JSON in audit/raw_meta/.\n",
    "# - API/net calls POLICED BY config['allow_internet'], future-proof for caching\n",
    "# - Prints method \u00d7 field null-matrix, diagnostic per-paper tables; reviewer/PI clarity prioritized\n",
    "# @title [Block 3] Forensic Metadata Extraction \u2013 Normalized, Path Handling Bugfix (v6.4.1)\n",
    "# Current Version 6.4.1\n",
    "#\n",
    "# Version Control Summaries\n",
    "# v6.0\u2013v6.4.0: See previous comments.\n",
    "# v6.4.1: [BUGFIX] Ensures pdfs is always a list of Path/str objects.\n",
    "#         - Loop never mutates pdf variable.\n",
    "#         - All extractors are called ONLY with Path/str (never dict).\n",
    "#         - Assert added at start of each loop for extra safety.\n",
    "#\n",
    "# Block Summary\n",
    "# - Restores robust method application to each PDF: no silent, systematic extraction failures.\n",
    "# - All audit, normalization, error logging, API/network logic from v6.4.0 retained.\n",
    "\n",
    "import os, re, fitz, pdfplumber, requests, json, yaml, hashlib, xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ... [Normalization helpers and previous definitions\u2014unchanged, see v6.4.0] ...\n",
    "\n",
    "# --- Normalization Helpers ---\n",
    "def normalize_doi(x):\n",
    "    if not x or not isinstance(x, str): return \"\"\n",
    "    x = x.strip().lower().replace(' ', '')\n",
    "    x = re.sub(r\"^(https?://(dx\\.)?doi\\.org/)\", \"\", x)\n",
    "    x = re.sub(r\"\\s\", \"\", x)\n",
    "    return x\n",
    "\n",
    "def normalize_author(raw):\n",
    "    if not raw: return \"\"\n",
    "    if isinstance(raw, list): vals = raw\n",
    "    else: vals = [raw]\n",
    "    def norm_piece(x):\n",
    "        if not x: return \"\"\n",
    "        x = str(x)\n",
    "        x = x.replace(\",\", \"\")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "        return x\n",
    "    flat = []\n",
    "    for v in vals:\n",
    "        if isinstance(v, str):\n",
    "            flat += [norm_piece(w) for w in re.split(r\";|,|&| and \", v) if w.strip()]\n",
    "        else:\n",
    "            flat.append(norm_piece(v))\n",
    "    seen, out = set(), []\n",
    "    for a in flat:\n",
    "        a_lc = a.lower()\n",
    "        if a_lc and a_lc not in seen:\n",
    "            out.append(a)\n",
    "            seen.add(a_lc)\n",
    "    return \"; \".join(out)\n",
    "\n",
    "def normalize_country(raw):\n",
    "    ISO_MAP = {'us': 'United States', 'gb':'United Kingdom', 'uk':'United Kingdom', 'au':'Australia', 'nz':'New Zealand', 'ca':'Canada'}\n",
    "    if not raw: return \"\"\n",
    "    vals = [w.strip() for w in str(raw).split(';') if w.strip()]\n",
    "    names = []\n",
    "    for v in vals:\n",
    "        v_lc = v.lower()\n",
    "        n = ISO_MAP.get(v_lc)\n",
    "        if n: names.append(n)\n",
    "        else:\n",
    "            c = re.sub(r'[^a-zA-Z ]+', '', v).strip()\n",
    "            if c and c.lower() not in [n.lower() for n in names]:\n",
    "                names.append(c)\n",
    "    return \"; \".join(dict.fromkeys(names))\n",
    "\n",
    "def normalize_keywords(raw):\n",
    "    if not raw: return \"\"\n",
    "    if isinstance(raw, list): vals = raw\n",
    "    else: vals = [raw]\n",
    "    flat = []\n",
    "    for v in vals:\n",
    "        if isinstance(v, str):\n",
    "            flat += [k.strip() for k in re.split(r\";|,|/|\\|\", v) if k.strip()]\n",
    "        else:\n",
    "            flat.append(str(v).strip())\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for k in flat:\n",
    "        kl = k.lower()\n",
    "        if kl and kl not in seen:\n",
    "            uniq.append(k)\n",
    "            seen.add(kl)\n",
    "    return \"; \".join(sorted(uniq, key=str.lower))\n",
    "\n",
    "with open(\"pipeline_env.json\", \"r\") as f: env = json.load(f)\n",
    "OP_DIR = Path(env[\"OPERATIONAL_DIR\"])\n",
    "CONFIG_PATH = OP_DIR / \"config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f: config = yaml.safe_load(f)\n",
    "PDF_DIR = Path(config[\"pdf_dir\"])\n",
    "pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))[:20]  # Always a list of Path objects!\n",
    "grobid_url = config.get(\"grobid_url\", \"http://localhost:8070/api/processHeaderDocument\")\n",
    "allow_internet = config.get(\"allow_internet\", True)\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get(config.get(\"openai_key_envvar\", \"OPENAI_API_KEY\"))\n",
    "except Exception:\n",
    "    OPENAI_API_KEY = os.environ.get(config.get(\"openai_key_envvar\", \"OPENAI_API_KEY\"), \"\")\n",
    "\n",
    "llm_model = config.get(\"llm_model\")\n",
    "\n",
    "fields = [\n",
    "    \"title\", \"author\", \"year\", \"doi\",\n",
    "    \"author_keywords\", \"country\", \"source_journal\", \"study_type\"\n",
    "]\n",
    "method_names = [\"llm\", \"grobid\", \"fitz\", \"pdfplumber\", \"filename\", \"crossref\", \"openalex\"]\n",
    "\n",
    "def pdf_hash_id(pdf_path): return f\"paper_ID_{hashlib.sha1(str(pdf_path).encode('utf-8')).hexdigest()[:8]}\"\n",
    "\n",
    "def search_doi_in_text(pdf_file):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file)\n",
    "        texts = []\n",
    "        for i in [0,1,-1]:\n",
    "            if 0 <= i < len(doc) or (i < 0 and len(doc) > abs(i)):\n",
    "                t = doc[i].get_text()\n",
    "                texts.append(t)\n",
    "        s = \"\\n\".join(texts)\n",
    "        m = re.search(r'(10\\.\\d{4,9}/[\\w\\.\\-;/\\(\\):]+)', s, flags=re.I)\n",
    "        if m: return normalize_doi(m.group(1))\n",
    "    except Exception: pass\n",
    "    return \"\"\n",
    "\n",
    "def flatten(val): return \"; \".join(str(x) for x in val) if isinstance(val, list) else str(val)\n",
    "\n",
    "def print_and_log(msg):\n",
    "    print(msg)\n",
    "    with open(OP_DIR / \"block3_debug_log.txt\", \"a\") as f: f.write(str(msg) + \"\\n\")\n",
    "\n",
    "def extract_first_page_text(pdf_file, debug=False):\n",
    "    text, err = \"\", None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file)\n",
    "        text = doc[0].get_text()[:3500]\n",
    "        if not text.strip() and len(doc)>1:\n",
    "            text2 = doc[1].get_text()[:2000]\n",
    "            text += \"\\n\" + text2\n",
    "    except Exception as e:\n",
    "        err = f\"[ERROR] extract_first_page_text: {e}\"\n",
    "        print_and_log(err)\n",
    "    return text, err\n",
    "\n",
    "def extract_fitz_metadata(pdf_file):\n",
    "    meta, err = {f:\"\" for f in fields}, None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_file)\n",
    "        m = doc.metadata\n",
    "        meta[\"title\"] = m.get(\"title\", \"\") or m.get(\"Title\", \"\")\n",
    "        meta[\"author\"] = m.get(\"author\", \"\") or m.get(\"Author\", \"\")\n",
    "        meta[\"year\"] = str(m.get(\"modDate\", \"\")[2:6]) if m.get(\"modDate\", \"\") else \"\"\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] fitz metadata: {e}\"\n",
    "        print_and_log(err)\n",
    "    return meta, err\n",
    "\n",
    "def extract_pdfplumber_metadata(pdf_file):\n",
    "    meta, err = {f:\"\" for f in fields}, None\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_file) as p:\n",
    "            info = p.metadata or {}\n",
    "            meta[\"title\"] = info.get(\"Title\", \"\")\n",
    "            meta[\"author\"] = info.get(\"Author\", \"\")\n",
    "            meta[\"year\"] = info.get(\"ModDate\", \"\")[2:6] if info.get(\"ModDate\",\"\") else \"\"\n",
    "            meta[\"author_keywords\"] = info.get(\"Keywords\", \"\")\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] pdfplumber metadata: {e}\"\n",
    "        print_and_log(err)\n",
    "    return meta, err\n",
    "\n",
    "def extract_from_filename_meta(pdf_file):\n",
    "    meta = {f:\"\" for f in fields}\n",
    "    base = Path(pdf_file).stem\n",
    "    m = re.match(r\"(.+?)\\s*-\\s*(\\d{4})\\s*-\\s*(.+)\", base)\n",
    "    if m:\n",
    "        meta[\"author\"], meta[\"year\"], meta[\"title\"] = m.groups()\n",
    "    meta[\"doi\"] = \"\"\n",
    "    m2 = re.search(r\"(10\\.\\d{4,9}/[\\w\\.\\-\\/]+)\", base)\n",
    "    if m2:\n",
    "        meta[\"doi\"] = m2.group(1)\n",
    "    return meta\n",
    "\n",
    "def extract_grobid_full(pdf_file):\n",
    "    meta, err = {f:\"\" for f in fields}, None\n",
    "    try:\n",
    "        with open(pdf_file, \"rb\") as f:\n",
    "            resp = requests.post(grobid_url, files={'input': f}, timeout=60)\n",
    "        tei = resp.text\n",
    "        if tei and '<TEI' in tei:\n",
    "            tree = ET.fromstring(tei)\n",
    "            title_el = tree.find('.//titleStmt/title')\n",
    "            if title_el is not None and title_el.text:\n",
    "                meta[\"title\"] = title_el.text.strip()\n",
    "            authors = []\n",
    "            for pers in tree.findall('.//author/persName'):\n",
    "                surname = pers.find('surname')\n",
    "                forename = pers.find('forename')\n",
    "                a = (forename.text if forename is not None else \"\")\n",
    "                b = (surname.text if surname is not None else \"\")\n",
    "                val = (a + \" \" + b).strip()\n",
    "                if val: authors.append(val)\n",
    "            meta[\"author\"] = \"; \".join(authors)\n",
    "            doi_el = tree.find('.//idno[@type=\"DOI\"]')\n",
    "            meta[\"doi\"] = doi_el.text.strip() if doi_el is not None and doi_el.text else \"\"\n",
    "            date_el = tree.find('.//publicationStmt/date')\n",
    "            meta[\"year\"] = date_el.attrib['when'][:4] if date_el is not None and 'when' in date_el.attrib else \"\"\n",
    "            j = tree.find('.//monogr/title')\n",
    "            meta[\"source_journal\"] = j.text.strip() if j is not None and j.text else \"\"\n",
    "            kws = [k.text.strip() for k in tree.findall('.//keywords/term') if k.text]\n",
    "            meta[\"author_keywords\"] = \"; \".join(kws)\n",
    "            ptype = tree.find('.//note[@type=\"studyType\"]')\n",
    "            meta[\"study_type\"] = ptype.text.strip() if ptype is not None and ptype.text else \"\"\n",
    "            countries = []\n",
    "            for aff in tree.findall('.//affiliation'):\n",
    "                country_el = aff.find('.//country')\n",
    "                if country_el is not None and country_el.text:\n",
    "                    countries.append(country_el.text.strip())\n",
    "            meta[\"country\"] = \"; \".join(set(countries))\n",
    "        else:\n",
    "            err = \"[WARN] Malformed or empty TEI from Grobid\"\n",
    "            print_and_log(err)\n",
    "    except Exception as e:\n",
    "        err = f\"[WARN] Grobid XML: {e}\"\n",
    "        print_and_log(err)\n",
    "    return meta, err\n",
    "\n",
    "def extract_ai_llm_full(first_page, api_key=None, model=None, debug=False):\n",
    "    api_key = api_key or OPENAI_API_KEY\n",
    "    error = None\n",
    "    if not api_key or not api_key.strip():\n",
    "        error = \"[CRIT] OpenAI API key missing/blank for LLM calls!\"\n",
    "        print_and_log(error)\n",
    "        return {k:\"\" for k in fields}, error\n",
    "    try:\n",
    "        import openai\n",
    "        openai.api_key = api_key\n",
    "        prompt = (\n",
    "            \"Extract the following metadata as a JSON object from the text provided: \"\n",
    "            \"title, author, year, doi, author_keywords, country, source_journal, study_type. \"\n",
    "            \"If a field is missing, leave blank or use null. Text follows:\\n\" + first_page\n",
    "        )\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=model or llm_model,\n",
    "            messages=[{\"role\":\"user\", \"content\": prompt}],\n",
    "            temperature=0, max_tokens=384\n",
    "        )\n",
    "        txt = resp.choices[0].message.content.strip()\n",
    "        if debug:\n",
    "            print_and_log(f\"[DEBUG] LLM RAW RESPONSE:\\n{txt}\\n---\")\n",
    "        if txt.startswith(\"```\"):\n",
    "            txt = txt.strip(\"` \\n\")\n",
    "            txt = txt[4:].strip() if txt.lower().startswith(\"json\") else txt\n",
    "        result = json.loads(txt)\n",
    "        assert isinstance(result, dict)\n",
    "        for k in fields:\n",
    "            result[k] = flatten(result.get(k, \"\"))\n",
    "        return {k: result.get(k, \"\") for k in fields}, None\n",
    "    except Exception as e:\n",
    "        error = f\"[ERROR] LLM extraction failed: {e}\"\n",
    "        print_and_log(error)\n",
    "        return {k: \"\" for k in fields}, error\n",
    "\n",
    "def extract_crossref_full(doi, title=None, author=None, year=None):\n",
    "    meta, error = {f: \"\" for f in fields}, None\n",
    "    if not allow_internet: return meta, \"[INFO] Skipped CrossRef (no internet allowed)\"\n",
    "    try:\n",
    "        r = None\n",
    "        if doi:\n",
    "            url = f\"https://api.crossref.org/works/{normalize_doi(doi)}\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            dat = r.json()[\"message\"]\n",
    "        elif title and (author or year):\n",
    "            qstr = f\"{title} {author or ''} {year or ''}\".strip()\n",
    "            url = f\"https://api.crossref.org/works?query.bibliographic={requests.utils.quote(qstr)}&rows=1\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            items = r.json()[\"message\"].get(\"items\", [])\n",
    "            dat = items[0] if items else {}\n",
    "        elif title:\n",
    "            url = f\"https://api.crossref.org/works?query.title={requests.utils.quote(title)}&rows=1\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            items = r.json()[\"message\"].get(\"items\", [])\n",
    "            dat = items[0] if items else {}\n",
    "        else:\n",
    "            dat = {}\n",
    "        meta[\"doi\"] = dat.get(\"DOI\", \"\")\n",
    "        meta[\"title\"] = dat.get(\"title\", [\"\"])[0] if dat.get(\"title\") else \"\"\n",
    "        meta[\"author\"] = \"; \".join(\n",
    "            \"{}, {}\".format(a.get(\"family\", \"\").strip(), a.get(\"given\", \"\").strip())\n",
    "            for a in dat.get(\"author\", [])\n",
    "        ) if dat.get(\"author\") else \"\"\n",
    "        meta[\"year\"] = str(dat.get(\"issued\", {}).get(\"date-parts\", [[None]])[0][0]) if dat.get(\"issued\") else \"\"\n",
    "        meta[\"author_keywords\"] = \"; \".join(dat.get(\"subject\", [])) if dat.get(\"subject\") else \"\"\n",
    "        meta[\"source_journal\"] = dat.get(\"container-title\", [\"\"])[0] if dat.get(\"container-title\") else \"\"\n",
    "        meta[\"study_type\"] = dat.get(\"type\", \"\")\n",
    "        meta[\"country\"] = \"\"\n",
    "    except Exception as e:\n",
    "        error = f\"[WARN] CrossRef: {e}\"\n",
    "        print_and_log(error)\n",
    "    return meta, error\n",
    "\n",
    "def extract_openalex_full(doi, title=None):\n",
    "    meta, error = {f: \"\" for f in fields}, None\n",
    "    if not allow_internet: return meta, \"[INFO] Skipped OpenAlex (no internet allowed)\"\n",
    "    try:\n",
    "        r = None\n",
    "        if doi:\n",
    "            url = f\"https://api.openalex.org/works/https://doi.org/{normalize_doi(doi)}\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            dat = r.json()\n",
    "        elif title:\n",
    "            url = f\"https://api.openalex.org/works?title.search={requests.utils.quote(title)}\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            dat = r.json().get(\"results\", [{}])[0] if \"results\" in r.json() else r.json()\n",
    "        else:\n",
    "            dat = {}\n",
    "        doi_val = dat.get(\"doi\", \"\")\n",
    "        if doi_val and doi_val.startswith(\"https://doi.org/\"):\n",
    "            doi_val = doi_val[len(\"https://doi.org/\") :]\n",
    "        meta[\"doi\"]   = doi_val\n",
    "        meta[\"title\"] = dat.get(\"title\", \"\")\n",
    "        meta[\"author\"] = \"; \".join(a.get(\"author\",{}).get(\"display_name\",\"\") for a in dat.get(\"authorships\",[]))\n",
    "        meta[\"year\"] = str(dat.get(\"publication_year\", \"\"))\n",
    "        meta[\"author_keywords\"] = \"; \".join(dat.get(\"keywords\", [])) if dat.get(\"keywords\") else \"\"\n",
    "        meta[\"source_journal\"] = dat.get(\"host_venue\", {}).get(\"display_name\",\"\") if dat.get(\"host_venue\") else \"\"\n",
    "        meta[\"study_type\"] = dat.get(\"type\", \"\")\n",
    "        countries = []\n",
    "        for a in dat.get(\"authorships\", []):\n",
    "            for inst in a.get(\"institutions\", []):\n",
    "                cc = inst.get(\"country_code\")\n",
    "                if cc: countries.append(cc)\n",
    "        meta[\"country\"] = \"; \".join(set([c for c in countries if c]))\n",
    "    except Exception as e:\n",
    "        error = f\"[WARN] OpenAlex: {e}\"\n",
    "        print_and_log(error)\n",
    "    return meta, error\n",
    "\n",
    "RAW_META_DIR = OP_DIR / \"audit\" / \"raw_meta\"\n",
    "RAW_META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "method_null_counts = {m: {f:0 for f in fields} for m in method_names}\n",
    "all_paper_tables = []\n",
    "\n",
    "# -- MAIN Extraction Loop with Path Handling Checked\n",
    "for idx, pdf in enumerate(pdfs):\n",
    "    assert isinstance(pdf, (Path, str)), f\"Unexpected type for pdf: {type(pdf)} -- value: {pdf}\"\n",
    "    pdf_id = pdf_hash_id(pdf)\n",
    "    errors = []\n",
    "\n",
    "    # Use stable local variable names for extracted outputs!\n",
    "    first_page, err_f = extract_first_page_text(pdf, debug=(idx==0))\n",
    "    if err_f: errors.append(err_f)\n",
    "    llm, err_llm = extract_ai_llm_full(first_page, api_key=OPENAI_API_KEY, model=llm_model, debug=(idx==0))\n",
    "    if err_llm: errors.append(err_llm)\n",
    "    grobid, err_grobid = extract_grobid_full(pdf)\n",
    "    if err_grobid: errors.append(err_grobid)\n",
    "    fitz_meta, err_fitz = extract_fitz_metadata(pdf)\n",
    "    if err_fitz: errors.append(err_fitz)\n",
    "    pdfplumber_meta, err_pdfplumber = extract_pdfplumber_metadata(pdf)\n",
    "    if err_pdfplumber: errors.append(err_pdfplumber)\n",
    "    filename_meta = extract_from_filename_meta(pdf)\n",
    "\n",
    "    doi_textscan = search_doi_in_text(pdf)\n",
    "    if not any([normalize_doi(x.get(\"doi\",\"\")) for x in (grobid, fitz_meta, pdfplumber_meta, filename_meta)]):\n",
    "        if doi_textscan:\n",
    "            filename_meta[\"doi\"] = doi_textscan\n",
    "\n",
    "    doi_for_api = normalize_doi(grobid.get(\"doi\",\"\") or fitz_meta.get(\"doi\",\"\") or pdfplumber_meta.get(\"doi\",\"\") or filename_meta.get(\"doi\",\"\") or llm.get(\"doi\",\"\") or \"\")\n",
    "    title_for_api = llm.get(\"title\") or grobid.get(\"title\") or fitz_meta.get(\"title\") or filename_meta.get(\"title\") or \"\"\n",
    "    author_for_api = llm.get(\"author\") or grobid.get(\"author\") or fitz_meta.get(\"author\") or filename_meta.get(\"author\") or \"\"\n",
    "    year_for_api = llm.get(\"year\") or grobid.get(\"year\") or fitz_meta.get(\"year\") or filename_meta.get(\"year\") or \"\"\n",
    "    crossref, err_crossref = extract_crossref_full(doi_for_api, title_for_api, author_for_api, year_for_api)\n",
    "    if err_crossref: errors.append(err_crossref)\n",
    "    if not doi_for_api and crossref.get(\"doi\"):\n",
    "        crossref[\"doi\"] = normalize_doi(crossref[\"doi\"])\n",
    "        doi_for_api = crossref[\"doi\"]\n",
    "    openalex, err_openalex = extract_openalex_full(doi_for_api, title_for_api)\n",
    "    if err_openalex: errors.append(err_openalex)\n",
    "\n",
    "    method_outputs = {\n",
    "        \"llm\":      {k: v for k,v in llm.items()},\n",
    "        \"grobid\":   {k: v for k,v in grobid.items()},\n",
    "        \"fitz\":     {k: v for k,v in fitz_meta.items()},\n",
    "        \"pdfplumber\":{k: v for k,v in pdfplumber_meta.items()},\n",
    "        \"filename\": {k: v for k,v in filename_meta.items()},\n",
    "        \"crossref\": {k: v for k,v in crossref.items()},\n",
    "        \"openalex\": {k: v for k,v in openalex.items()},\n",
    "    }\n",
    "    for m in method_names:\n",
    "        for f in fields:\n",
    "            orig = method_outputs[m][f]\n",
    "            if f == \"doi\":\n",
    "                method_outputs[m][f] = normalize_doi(orig)\n",
    "            elif f == \"author\":\n",
    "                method_outputs[m][f] = normalize_author(orig)\n",
    "            elif f == \"country\":\n",
    "                method_outputs[m][f] = normalize_country(orig)\n",
    "            elif f == \"author_keywords\":\n",
    "                method_outputs[m][f] = normalize_keywords(orig)\n",
    "            else:\n",
    "                method_outputs[m][f] = str(orig).strip() if orig and orig != \"null\" else \"\"\n",
    "    for m in method_names:\n",
    "        for f in fields:\n",
    "            val = method_outputs[m][f]\n",
    "            if (not val) or val.lower() == \"null\":\n",
    "                method_null_counts[m][f] += 1\n",
    "\n",
    "    raw_audit = {\n",
    "        \"pdf_id\": pdf_id, \"filename\": str(pdf),\n",
    "        \"methods\": method_outputs,\n",
    "        \"error_log\": errors\n",
    "    }\n",
    "    with open(RAW_META_DIR / f'{pdf_id}_raw_meta.json', \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(raw_audit, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    df_this_paper = pd.DataFrame({f: [method_outputs[m][f] for m in method_names] for f in fields}, index=method_names)\n",
    "    all_paper_tables.append({\"pdf_id\": pdf_id, \"fname\": str(pdf), \"table\": df_this_paper})\n",
    "\n",
    "print('\\n--- Null/Blank Value Table (methods \u00d7 fields, post-normalization) ---')\n",
    "df_null = pd.DataFrame(method_null_counts)\n",
    "print(df_null.T)\n",
    "\n",
    "for d in all_paper_tables:\n",
    "    print(f\"\\n\\n======== PAPER: {d['pdf_id']} / {d['fname']} ========\")\n",
    "    print(d[\"table\"].to_string())\n",
    "print(f\"\\n[INFO] Raw method+error audit JSON written for each PDF in {RAW_META_DIR}\")\n",
    "print(\"--- End Block 3 v6.4.1 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DF2A31168An",
    "outputId": "dbdf0920-96ca-489b-9487-a85059376421"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-d6cc2d1d6ecf>:131: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2024' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[idx, f\"{field}_final\"] = llm_val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Table 2a: Reviewer Correction Summary (AFTER AUTOFIX) ---\n",
      "title   : 0 flagged (manual review needed)\n",
      "author  : 3 flagged (manual review needed)\n",
      "year    : 1 flagged (manual review needed)\n",
      "doi     : 0 flagged (manual review needed)\n",
      "3 papers to review (out of 4)\n",
      "Autofixed fields breakdown: {'title': 4, 'year': 3, 'doi': 4, 'author': 1}\n",
      "\n",
      "Please TYPE YOUR FULL NAME for manifest lock (for audit):\n",
      "> David Brydon\n",
      "\n",
      "Do you wish to review/correct flagged fields now? [Y/N]: Y\n",
      "\n",
      "--- Reviewer Correction: Step through flagged fields ---\n",
      "\n",
      "[REVIEW] pdf_id: pdfid_0391138757b3 | FIELD: AUTHOR | flagged: author_disagree_or_missing\n",
      "[A] llm_raw  : [Missing]\n",
      "[B] grobid   : Mohammad Mengyang Yin\n",
      "[C] fitz     : [Missing]\n",
      "[D] crossref : Alkhalaf; Yin; Deng; Chang; Yu\n",
      "[E] openalex : Mohammad Alkhalaf; Mengyang Yin; Chao Deng; Hui\u2010Chen Chang; Ping Yu\n",
      "[F] filename : Copy of Alkhalaf et al.\n",
      "Choose value for author (A-F) or type MANUAL: e\n",
      "\n",
      "[REVIEW] pdf_id: pdfid_e4dd16ac4a87 | FIELD: AUTHOR | flagged: author_disagree_or_missing\n",
      "[A] llm_raw  : Nadine Bienefeld;Emanuela Keller;Gudela Grote\n",
      "[B] grobid   : Nadine Emanuela Keller\n",
      "[C] fitz     : [Missing]\n",
      "[D] crossref : Bienefeld; Keller; Grote\n",
      "[E] openalex : [Missing]\n",
      "[F] filename : Copy of Bienefeld et al.\n",
      "Choose value for author (A-F) or type MANUAL: a\n",
      "\n",
      "[REVIEW] pdf_id: pdfid_d3079134f529 | FIELD: AUTHOR | flagged: author_disagree_or_missing;year_disagree_or_missing\n",
      "[A] llm_raw  : Chiyoung Cha;Aram Cho;Gumhee Baek\n",
      "[B] grobid   : Chiyoung Aram Cho\n",
      "[C] fitz     : [Missing]\n",
      "[D] crossref : Cho; Cha; Baek\n",
      "[E] openalex : [Missing]\n",
      "[F] filename : Copy of Cho et al.\n",
      "Choose value for author (A-F) or type MANUAL: a\n",
      "\n",
      "[REVIEW] pdf_id: pdfid_d3079134f529 | FIELD: YEAR | flagged: author_disagree_or_missing;year_disagree_or_missing\n",
      "[A] llm_raw  : 2023\n",
      "[B] grobid   : [Missing]\n",
      "[C] fitz     : [Missing]\n",
      "[D] crossref : 2024\n",
      "[E] openalex : [Missing]\n",
      "[F] filename : 2024\n",
      "Choose value for year (A-F) or type MANUAL: f\n",
      "\n",
      "[Reviewer correction input complete.]\n",
      "\n",
      "--- SUMMARY: Final Manifest Table (LOCKED after Review) ---\n",
      "            pdf_id                                      title_final                                     author_final year_final                  doi_final  needs_review                                    review_reason\n",
      "pdfid_0391138757b3 Machine Learning Model to Extract Malnutritio... Mohammad Alkhalaf; Mengyang Yin; Chao Deng; H...       2024         10.3233/shti231240          True                       author_disagree_or_missing\n",
      "pdfid_e4dd16ac4a87 Human-AI Teaming in the ICU: A Comparative An...    Nadine Bienefeld;Emanuela Keller;Gudela Grote       2023              10.2196/50130          True                       author_disagree_or_missing\n",
      "pdfid_d3079134f529 AI-Based Tailored Mobile Intervention for Nur...                Chiyoung Cha;Aram Cho;Gumhee Baek       2024              10.2196/54029          True author_disagree_or_missing;year_disagree_or_m...\n",
      "pdfid_e6d59a511390 Appropriateness of Frequently Asked Patient Q... Jeremy A. Dubin; Sandeep S. Bains; Michael J....       2024 10.1016/j.arth.2024.04.020         False                                                 \n",
      "\n",
      "COUNT SUMMARY by FIELD source:\n",
      "title   : {'openalex/crossref': 2, 'llm_raw': 2}\n",
      "author  : {'openalex': 1, 'llm_raw': 2, 'openalex/crossref': 1}\n",
      "year    : {'llm_raw': 3, 'filename': 1}\n",
      "doi     : {'llm_raw': 2, 'crossref': 2}\n",
      "\n",
      "[REVIEWER FINAL CHECK] Submit this manifest as LOCKED for submission? [Y/N]: Y\n",
      "Manifest approved by reviewer for submission. Manifest is now LOCKED in audit log.\n",
      "\n",
      "--- Table 5: Extraction Source-Field Agreement Matrix ---\n",
      "        llm_raw  grobid  fitz  crossref  openalex  filename\n",
      "title         2       0     0         0         0         0\n",
      "author        2       0     0         0         1         0\n",
      "year          3       0     0         0         0         1\n",
      "doi           2       0     0         2         0         0\n",
      "\n",
      "[Final manifest saved to /content/drive/My Drive/Pilot/Nurse-AI_ScR_250509_1448/operational/final_manifest.csv]\n",
      "[Review correction log saved to review_correction_log.json]\n",
      "[SHA256 hashes written for outputs. Audit log updated at: /content/drive/My Drive/Pilot/Nurse-AI_ScR_250509_1448/operational/reviewer_block4_audit.jsonl]\n",
      "--- End Block 4 ---\n"
     ]
    }
   ],
   "source": [
    "# @title [Block 4] Reviewer QA, Correction, Approval of Manifest (v6.2.2)\n",
    "#\n",
    "# ----------- [Block Summary] -----------\n",
    "# - Loads extraction_manifest.csv and ensures all *_votes columns present.\n",
    "# - Prints flagged summary and requires reviewer name/custody.\n",
    "# - Field-by-field correction modal, manual/choice, audit-logs output.\n",
    "# - Outputs final locked manifest, reviewer log, agreement matrix.\n",
    "# - All outputs are hash-logged for audit/forensic fixity.\n",
    "#\n",
    "# ----------- [Version Control] -----------\n",
    "# v6.2.1: Robust *_votes handling, audit hashing, custody modal, reviewer workflow robust for PI/QA use.\n",
    "# v6.2.2: Reviewer QA, Autofix, Correction, & Submission Approval\n",
    "#\n",
    "# ----------- [Block Summary] -----------\n",
    "# - Loads extraction_manifest.csv, parses *_votes JSON\n",
    "# - Auto-confirms a field if \u22652 non-missing sources match LLM (lenient for title/author)\n",
    "# - For title/author: uses fuzzy normalization for consensus (OpenAlex/CrossRef selected if close to LLM)\n",
    "# - Prompts reviewer only on non-consensus fields\n",
    "# - At end, prints and requests reviewer \u201cReady to submit?\u201d approval\n",
    "# - All outputs hash-logged, custody and audit tracked\n",
    "\n",
    "import json, pandas as pd, datetime, hashlib\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def sha256_file(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def norm_str(x):\n",
    "    \"\"\" Normalize a string for lenient matching: lowercase, strip, no punct, no space \"\"\"\n",
    "    import re\n",
    "    return re.sub(r\"[\\s\\W_]+\", \"\", str(x or \"\").lower())\n",
    "\n",
    "def fuzzy_ratio(a, b):\n",
    "    \"\"\" Return difflib ratio between two strings (0-100) \"\"\"\n",
    "    return int(SequenceMatcher(None, str(a or \"\"), str(b or \"\")).ratio() * 100)\n",
    "\n",
    "with open(\"pipeline_env.json\", \"r\") as f:\n",
    "    env = json.load(f)\n",
    "OPERATIONAL_DIR = Path(env[\"OPERATIONAL_DIR\"])\n",
    "EXTRACTION_PATH = OPERATIONAL_DIR / \"extraction_manifest.csv\"\n",
    "REVIEW_LOG_PATH = OPERATIONAL_DIR / \"review_correction_log.json\"\n",
    "FINAL_MANIFEST_PATH = OPERATIONAL_DIR / \"final_manifest.csv\"\n",
    "AUDIT_PATH = OPERATIONAL_DIR / \"reviewer_block4_audit.jsonl\"\n",
    "\n",
    "fields = [\"title\", \"author\", \"year\", \"doi\"]\n",
    "methods = [\"llm_raw\", \"grobid\", \"fitz\", \"crossref\", \"openalex\", \"filename\"]\n",
    "\n",
    "def robust_json_parse(val):\n",
    "    if isinstance(val, dict): return val\n",
    "    if pd.isna(val) or str(val).strip() in [\"\", \"{}\", \"nan\", \"None\"]:\n",
    "        return {m:\"\" for m in methods}\n",
    "    try:\n",
    "        js = json.loads(val)\n",
    "        if not isinstance(js, dict): return {m:\"\" for m in methods}\n",
    "        for m in methods:\n",
    "            js.setdefault(m, \"\")\n",
    "        return js\n",
    "    except Exception:\n",
    "        try:\n",
    "            import ast\n",
    "            js = ast.literal_eval(val)\n",
    "            return dict(js) if isinstance(js, dict) else {m:\"\" for m in methods}\n",
    "        except Exception:\n",
    "            return {m:\"\" for m in methods}\n",
    "\n",
    "df = pd.read_csv(EXTRACTION_PATH)\n",
    "for field in fields:\n",
    "    col = f\"{field}_votes\"\n",
    "    if col not in df.columns:\n",
    "        df[col] = [{m:\"\" for m in methods} for _ in range(len(df))]\n",
    "    else:\n",
    "        df[col] = df[col].apply(robust_json_parse)\n",
    "\n",
    "# -------- Consensus/Autofix Step --------------\n",
    "autofix_counter = Counter()\n",
    "needs_manual_idx = []\n",
    "for idx, row in df.iterrows():\n",
    "    auto_decided = {}\n",
    "    for field in fields:\n",
    "        votes = row.get(f\"{field}_votes\", {m:\"\" for m in methods}).copy()\n",
    "        values = [v for v in votes.values() if v and v != \"[Missing]\"]\n",
    "        llm_val = votes.get(\"llm_raw\", \"\")\n",
    "        nonblank = [m for m in methods if votes[m] and votes[m] != \"[Missing]\"]\n",
    "\n",
    "        # Helper: Fuzzy or strict match\n",
    "        def close_enough(a, b, field):\n",
    "            if not a or not b: return False\n",
    "            if field in [\"title\", \"author\"]:\n",
    "                if norm_str(a) == norm_str(b): return True\n",
    "                return fuzzy_ratio(a, b) >= (88 if field == \"title\" else 82) # tunable\n",
    "            else:\n",
    "                return str(a).strip().lower() == str(b).strip().lower()\n",
    "\n",
    "        # For author/title, check if OpenAlex or CrossRef is \"close\" to LLM\n",
    "        preferred_raw = None\n",
    "        for m in [\"openalex\", \"crossref\"]:\n",
    "            if close_enough(votes.get(\"llm_raw\", \"\"), votes.get(m, \"\"), field):\n",
    "                preferred_raw = votes[m]\n",
    "                break\n",
    "\n",
    "        # Consensus: \u22652 methods \"close enough\" to LLM and non-missing OR LLM+OpenAlex/CrossRef is close\n",
    "        match_methods = [m for m in methods if close_enough(llm_val, votes[m], field)]\n",
    "        n_matches = len([k for k in match_methods if votes[k]])\n",
    "\n",
    "        # Ignore filename/fitz if only partial or non-informative (skip as consensus candidates for title/author)\n",
    "        valid_compare_methods = [m for m in methods if m != \"filename\" and votes[m]]\n",
    "\n",
    "        if field in [\"title\", \"author\"]:\n",
    "            # More aggressive autofix for noisy fields\n",
    "            if preferred_raw:\n",
    "                df.loc[idx, f\"{field}_final\"] = preferred_raw\n",
    "                df.loc[idx, f\"{field}_src\"] = \"openalex/crossref\"\n",
    "                auto_decided[field] = True\n",
    "                autofix_counter[field] += 1\n",
    "                continue\n",
    "            if n_matches >= 2 and set(match_methods).intersection(set(valid_compare_methods)):\n",
    "                df.loc[idx, f\"{field}_final\"] = llm_val\n",
    "                df.loc[idx, f\"{field}_src\"] = \"llm_raw\"\n",
    "                auto_decided[field] = True\n",
    "                autofix_counter[field] += 1\n",
    "                continue\n",
    "        else:\n",
    "            if n_matches >= 2:\n",
    "                df.loc[idx, f\"{field}_final\"] = llm_val\n",
    "                df.loc[idx, f\"{field}_src\"] = \"llm_raw\"\n",
    "                auto_decided[field] = True\n",
    "                autofix_counter[field] += 1\n",
    "                continue\n",
    "\n",
    "        # If only one non-missing value, auto-select\n",
    "        if len(nonblank) == 1:\n",
    "            df.loc[idx, f\"{field}_final\"] = votes[nonblank[0]]\n",
    "            df.loc[idx, f\"{field}_src\"] = nonblank[0]\n",
    "            auto_decided[field] = True\n",
    "            autofix_counter[field] += 1\n",
    "            continue\n",
    "\n",
    "        # Too ambiguous or no close match\u2014needs reviewer\n",
    "        if not auto_decided.get(field, False):\n",
    "            needs_manual_idx.append((idx, field))\n",
    "\n",
    "# Second pass: Flag only fields needing review\n",
    "needs_review = set([idx for idx, f in needs_manual_idx])\n",
    "df[\"needs_review\"] = df.index.isin(needs_review)\n",
    "df[\"review_reason\"] = \"\"\n",
    "for idx in range(len(df)):\n",
    "    missing = [f for f in fields if (idx, f) in needs_manual_idx]\n",
    "    df.loc[idx, \"review_reason\"] = \";\".join(f\"{f}_disagree_or_missing\" for f in missing)\n",
    "\n",
    "# --------- Table 2a: Reviewer Correction Summary -----------\n",
    "print(\"\\n--- Table 2a: Reviewer Correction Summary (AFTER AUTOFIX) ---\")\n",
    "needreview = df[df[\"needs_review\"] == True]\n",
    "reason_counter = Counter()\n",
    "for _, row in needreview.iterrows():\n",
    "    for reason in str(row.get(\"review_reason\", \"\")).split(\";\"):\n",
    "        if reason.strip():\n",
    "            reason_counter[reason.split(\"_\")[0]] += 1\n",
    "for f in fields:\n",
    "    print(f\"{f:8}: {reason_counter.get(f,0)} flagged (manual review needed)\")\n",
    "print(f\"{len(needreview)} papers to review (out of {len(df)})\")\n",
    "print(f\"Autofixed fields breakdown: {dict(autofix_counter)}\")\n",
    "\n",
    "# ----------- Reviewer name/custody modal --------------\n",
    "reviewer_name = None\n",
    "while not reviewer_name or len(reviewer_name.strip()) < 2:\n",
    "    reviewer_name = input(\"\\nPlease TYPE YOUR FULL NAME for manifest lock (for audit):\\n> \").strip()\n",
    "\n",
    "result_decision = input(\n",
    "    \"\\nDo you wish to review/correct flagged fields now? [Y/N]: \"\n",
    ").lower().strip()\n",
    "corrections = []\n",
    "\n",
    "if result_decision == \"y\" and len(needreview):\n",
    "    print(\"\\n--- Reviewer Correction: Step through flagged fields ---\")\n",
    "    for idx, row in needreview.iterrows():\n",
    "        pdf_id = row[\"pdf_id\"]\n",
    "        for field in fields:\n",
    "            if f\"{field}_disagree_or_missing\" not in str(row.get(\"review_reason\", \"\")):\n",
    "                continue\n",
    "            votes = row.get(f\"{field}_votes\", {m:\"\" for m in methods})\n",
    "            print(f\"\\n[REVIEW] pdf_id: {pdf_id} | FIELD: {field.upper()} | flagged: {row.get('review_reason','')}\")\n",
    "            for i, method in enumerate(methods):\n",
    "                val_disp = votes.get(method, \"\") if votes.get(method, \"\") else \"[Missing]\"\n",
    "                print(f\"[{chr(65+i)}] {method:<9}: {val_disp}\")\n",
    "            sel = input(f\"Choose value for {field} (A-{chr(65+len(methods)-1)}) or type MANUAL: \").strip()\n",
    "            if sel.lower() == \"manual\":\n",
    "                val = input(f\"Manual value for {field}: \").strip()\n",
    "                df.loc[idx, f\"{field}_final\"] = val\n",
    "                df.loc[idx, f\"{field}_src\"] = \"manual\"\n",
    "                corrections.append(\n",
    "                    {\"pdf_id\": pdf_id, \"field\": field, \"chosen\": val, \"src\": \"manual\", \"reviewer\": reviewer_name}\n",
    "                )\n",
    "            elif len(sel) == 1 and chr(65) <= sel.upper() < chr(65+len(methods)):\n",
    "                chosen_method = methods[ord(sel.upper()) - 65]\n",
    "                val = votes.get(chosen_method, \"\")\n",
    "                df.loc[idx, f\"{field}_final\"] = val\n",
    "                df.loc[idx, f\"{field}_src\"] = chosen_method\n",
    "                corrections.append(\n",
    "                    {\"pdf_id\": pdf_id, \"field\": field, \"chosen\": val, \"src\": chosen_method, \"reviewer\": reviewer_name}\n",
    "                )\n",
    "    print(\"\\n[Reviewer correction input complete.]\")\n",
    "else:\n",
    "    print(\"\\nNo manual review required or corrections skipped.\")\n",
    "\n",
    "# --------- Final Manifest Summary & Ready-to-Submit Step -----------\n",
    "print(\"\\n--- SUMMARY: Final Manifest Table (LOCKED after Review) ---\")\n",
    "finalcols = [\"pdf_id\"] + [f\"{field}_final\" for field in fields] + [\"needs_review\", \"review_reason\"]\n",
    "print(df[finalcols].to_string(index=False, max_colwidth=48))\n",
    "print(\"\\nCOUNT SUMMARY by FIELD source:\")\n",
    "for field in fields:\n",
    "    sources = Counter([str(row.get(f\"{field}_src\", \"\")) for _, row in df.iterrows()])\n",
    "    print(f\"{field:8}: {dict(sources)}\")\n",
    "\n",
    "final_approve = input(\"\\n[REVIEWER FINAL CHECK] Submit this manifest as LOCKED for submission? [Y/N]: \").lower().strip()\n",
    "if final_approve != \"y\":\n",
    "    print(\"Lock/review decision withheld. No submission flagged. Please rerun this after changes if needed.\")\n",
    "else:\n",
    "    print(\"Manifest approved by reviewer for submission. Manifest is now LOCKED in audit log.\")\n",
    "\n",
    "# ---------- Table 5: Extraction Source-Field Agreement Matrix --------\n",
    "print(\"\\n--- Table 5: Extraction Source-Field Agreement Matrix ---\")\n",
    "method_counts = pd.DataFrame(0, index=fields, columns=methods)\n",
    "for idx, row in df.iterrows():\n",
    "    for field in fields:\n",
    "        src = str(row.get(f\"{field}_src\", \"\")).lower()\n",
    "        if src in method_counts.columns:\n",
    "            method_counts.at[field, src] += 1\n",
    "print(method_counts.to_string())\n",
    "\n",
    "# ------------- Write reviewer-locked manifest/log (hash-logged) -------------\n",
    "df.to_csv(FINAL_MANIFEST_PATH, index=False)\n",
    "with open(REVIEW_LOG_PATH, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"reviewer\": reviewer_name,\n",
    "        \"timestamp_utc\": str(datetime.datetime.utcnow()) + \"Z\",\n",
    "        \"timestamp_nzdt\": str(datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=12))),),\n",
    "        \"changes\": corrections,\n",
    "        \"final_approved\": final_approve == \"y\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "out_hashes = {\n",
    "    \"final_manifest.csv.sha256\": sha256_file(FINAL_MANIFEST_PATH),\n",
    "    \"review_correction_log.json.sha256\": sha256_file(REVIEW_LOG_PATH)\n",
    "}\n",
    "for out, hashv in out_hashes.items():\n",
    "    with open(OPERATIONAL_DIR / out, \"w\") as f:\n",
    "        f.write(hashv or \"\")\n",
    "\n",
    "# Minimal audit log\n",
    "with open(AUDIT_PATH, \"a\") as f:\n",
    "    f.write(json.dumps({\n",
    "        \"step\": \"block4_reviewer_approval\",\n",
    "        \"timestamp_utc\": str(datetime.datetime.utcnow()) + \"Z\",\n",
    "        \"timestamp_nzdt\": str(datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=12))),),\n",
    "        \"outputs\": {k: str(OPERATIONAL_DIR / k) for k in out_hashes},\n",
    "        \"hashes\": out_hashes,\n",
    "        \"reviewer\": reviewer_name,\n",
    "        \"final_approved\": final_approve == \"y\"\n",
    "    }) + \"\\n\")\n",
    "\n",
    "print(f\"\\n[Final manifest saved to {FINAL_MANIFEST_PATH}]\")\n",
    "print(\"[Review correction log saved to review_correction_log.json]\")\n",
    "print(f\"[SHA256 hashes written for outputs. Audit log updated at: {AUDIT_PATH}]\")\n",
    "print(\"--- End Block 4 ---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}